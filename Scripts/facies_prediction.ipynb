{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* System Append to set proper path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lasio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pandas Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Source.Utils import welllog\n",
    "from Source.Utils import multi_df\n",
    "from Source.Utils import well_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tqdm Progress Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "source": [
    "# Checkpoint import"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.read_csv('../checkpoints/litho_data_3.csv.gz', compression='gzip') # already non-null litho classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df['LITHOLOGY_GEOLINK_2'] = total_df['LITHOLOGY_GEOLINK']\n",
    "\n",
    "for code in tqdm_notebook(total_df.index, desc='Regrouping lithologies'):\n",
    "\n",
    "    if (total_df['LITHOLOGY_GEOLINK_2'][code] == 2 or total_df['LITHOLOGY_GEOLINK_2'][code] == 3):\n",
    "\n",
    "        total_df['LITHOLOGY_GEOLINK_2'][code] = 1\n",
    "\n",
    "    elif total_df['LITHOLOGY_GEOLINK_2'][code] == 6:\n",
    "\n",
    "        total_df['LITHOLOGY_GEOLINK_2'][code] = 5\n",
    "\n",
    "    elif total_df['LITHOLOGY_GEOLINK_2'][code] == 34:\n",
    "\n",
    "        total_df['LITHOLOGY_GEOLINK_2'][code] = 33\n",
    "\n",
    "\n",
    "    elif total_df['LITHOLOGY_GEOLINK_2'][code] == 13:\n",
    "\n",
    "        total_df['LITHOLOGY_GEOLINK_2'][code] = 12\n",
    "\n",
    "    else:\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "source": [
    "# Lithology Code Prediction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LITHOLOGY_GEOLINK non NULL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#litho_data = total_df.drop(columns=['WELL_NAME', 'LITHOLOGY_GEOLINK_2', 'LITHOLOGY_GEOLINK_3', 'DRDN', 'AI']) # litho1 optimum set (84.9)\n",
    "\n",
    "litho_data = total_df.drop(columns=['WELL_NAME', 'LITHOLOGY_GEOLINK', 'LITHOLOGY_GEOLINK_3', 'DRDN', 'AI']) # litho2 optimum set (84.5)\n",
    "\n",
    "# litho_data = total_df.drop(columns=['WELL_NAME', 'LITHOLOGY_GEOLINK_2', 'LITHOLOGY_GEOLINK']) # litho3 optimum set\n",
    "\n",
    "litho_data.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(litho_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * Converting LITHOLOGY_GEOLINK to int type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "litho_data['LITHOLOGY_GEOLINK_2'] = litho_data['LITHOLOGY_GEOLINK_2'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * Checking classes balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsorted_bal_class = dict(Counter(litho_data['LITHOLOGY_GEOLINK_2'].values))\n",
    "\n",
    "sorted_bal_class = {k: v for k, v in sorted(unsorted_bal_class.items(), key=lambda item: item[1])}\n",
    "\n",
    "sorted_bal_class # although some classes have a considerable less representation than others, we need to respect this distribution to maintain the geological setting of the area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrmat_litho = abs(litho_data.corr()) # absolute correlation\n",
    "\n",
    "# plt.figure(figsize=(25,15))\n",
    "\n",
    "# sns.heatmap(corrmat_litho, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = litho_data.drop(columns='LITHOLOGY_GEOLINK_2')\n",
    "\n",
    "Y = litho_data['LITHOLOGY_GEOLINK_2'].values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.5, random_state=42) # close to the tabnet training dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('### Training Dataset: ### ', x_train.shape, y_train.shape)\n",
    "print('### Test Dataset: ### ', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold, RandomizedSearchCV, cross_validate\n",
    "\n",
    "class_pipelines = []\n",
    "\n",
    "class_pipelines.append(('ScaledRidge', Pipeline([('Scaler', RobustScaler()),('Ridge', linear_model.RidgeClassifier())])))\n",
    "class_pipelines.append(('ScaledSGDCls', Pipeline([('Scaler', RobustScaler()),('SGDCls', linear_model.SGDClassifier(n_jobs=30))])))\n",
    "class_pipelines.append(('ScaledKNNCls', Pipeline([('Scaler', RobustScaler()),('KNNCls', KNeighborsClassifier(n_jobs=30))])))\n",
    "class_pipelines.append(('ScaledDTC', Pipeline([('Scaler', RobustScaler()),('DTC', DecisionTreeClassifier())])))\n",
    "class_pipelines.append(('ScaledRFC', Pipeline([('Scaler', RobustScaler()),('RFC', RandomForestClassifier(n_jobs=30))])))\n",
    "class_pipelines.append(('ScaledADA', Pipeline([('Scaler', RobustScaler()),('ADA', AdaBoostClassifier())])))\n",
    "class_pipelines.append(('ScaledGBC', Pipeline([('Scaler', RobustScaler()),('GBC', GradientBoostingClassifier())])))\n",
    "class_pipelines.append(('ScaledXBG', Pipeline([('Scaler', RobustScaler()),('XBG', XGBClassifier(n_jobs=30))])))\n",
    "class_pipelines.append(('ScaledCAT', Pipeline([('Scaler', RobustScaler()),('CAT', CatBoostClassifier())])))\n",
    "class_pipelines.append(('ScaledLGBM', Pipeline([('Scaler', RobustScaler()),('LGBM', LGBMClassifier(n_jobs=30))])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, balanced_accuracy_score, make_scorer\n",
    "\n",
    "results = []\n",
    "\n",
    "names = []\n",
    "\n",
    "for name, model in tqdm_notebook(class_pipelines, desc='Cross-Validation Procedure'):\n",
    "\n",
    "    kfold = KFold(n_splits=5, random_state=42)\n",
    "\n",
    "    scorers = {'accuracy': make_scorer(accuracy_score), 'balanced_accuracy': make_scorer(balanced_accuracy_score), 'f1': make_scorer(f1_score, average='macro')}\n",
    "\n",
    "    final_scorers = cross_validate(model, x_train, y_train, cv=kfold, scoring=scorers)\n",
    "    results.append(final_scorers)\n",
    "    names.append(name)\n",
    "    print(name, 'Acc: ', final_scorers['test_accuracy'].mean(), '\\\\', final_scorers['test_accuracy'].std(), '\\n')\n",
    "    print(name, 'BalAcc: ', final_scorers['test_balanced_accuracy'].mean(), '\\\\', final_scorers['test_balanced_accuracy'].std(), '\\n')\n",
    "    print(name, 'F1: ', final_scorers['test_f1'].mean(), '\\\\', final_scorers['test_f1'].std(), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * Hyperparameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# # Number of trees in random forest\n",
    "# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# # Number of features to consider at every split\n",
    "# max_features = ['auto', 'sqrt']\n",
    "# # Maximum number of levels in tree\n",
    "# max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "# max_depth.append(None)\n",
    "# # Minimum number of samples required to split a node\n",
    "# min_samples_split = [2, 5, 10]\n",
    "# # Minimum number of samples required at each leaf node\n",
    "# min_samples_leaf = [1, 2, 4]\n",
    "# # Method of selecting samples for training each tree\n",
    "# bootstrap = [True, False]\n",
    "# # Create the random grid\n",
    "# random_grid = {'n_estimators': n_estimators,\n",
    "#                'max_features': max_features,\n",
    "#                'max_depth': max_depth,\n",
    "#                'min_samples_split': min_samples_split,\n",
    "#                'min_samples_leaf': min_samples_leaf,\n",
    "#                'bootstrap': bootstrap}\n",
    "\n",
    "# rfc = RandomForestClassifier(n_jobs=60)\n",
    "\n",
    "# kfold = KFold(n_splits=5, random_state=42)\n",
    "\n",
    "# rfc_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, n_iter = 100, cv = kfold, verbose=10, random_state=42, scoring='balanced_accuracy')\n",
    "\n",
    "# rfc_random.fit(RobustScaler().fit_transform(x_train), y_train)\n",
    "\n",
    "# print(str(round((time.time() - start_time)/60,1)) + ' minutes taken') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_cv = pd.DataFrame.from_dict(rfc_random.cv_results_)\n",
    "\n",
    "# result_cv.to_csv('../models/results/rand_search_rfc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "best_rfc = RandomForestClassifier(max_depth=40, n_estimators=1400, max_features='auto', min_samples_split=2, bootstrap=False, n_jobs=60) # the rest of the best parameters are  the default ones\n",
    "\n",
    "best_rfc.fit(RobustScaler().fit_transform(x_train), y_train)\n",
    "\n",
    "print(str(round((time.time() - start_time)/60,1)) + ' minutes taken') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * Model Accuracy Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "y_predict = best_rfc.predict(RobustScaler().fit_transform(x_test))\n",
    "\n",
    "print(str(round((time.time() - start_time)/60,1)) + ' minutes taken') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Accuracy: ', str(accuracy_score(y_test, y_predict)), '\\n')\n",
    "print('########################', '\\n')\n",
    "print('Balanced Accuracy: ', str(balanced_accuracy_score(y_test, y_predict)))\n",
    "print('########################', '\\n')\n",
    "print('F1-Score Micro: ', str(f1_score(y_test, y_predict, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_feature_importances = (best_rfc.feature_importances_)\n",
    "sorted_idx = tree_feature_importances.argsort()\n",
    "y_ticks = np.arange(0, len(X.columns))\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(y_ticks, tree_feature_importances[sorted_idx])\n",
    "ax.set_yticklabels(X.columns[sorted_idx])\n",
    "ax.set_yticks(y_ticks)\n",
    "ax.set_title(\"Random Forest Feature Importances (MDI)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "plot_confusion_matrix(best_rfc, RobustScaler().fit_transform(x_test), y_test, normalize='true', cmap=plt.cm.Blues, ax=ax, values_format='.1f')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_feature_importances = best_rfc.feature_importances_\n",
    "sorted_idx = tree_feature_importances.argsort()\n",
    "\n",
    "y_ticks = np.arange(0, len(X.columns))\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(y_ticks, tree_feature_importances[sorted_idx])\n",
    "ax.set_yticklabels(X.columns[sorted_idx])\n",
    "ax.set_yticks(y_ticks)\n",
    "ax.set_title(\"Random Forest Feature Importances (MDI)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "    * Null Values Prediction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "        * Null Values Dataset "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "null_litho_dataset = total_df[pd.isnull(total_df['LITHOLOGY_GEOLINK'])].drop(columns=['WELL_NAME', 'LITHOLOGY_GEOLINK'])"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "        * Prediction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "litho_prediction = best_rfc.predict(RobustScaler().fit_transform(null_litho_dataset))"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "        * Replacing Null Values for Predicted Ones"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "predicted_litho_dataset = null_litho_dataset\n",
    "\n",
    "predicted_litho_dataset['LITHOLOGY_GEOLINK'] = litho_prediction\n",
    "\n",
    "predicted_litho_dataset.head(n=5)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "    * Final Dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "final_df = litho_data.append(predicted_litho_dataset)\n",
    "\n",
    "final_df.sort_index(inplace=True)\n",
    "\n",
    "final_df['WELL_NAME'] = total_df['WELL_NAME'].values\n",
    "\n",
    "final_df.head(n=5)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Checkpoints"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "path_file_final_df = '../checkpoints/final_df.csv.gz'\n",
    "\n",
    "final_df.to_csv(path_file_final_df,index=False, compression='gzip')"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}