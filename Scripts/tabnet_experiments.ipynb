{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bit42bf01c25ac14defb7b59bc3ba727332",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Importing Libraryies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* System Append to set proper path"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')"
   ]
  },
  {
   "source": [
    "* Default"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lasio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from collections import Counter"
   ]
  },
  {
   "source": [
    "* Pandas Option"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', None)"
   ]
  },
  {
   "source": [
    "* Tqdm Progress Bar"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "source": [
    "# Checkpoint Import"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../checkpoints/total_df.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        CALI      NPHI   RHOB         GR         DTC      RDEP  \\\n",
       "0  19.406000  0.475704  1.789  64.497482  167.582153  1.574993   \n",
       "1  19.406000  0.479429  1.754  62.406261  167.425064  1.569011   \n",
       "2  19.406000  0.474963  1.778  62.629055  167.808395  1.578010   \n",
       "3  19.452999  0.504394  1.642  65.998596  169.244873  1.586024   \n",
       "4  19.452999  0.480163  1.563  64.997223  170.635086  1.603011   \n",
       "\n",
       "   LITHOLOGY_GEOLINK       DEPTH WELL_NAME  \n",
       "0                NaN  493.493134   15_9-12  \n",
       "1                NaN  493.645538   15_9-12  \n",
       "2                NaN  493.797943   15_9-12  \n",
       "3                NaN  493.950348   15_9-12  \n",
       "4                NaN  494.102722   15_9-12  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CALI</th>\n      <th>NPHI</th>\n      <th>RHOB</th>\n      <th>GR</th>\n      <th>DTC</th>\n      <th>RDEP</th>\n      <th>LITHOLOGY_GEOLINK</th>\n      <th>DEPTH</th>\n      <th>WELL_NAME</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>19.406000</td>\n      <td>0.475704</td>\n      <td>1.789</td>\n      <td>64.497482</td>\n      <td>167.582153</td>\n      <td>1.574993</td>\n      <td>NaN</td>\n      <td>493.493134</td>\n      <td>15_9-12</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>19.406000</td>\n      <td>0.479429</td>\n      <td>1.754</td>\n      <td>62.406261</td>\n      <td>167.425064</td>\n      <td>1.569011</td>\n      <td>NaN</td>\n      <td>493.645538</td>\n      <td>15_9-12</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19.406000</td>\n      <td>0.474963</td>\n      <td>1.778</td>\n      <td>62.629055</td>\n      <td>167.808395</td>\n      <td>1.578010</td>\n      <td>NaN</td>\n      <td>493.797943</td>\n      <td>15_9-12</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>19.452999</td>\n      <td>0.504394</td>\n      <td>1.642</td>\n      <td>65.998596</td>\n      <td>169.244873</td>\n      <td>1.586024</td>\n      <td>NaN</td>\n      <td>493.950348</td>\n      <td>15_9-12</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>19.452999</td>\n      <td>0.480163</td>\n      <td>1.563</td>\n      <td>64.997223</td>\n      <td>170.635086</td>\n      <td>1.603011</td>\n      <td>NaN</td>\n      <td>494.102722</td>\n      <td>15_9-12</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "source": [
    "# Lithology Code Prediction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "litho_data = df[pd.notnull(df['LITHOLOGY_GEOLINK'])].drop(columns=['WELL_NAME']) # not null dataframe (model training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1294715"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "len(litho_data)"
   ]
  },
  {
   "source": [
    "    * Converting Lithology Data to Integer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "litho_data['LITHOLOGY_GEOLINK'] = litho_data['LITHOLOGY_GEOLINK'].astype(int)"
   ]
  },
  {
   "source": [
    "    * Reducing Dataset Size"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "litho_data[\"Set\"] = np.random.choice([\"train_red\", \"rest\"], p =[.4, .6], size=(litho_data.shape[0],))\n",
    "\n",
    "train = litho_data[litho_data.Set == 'train_red']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(519041, 9) (1294715, 9)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, litho_data.shape)"
   ]
  },
  {
   "source": [
    "    * Dataset Split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train.drop(columns=['LITHOLOGY_GEOLINK', 'Set'])\n",
    "\n",
    "Y = train['LITHOLOGY_GEOLINK']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state=42)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "### Training Dataset: ###  (332185, 7) (332185,)\n### Validation Dataset: ###  (83047, 7) (83047,)\n### Test Dataset: ###  (103809, 7) (103809,)\n"
     ]
    }
   ],
   "source": [
    "print('### Training Dataset: ### ', x_train.shape, y_train.shape)\n",
    "print('### Validation Dataset: ### ', x_val.shape, y_val.shape)\n",
    "print('### Test Dataset: ### ', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "x_train = std_scaler.fit_transform(x_train)\n",
    "x_val = std_scaler.fit_transform(x_val)\n",
    "x_test = std_scaler.fit_transform(x_test)"
   ]
  },
  {
   "source": [
    "    * Hyper-Parameter Tunning --> Random Search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "        * Parameter Grid Definition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "grid = {\n",
    "    \"n_a\": [4, 8, 16, 32, 64, 128],\n",
    "    \"n_independent\": [1, 2, 3, 4, 5, 7, 8, 9, 10],\n",
    "    \"n_shared\": [1, 2, 3, 4, 5], \n",
    "    \"n_steps\": [3, 5, 8, 11, 13],\n",
    "    \"clip_value\": [10, 1., 0.5, 0.01],\n",
    "    \"momentum\": [2 ,1, 0.1, 0.05, 0.02, 0.005],\n",
    "    \"lambda_sparse\": [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "    \"scheduler_params\": [{'gamma': 0.001}, {'gamma': 0.01}, {'gamma': 0.05}, {'gamma': 1},    {'gamma': 1.3}, {'gamma': 1.8}, {'gamma': 2.2}, {'gamma': 3}, {'gamma': 4}, {'step_size': 10}, {'step_size': 25}, {'step_size': 50}, {'step_size': 75}, {'step_size': 100}],\n",
    "    \"optimizer_params\": [{'lr': 1e-2}, {'lr': 1e-3}, {'lr': 1e-4}, {'lr': 1e-5}]\n",
    "}"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 15,
   "outputs": []
  },
  {
   "source": [
    "        * TabNet Wrapper for Hyper-Tunning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier"
   ]
  },
  {
   "source": [
    "class TabNetTuner(TabNetClassifier):\n",
    "    def fit(self, X, y, *args, **kwargs):\n",
    "        \n",
    "        self.n_d = self.n_a\n",
    "\n",
    "        x_train, x_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
    "        \n",
    "        return super().fit(\n",
    "            x_train,y_train, eval_set=[(x_train, y_train), (x_val, y_val)],\n",
    "            eval_name=['train', 'valid'],\n",
    "            eval_metric=['balanced_accuracy'], max_epochs=100,\n",
    "            batch_size=1024)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 17,
   "outputs": []
  },
  {
   "source": [
    "        * Grid Search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "\n",
    "tb = TabNetTuner() \n",
    "\n",
    "kfold = KFold(n_splits=5, random_state=42)\n",
    "\n",
    "rand_search = RandomizedSearchCV(\n",
    "    tb, grid,\n",
    "    scoring=\"balanced_accuracy\", n_jobs=1,\n",
    "    cv=kfold,\n",
    "    verbose=2, \n",
    "    return_train_score=True,\n",
    "    n_iter=10\n",
    "    )"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device used : cuda\n"
     ]
    }
   ]
  },
  {
   "source": [
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "with ProgressBar():\n",
    "    rand_search.fit(x_train, y_train)\n",
    "rand_search.best_params_"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device used : cuda\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'gamma': 4}, optimizer_params={'lr': 1e-05}, n_steps=13, n_shared=1, n_independent=3, n_a=64, momentum=1, lambda_sparse=0.0001, clip_value=0.5 \n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "epoch 0  | loss: 21.15002| train_balanced_accuracy: 0.05256 | valid_balanced_accuracy: 0.05213 |  0:01:06s\n",
      "epoch 1  | loss: 19.97347| train_balanced_accuracy: 0.03863 | valid_balanced_accuracy: 0.03732 |  0:02:12s\n",
      "epoch 2  | loss: 18.86325| train_balanced_accuracy: 0.04406 | valid_balanced_accuracy: 0.04497 |  0:03:19s\n",
      "epoch 3  | loss: 17.73281| train_balanced_accuracy: 0.03913 | valid_balanced_accuracy: 0.03882 |  0:04:25s\n",
      "epoch 4  | loss: 16.7149 | train_balanced_accuracy: 0.04569 | valid_balanced_accuracy: 0.04489 |  0:05:32s\n",
      "epoch 5  | loss: 15.7712 | train_balanced_accuracy: 0.0361  | valid_balanced_accuracy: 0.03705 |  0:06:37s\n",
      "epoch 6  | loss: 14.86574| train_balanced_accuracy: 0.03831 | valid_balanced_accuracy: 0.03739 |  0:07:44s\n",
      "epoch 7  | loss: 13.91366| train_balanced_accuracy: 0.03969 | valid_balanced_accuracy: 0.04055 |  0:08:51s\n",
      "epoch 8  | loss: 13.05784| train_balanced_accuracy: 0.03751 | valid_balanced_accuracy: 0.03621 |  0:09:58s\n",
      "epoch 9  | loss: 12.1538 | train_balanced_accuracy: 0.04452 | valid_balanced_accuracy: 0.04489 |  0:11:06s\n",
      "epoch 10 | loss: 11.22772| train_balanced_accuracy: 0.04241 | valid_balanced_accuracy: 0.04107 |  0:12:13s\n",
      "\n",
      "Early stopping occured at epoch 10 with best_epoch = 0 and best_valid_balanced_accuracy = 0.05213\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'gamma': 4}, optimizer_params={'lr': 1e-05}, n_steps=13, n_shared=1, n_independent=3, n_a=64, momentum=1, lambda_sparse=0.0001, clip_value=0.5, total=12.6min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'gamma': 4}, optimizer_params={'lr': 1e-05}, n_steps=13, n_shared=1, n_independent=3, n_a=64, momentum=1, lambda_sparse=0.0001, clip_value=0.5 \n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 12.9min remaining:    0.0s\n",
      "epoch 0  | loss: 21.1917 | train_balanced_accuracy: 0.045   | valid_balanced_accuracy: 0.04504 |  0:01:07s\n",
      "epoch 1  | loss: 20.06826| train_balanced_accuracy: 0.0448  | valid_balanced_accuracy: 0.04398 |  0:02:15s\n",
      "epoch 2  | loss: 18.99732| train_balanced_accuracy: 0.04526 | valid_balanced_accuracy: 0.04653 |  0:03:22s\n",
      "epoch 3  | loss: 17.97869| train_balanced_accuracy: 0.0341  | valid_balanced_accuracy: 0.03233 |  0:04:29s\n",
      "epoch 4  | loss: 17.03256| train_balanced_accuracy: 0.04565 | valid_balanced_accuracy: 0.04548 |  0:05:35s\n",
      "epoch 5  | loss: 16.0829 | train_balanced_accuracy: 0.03711 | valid_balanced_accuracy: 0.03907 |  0:06:41s\n",
      "epoch 6  | loss: 15.0885 | train_balanced_accuracy: 0.04552 | valid_balanced_accuracy: 0.04429 |  0:07:47s\n",
      "epoch 7  | loss: 14.10144| train_balanced_accuracy: 0.04116 | valid_balanced_accuracy: 0.04298 |  0:08:53s\n",
      "epoch 8  | loss: 13.1636 | train_balanced_accuracy: 0.03178 | valid_balanced_accuracy: 0.03062 |  0:09:59s\n",
      "epoch 9  | loss: 12.26523| train_balanced_accuracy: 0.03676 | valid_balanced_accuracy: 0.03628 |  0:11:05s\n",
      "epoch 10 | loss: 11.53409| train_balanced_accuracy: 0.03823 | valid_balanced_accuracy: 0.03925 |  0:12:11s\n",
      "epoch 11 | loss: 10.74455| train_balanced_accuracy: 0.04949 | valid_balanced_accuracy: 0.04848 |  0:13:17s\n",
      "epoch 12 | loss: 10.057  | train_balanced_accuracy: 0.0374  | valid_balanced_accuracy: 0.03579 |  0:14:23s\n",
      "epoch 13 | loss: 9.40992 | train_balanced_accuracy: 0.03853 | valid_balanced_accuracy: 0.03813 |  0:15:29s\n",
      "epoch 14 | loss: 8.74826 | train_balanced_accuracy: 0.03925 | valid_balanced_accuracy: 0.03801 |  0:16:35s\n",
      "epoch 15 | loss: 8.2015  | train_balanced_accuracy: 0.03933 | valid_balanced_accuracy: 0.04006 |  0:17:43s\n",
      "epoch 16 | loss: 7.68869 | train_balanced_accuracy: 0.03858 | valid_balanced_accuracy: 0.0376  |  0:18:50s\n",
      "epoch 17 | loss: 7.19724 | train_balanced_accuracy: 0.05124 | valid_balanced_accuracy: 0.05255 |  0:19:57s\n",
      "epoch 18 | loss: 6.74718 | train_balanced_accuracy: 0.04706 | valid_balanced_accuracy: 0.04592 |  0:21:05s\n",
      "epoch 19 | loss: 6.37474 | train_balanced_accuracy: 0.04713 | valid_balanced_accuracy: 0.04617 |  0:22:12s\n",
      "epoch 20 | loss: 6.01757 | train_balanced_accuracy: 0.0516  | valid_balanced_accuracy: 0.05241 |  0:23:19s\n",
      "epoch 21 | loss: 5.71658 | train_balanced_accuracy: 0.05349 | valid_balanced_accuracy: 0.05306 |  0:24:25s\n",
      "epoch 22 | loss: 5.40754 | train_balanced_accuracy: 0.06843 | valid_balanced_accuracy: 0.0671  |  0:25:30s\n",
      "epoch 23 | loss: 5.09777 | train_balanced_accuracy: 0.08021 | valid_balanced_accuracy: 0.07744 |  0:26:37s\n",
      "epoch 24 | loss: 4.86457 | train_balanced_accuracy: 0.08097 | valid_balanced_accuracy: 0.08137 |  0:27:43s\n",
      "epoch 25 | loss: 4.64631 | train_balanced_accuracy: 0.08162 | valid_balanced_accuracy: 0.08102 |  0:28:49s\n",
      "epoch 26 | loss: 4.45575 | train_balanced_accuracy: 0.07172 | valid_balanced_accuracy: 0.07133 |  0:29:55s\n",
      "epoch 27 | loss: 4.28454 | train_balanced_accuracy: 0.08798 | valid_balanced_accuracy: 0.08797 |  0:31:00s\n",
      "epoch 28 | loss: 4.14704 | train_balanced_accuracy: 0.08582 | valid_balanced_accuracy: 0.08779 |  0:32:06s\n",
      "epoch 29 | loss: 3.99598 | train_balanced_accuracy: 0.08956 | valid_balanced_accuracy: 0.08972 |  0:33:11s\n",
      "epoch 30 | loss: 3.86124 | train_balanced_accuracy: 0.0786  | valid_balanced_accuracy: 0.07746 |  0:34:17s\n",
      "epoch 31 | loss: 3.74738 | train_balanced_accuracy: 0.09974 | valid_balanced_accuracy: 0.10012 |  0:35:23s\n",
      "epoch 32 | loss: 3.64234 | train_balanced_accuracy: 0.09414 | valid_balanced_accuracy: 0.09572 |  0:36:30s\n",
      "epoch 33 | loss: 3.55743 | train_balanced_accuracy: 0.11174 | valid_balanced_accuracy: 0.11414 |  0:37:36s\n",
      "epoch 34 | loss: 3.46049 | train_balanced_accuracy: 0.0957  | valid_balanced_accuracy: 0.09745 |  0:38:43s\n",
      "epoch 35 | loss: 3.38039 | train_balanced_accuracy: 0.10804 | valid_balanced_accuracy: 0.10812 |  0:39:49s\n",
      "epoch 36 | loss: 3.31215 | train_balanced_accuracy: 0.12099 | valid_balanced_accuracy: 0.11945 |  0:40:55s\n",
      "epoch 37 | loss: 3.23871 | train_balanced_accuracy: 0.11069 | valid_balanced_accuracy: 0.11108 |  0:42:01s\n",
      "epoch 38 | loss: 3.18555 | train_balanced_accuracy: 0.11438 | valid_balanced_accuracy: 0.11086 |  0:43:06s\n",
      "epoch 39 | loss: 3.11557 | train_balanced_accuracy: 0.08865 | valid_balanced_accuracy: 0.0893  |  0:44:12s\n",
      "epoch 40 | loss: 3.05788 | train_balanced_accuracy: 0.12154 | valid_balanced_accuracy: 0.12018 |  0:45:18s\n",
      "epoch 41 | loss: 3.01082 | train_balanced_accuracy: 0.10866 | valid_balanced_accuracy: 0.10927 |  0:46:23s\n",
      "epoch 42 | loss: 2.95949 | train_balanced_accuracy: 0.11657 | valid_balanced_accuracy: 0.11535 |  0:47:29s\n",
      "epoch 43 | loss: 2.92009 | train_balanced_accuracy: 0.14338 | valid_balanced_accuracy: 0.14368 |  0:48:35s\n",
      "epoch 44 | loss: 2.86761 | train_balanced_accuracy: 0.10768 | valid_balanced_accuracy: 0.11256 |  0:49:42s\n",
      "epoch 45 | loss: 2.83839 | train_balanced_accuracy: 0.11846 | valid_balanced_accuracy: 0.118   |  0:50:49s\n",
      "epoch 46 | loss: 2.79297 | train_balanced_accuracy: 0.1334  | valid_balanced_accuracy: 0.13211 |  0:51:56s\n",
      "epoch 47 | loss: 2.75988 | train_balanced_accuracy: 0.15546 | valid_balanced_accuracy: 0.15502 |  0:53:03s\n",
      "epoch 48 | loss: 2.72138 | train_balanced_accuracy: 0.14215 | valid_balanced_accuracy: 0.14158 |  0:54:10s\n",
      "epoch 49 | loss: 2.69258 | train_balanced_accuracy: 0.12354 | valid_balanced_accuracy: 0.12357 |  0:55:16s\n",
      "epoch 50 | loss: 2.6661  | train_balanced_accuracy: 0.14679 | valid_balanced_accuracy: 0.14625 |  0:56:22s\n",
      "epoch 51 | loss: 2.63445 | train_balanced_accuracy: 0.11135 | valid_balanced_accuracy: 0.11072 |  0:57:28s\n",
      "epoch 52 | loss: 2.60968 | train_balanced_accuracy: 0.10925 | valid_balanced_accuracy: 0.1116  |  0:58:34s\n",
      "epoch 53 | loss: 2.57965 | train_balanced_accuracy: 0.13126 | valid_balanced_accuracy: 0.12943 |  0:59:40s\n",
      "epoch 54 | loss: 2.54357 | train_balanced_accuracy: 0.14573 | valid_balanced_accuracy: 0.14492 |  1:00:48s\n",
      "epoch 55 | loss: 2.51728 | train_balanced_accuracy: 0.13338 | valid_balanced_accuracy: 0.13466 |  1:01:55s\n",
      "epoch 56 | loss: 2.48933 | train_balanced_accuracy: 0.14035 | valid_balanced_accuracy: 0.14053 |  1:03:03s\n",
      "epoch 57 | loss: 2.46715 | train_balanced_accuracy: 0.15162 | valid_balanced_accuracy: 0.15037 |  1:04:10s\n",
      "\n",
      "Early stopping occured at epoch 57 with best_epoch = 47 and best_valid_balanced_accuracy = 0.15502\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'gamma': 4}, optimizer_params={'lr': 1e-05}, n_steps=13, n_shared=1, n_independent=3, n_a=64, momentum=1, lambda_sparse=0.0001, clip_value=0.5, total=64.5min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'gamma': 4}, optimizer_params={'lr': 1e-05}, n_steps=13, n_shared=1, n_independent=3, n_a=64, momentum=1, lambda_sparse=0.0001, clip_value=0.5 \n",
      "epoch 0  | loss: 21.19429| train_balanced_accuracy: 0.04005 | valid_balanced_accuracy: 0.0394  |  0:01:07s\n",
      "epoch 1  | loss: 20.18092| train_balanced_accuracy: 0.04226 | valid_balanced_accuracy: 0.04242 |  0:02:15s\n",
      "epoch 2  | loss: 19.10341| train_balanced_accuracy: 0.04206 | valid_balanced_accuracy: 0.04248 |  0:03:22s\n",
      "epoch 3  | loss: 17.92644| train_balanced_accuracy: 0.03309 | valid_balanced_accuracy: 0.03349 |  0:04:29s\n",
      "epoch 4  | loss: 16.87058| train_balanced_accuracy: 0.03674 | valid_balanced_accuracy: 0.03631 |  0:05:37s\n",
      "epoch 5  | loss: 15.81891| train_balanced_accuracy: 0.04618 | valid_balanced_accuracy: 0.047   |  0:06:44s\n",
      "epoch 6  | loss: 14.90358| train_balanced_accuracy: 0.04361 | valid_balanced_accuracy: 0.04432 |  0:07:51s\n",
      "epoch 7  | loss: 13.96615| train_balanced_accuracy: 0.04457 | valid_balanced_accuracy: 0.04481 |  0:08:58s\n",
      "epoch 8  | loss: 13.06297| train_balanced_accuracy: 0.03764 | valid_balanced_accuracy: 0.0368  |  0:10:06s\n",
      "epoch 9  | loss: 12.23099| train_balanced_accuracy: 0.03738 | valid_balanced_accuracy: 0.03685 |  0:11:13s\n",
      "epoch 10 | loss: 11.40173| train_balanced_accuracy: 0.03843 | valid_balanced_accuracy: 0.03917 |  0:12:20s\n",
      "epoch 11 | loss: 10.59895| train_balanced_accuracy: 0.0415  | valid_balanced_accuracy: 0.04147 |  0:13:27s\n",
      "epoch 12 | loss: 9.86795 | train_balanced_accuracy: 0.03589 | valid_balanced_accuracy: 0.03648 |  0:14:34s\n",
      "epoch 13 | loss: 9.1938  | train_balanced_accuracy: 0.0463  | valid_balanced_accuracy: 0.04998 |  0:15:41s\n",
      "epoch 14 | loss: 8.54598 | train_balanced_accuracy: 0.04225 | valid_balanced_accuracy: 0.04342 |  0:16:49s\n",
      "epoch 15 | loss: 7.99117 | train_balanced_accuracy: 0.0512  | valid_balanced_accuracy: 0.05083 |  0:17:56s\n",
      "epoch 16 | loss: 7.47894 | train_balanced_accuracy: 0.04949 | valid_balanced_accuracy: 0.04945 |  0:19:03s\n",
      "epoch 17 | loss: 7.00584 | train_balanced_accuracy: 0.06703 | valid_balanced_accuracy: 0.06592 |  0:20:11s\n",
      "epoch 18 | loss: 6.58868 | train_balanced_accuracy: 0.04875 | valid_balanced_accuracy: 0.04745 |  0:21:18s\n",
      "epoch 19 | loss: 6.22899 | train_balanced_accuracy: 0.05775 | valid_balanced_accuracy: 0.05757 |  0:22:25s\n",
      "epoch 20 | loss: 5.92703 | train_balanced_accuracy: 0.05492 | valid_balanced_accuracy: 0.05455 |  0:23:33s\n",
      "epoch 21 | loss: 5.6063  | train_balanced_accuracy: 0.06424 | valid_balanced_accuracy: 0.06514 |  0:24:40s\n",
      "epoch 22 | loss: 5.29822 | train_balanced_accuracy: 0.06721 | valid_balanced_accuracy: 0.06875 |  0:25:48s\n",
      "epoch 23 | loss: 5.00017 | train_balanced_accuracy: 0.06235 | valid_balanced_accuracy: 0.06222 |  0:26:54s\n",
      "epoch 24 | loss: 4.78234 | train_balanced_accuracy: 0.07914 | valid_balanced_accuracy: 0.07689 |  0:28:00s\n",
      "epoch 25 | loss: 4.57159 | train_balanced_accuracy: 0.06513 | valid_balanced_accuracy: 0.06621 |  0:29:06s\n",
      "epoch 26 | loss: 4.36403 | train_balanced_accuracy: 0.07946 | valid_balanced_accuracy: 0.08066 |  0:30:12s\n",
      "epoch 27 | loss: 4.19753 | train_balanced_accuracy: 0.07461 | valid_balanced_accuracy: 0.07555 |  0:31:18s\n",
      "epoch 28 | loss: 4.03322 | train_balanced_accuracy: 0.09726 | valid_balanced_accuracy: 0.09782 |  0:32:24s\n",
      "epoch 29 | loss: 3.89672 | train_balanced_accuracy: 0.09533 | valid_balanced_accuracy: 0.09632 |  0:33:30s\n",
      "epoch 30 | loss: 3.76766 | train_balanced_accuracy: 0.09426 | valid_balanced_accuracy: 0.09419 |  0:34:36s\n",
      "epoch 31 | loss: 3.66593 | train_balanced_accuracy: 0.12123 | valid_balanced_accuracy: 0.11786 |  0:35:43s\n",
      "epoch 32 | loss: 3.56414 | train_balanced_accuracy: 0.11019 | valid_balanced_accuracy: 0.11    |  0:36:49s\n",
      "epoch 33 | loss: 3.47941 | train_balanced_accuracy: 0.11626 | valid_balanced_accuracy: 0.11703 |  0:37:55s\n",
      "epoch 34 | loss: 3.39889 | train_balanced_accuracy: 0.13353 | valid_balanced_accuracy: 0.13111 |  0:39:01s\n",
      "epoch 35 | loss: 3.32304 | train_balanced_accuracy: 0.12939 | valid_balanced_accuracy: 0.13017 |  0:40:07s\n",
      "epoch 36 | loss: 3.26782 | train_balanced_accuracy: 0.11829 | valid_balanced_accuracy: 0.11591 |  0:41:13s\n",
      "epoch 37 | loss: 3.20119 | train_balanced_accuracy: 0.12726 | valid_balanced_accuracy: 0.12586 |  0:42:20s\n",
      "epoch 38 | loss: 3.14522 | train_balanced_accuracy: 0.1208  | valid_balanced_accuracy: 0.11825 |  0:43:26s\n",
      "epoch 39 | loss: 3.09184 | train_balanced_accuracy: 0.1184  | valid_balanced_accuracy: 0.11819 |  0:44:32s\n",
      "epoch 40 | loss: 3.03262 | train_balanced_accuracy: 0.12428 | valid_balanced_accuracy: 0.12125 |  0:45:39s\n",
      "epoch 41 | loss: 2.97861 | train_balanced_accuracy: 0.1169  | valid_balanced_accuracy: 0.11677 |  0:46:45s\n",
      "epoch 42 | loss: 2.92859 | train_balanced_accuracy: 0.13415 | valid_balanced_accuracy: 0.13291 |  0:47:52s\n",
      "epoch 43 | loss: 2.88365 | train_balanced_accuracy: 0.14936 | valid_balanced_accuracy: 0.14897 |  0:48:58s\n",
      "epoch 44 | loss: 2.84615 | train_balanced_accuracy: 0.13909 | valid_balanced_accuracy: 0.13855 |  0:50:06s\n",
      "epoch 45 | loss: 2.80878 | train_balanced_accuracy: 0.12968 | valid_balanced_accuracy: 0.13026 |  0:51:15s\n",
      "epoch 46 | loss: 2.77669 | train_balanced_accuracy: 0.13828 | valid_balanced_accuracy: 0.13712 |  0:52:23s\n",
      "epoch 47 | loss: 2.74068 | train_balanced_accuracy: 0.13434 | valid_balanced_accuracy: 0.13416 |  0:53:32s\n",
      "epoch 48 | loss: 2.70969 | train_balanced_accuracy: 0.14782 | valid_balanced_accuracy: 0.14576 |  0:54:40s\n",
      "epoch 49 | loss: 2.67973 | train_balanced_accuracy: 0.13783 | valid_balanced_accuracy: 0.13919 |  0:55:49s\n",
      "epoch 50 | loss: 2.65287 | train_balanced_accuracy: 0.15423 | valid_balanced_accuracy: 0.15381 |  0:56:57s\n",
      "epoch 51 | loss: 2.61161 | train_balanced_accuracy: 0.13617 | valid_balanced_accuracy: 0.13322 |  0:58:06s\n",
      "epoch 52 | loss: 2.58963 | train_balanced_accuracy: 0.10888 | valid_balanced_accuracy: 0.10968 |  0:59:14s\n",
      "epoch 53 | loss: 2.56485 | train_balanced_accuracy: 0.1359  | valid_balanced_accuracy: 0.13423 |  1:00:23s\n",
      "epoch 54 | loss: 2.5364  | train_balanced_accuracy: 0.15985 | valid_balanced_accuracy: 0.16131 |  1:01:31s\n",
      "epoch 55 | loss: 2.51487 | train_balanced_accuracy: 0.14483 | valid_balanced_accuracy: 0.14418 |  1:02:39s\n",
      "epoch 56 | loss: 2.49293 | train_balanced_accuracy: 0.15318 | valid_balanced_accuracy: 0.15096 |  1:03:47s\n",
      "epoch 57 | loss: 2.47208 | train_balanced_accuracy: 0.15811 | valid_balanced_accuracy: 0.15644 |  1:04:55s\n",
      "epoch 58 | loss: 2.44814 | train_balanced_accuracy: 0.16006 | valid_balanced_accuracy: 0.16003 |  1:06:03s\n",
      "epoch 59 | loss: 2.43206 | train_balanced_accuracy: 0.16251 | valid_balanced_accuracy: 0.16387 |  1:07:11s\n",
      "epoch 60 | loss: 2.4138  | train_balanced_accuracy: 0.14415 | valid_balanced_accuracy: 0.1446  |  1:08:20s\n",
      "epoch 61 | loss: 2.39008 | train_balanced_accuracy: 0.15954 | valid_balanced_accuracy: 0.16017 |  1:09:28s\n",
      "epoch 62 | loss: 2.38371 | train_balanced_accuracy: 0.15154 | valid_balanced_accuracy: 0.15096 |  1:10:36s\n",
      "epoch 63 | loss: 2.36159 | train_balanced_accuracy: 0.16072 | valid_balanced_accuracy: 0.16172 |  1:11:43s\n",
      "epoch 64 | loss: 2.34343 | train_balanced_accuracy: 0.15488 | valid_balanced_accuracy: 0.15279 |  1:12:49s\n",
      "epoch 65 | loss: 2.31984 | train_balanced_accuracy: 0.16269 | valid_balanced_accuracy: 0.16494 |  1:13:55s\n",
      "epoch 66 | loss: 2.30487 | train_balanced_accuracy: 0.16496 | valid_balanced_accuracy: 0.16491 |  1:15:02s\n",
      "epoch 67 | loss: 2.29742 | train_balanced_accuracy: 0.15649 | valid_balanced_accuracy: 0.15395 |  1:16:09s\n",
      "epoch 68 | loss: 2.2799  | train_balanced_accuracy: 0.16707 | valid_balanced_accuracy: 0.16802 |  1:17:15s\n",
      "epoch 69 | loss: 2.26662 | train_balanced_accuracy: 0.1783  | valid_balanced_accuracy: 0.17682 |  1:18:22s\n",
      "epoch 70 | loss: 2.25984 | train_balanced_accuracy: 0.14432 | valid_balanced_accuracy: 0.14586 |  1:19:28s\n",
      "epoch 71 | loss: 2.24427 | train_balanced_accuracy: 0.17061 | valid_balanced_accuracy: 0.17078 |  1:20:34s\n",
      "epoch 72 | loss: 2.23346 | train_balanced_accuracy: 0.17535 | valid_balanced_accuracy: 0.17256 |  1:21:40s\n",
      "epoch 73 | loss: 2.22043 | train_balanced_accuracy: 0.15168 | valid_balanced_accuracy: 0.15231 |  1:22:47s\n",
      "epoch 74 | loss: 2.20864 | train_balanced_accuracy: 0.148   | valid_balanced_accuracy: 0.1457  |  1:23:53s\n",
      "epoch 75 | loss: 2.20076 | train_balanced_accuracy: 0.17467 | valid_balanced_accuracy: 0.16834 |  1:24:59s\n",
      "epoch 76 | loss: 2.18895 | train_balanced_accuracy: 0.17745 | valid_balanced_accuracy: 0.17553 |  1:26:06s\n",
      "epoch 77 | loss: 2.17926 | train_balanced_accuracy: 0.16601 | valid_balanced_accuracy: 0.16345 |  1:27:13s\n",
      "epoch 78 | loss: 2.17019 | train_balanced_accuracy: 0.16339 | valid_balanced_accuracy: 0.16401 |  1:28:20s\n",
      "epoch 79 | loss: 2.17151 | train_balanced_accuracy: 0.17927 | valid_balanced_accuracy: 0.17908 |  1:29:27s\n",
      "epoch 80 | loss: 2.16411 | train_balanced_accuracy: 0.17622 | valid_balanced_accuracy: 0.17585 |  1:30:34s\n",
      "epoch 81 | loss: 2.15395 | train_balanced_accuracy: 0.16781 | valid_balanced_accuracy: 0.16782 |  1:31:41s\n",
      "epoch 82 | loss: 2.14129 | train_balanced_accuracy: 0.18544 | valid_balanced_accuracy: 0.18479 |  1:32:47s\n",
      "epoch 83 | loss: 2.14046 | train_balanced_accuracy: 0.17344 | valid_balanced_accuracy: 0.1747  |  1:33:53s\n",
      "epoch 84 | loss: 2.13144 | train_balanced_accuracy: 0.16779 | valid_balanced_accuracy: 0.16808 |  1:34:59s\n",
      "epoch 85 | loss: 2.12462 | train_balanced_accuracy: 0.17393 | valid_balanced_accuracy: 0.17223 |  1:36:05s\n",
      "epoch 86 | loss: 2.11929 | train_balanced_accuracy: 0.15    | valid_balanced_accuracy: 0.14898 |  1:37:11s\n",
      "epoch 87 | loss: 2.10987 | train_balanced_accuracy: 0.16713 | valid_balanced_accuracy: 0.16719 |  1:38:17s\n",
      "epoch 88 | loss: 2.10343 | train_balanced_accuracy: 0.17646 | valid_balanced_accuracy: 0.17532 |  1:39:25s\n",
      "epoch 89 | loss: 2.09677 | train_balanced_accuracy: 0.16455 | valid_balanced_accuracy: 0.1676  |  1:40:33s\n",
      "epoch 90 | loss: 2.08513 | train_balanced_accuracy: 0.1803  | valid_balanced_accuracy: 0.17912 |  1:41:41s\n",
      "epoch 91 | loss: 2.08262 | train_balanced_accuracy: 0.17198 | valid_balanced_accuracy: 0.17179 |  1:42:49s\n",
      "epoch 92 | loss: 2.07347 | train_balanced_accuracy: 0.17316 | valid_balanced_accuracy: 0.17092 |  1:43:56s\n",
      "\n",
      "Early stopping occured at epoch 92 with best_epoch = 82 and best_valid_balanced_accuracy = 0.18479\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'gamma': 4}, optimizer_params={'lr': 1e-05}, n_steps=13, n_shared=1, n_independent=3, n_a=64, momentum=1, lambda_sparse=0.0001, clip_value=0.5, total=104.3min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'gamma': 4}, optimizer_params={'lr': 1e-05}, n_steps=13, n_shared=1, n_independent=3, n_a=64, momentum=1, lambda_sparse=0.0001, clip_value=0.5 \n",
      "epoch 0  | loss: 21.2195 | train_balanced_accuracy: 0.04239 | valid_balanced_accuracy: 0.04264 |  0:01:06s\n",
      "epoch 1  | loss: 20.12777| train_balanced_accuracy: 0.04245 | valid_balanced_accuracy: 0.04204 |  0:02:12s\n",
      "epoch 2  | loss: 19.14388| train_balanced_accuracy: 0.04733 | valid_balanced_accuracy: 0.04503 |  0:03:19s\n",
      "epoch 3  | loss: 18.09532| train_balanced_accuracy: 0.03916 | valid_balanced_accuracy: 0.03831 |  0:04:25s\n",
      "epoch 4  | loss: 17.05406| train_balanced_accuracy: 0.03716 | valid_balanced_accuracy: 0.03798 |  0:05:31s\n",
      "epoch 5  | loss: 16.07835| train_balanced_accuracy: 0.04408 | valid_balanced_accuracy: 0.0451  |  0:06:38s\n",
      "epoch 6  | loss: 15.06493| train_balanced_accuracy: 0.03631 | valid_balanced_accuracy: 0.03647 |  0:07:44s\n",
      "epoch 7  | loss: 14.14561| train_balanced_accuracy: 0.03933 | valid_balanced_accuracy: 0.03962 |  0:08:50s\n",
      "epoch 8  | loss: 13.21221| train_balanced_accuracy: 0.04275 | valid_balanced_accuracy: 0.04317 |  0:09:57s\n",
      "epoch 9  | loss: 12.36062| train_balanced_accuracy: 0.03949 | valid_balanced_accuracy: 0.0388  |  0:11:05s\n",
      "epoch 10 | loss: 11.42852| train_balanced_accuracy: 0.04568 | valid_balanced_accuracy: 0.04645 |  0:12:13s\n",
      "epoch 11 | loss: 10.61108| train_balanced_accuracy: 0.04052 | valid_balanced_accuracy: 0.04066 |  0:13:21s\n",
      "epoch 12 | loss: 9.9991  | train_balanced_accuracy: 0.03678 | valid_balanced_accuracy: 0.03765 |  0:14:30s\n",
      "epoch 13 | loss: 9.33159 | train_balanced_accuracy: 0.04155 | valid_balanced_accuracy: 0.0433  |  0:15:37s\n",
      "epoch 14 | loss: 8.65318 | train_balanced_accuracy: 0.04797 | valid_balanced_accuracy: 0.04703 |  0:16:44s\n",
      "epoch 15 | loss: 8.00539 | train_balanced_accuracy: 0.04896 | valid_balanced_accuracy: 0.04896 |  0:17:50s\n",
      "epoch 16 | loss: 7.47218 | train_balanced_accuracy: 0.04398 | valid_balanced_accuracy: 0.04557 |  0:18:56s\n",
      "epoch 17 | loss: 7.02285 | train_balanced_accuracy: 0.05171 | valid_balanced_accuracy: 0.05296 |  0:20:02s\n",
      "epoch 18 | loss: 6.54159 | train_balanced_accuracy: 0.04508 | valid_balanced_accuracy: 0.04433 |  0:21:08s\n",
      "epoch 19 | loss: 6.14022 | train_balanced_accuracy: 0.05223 | valid_balanced_accuracy: 0.05126 |  0:22:14s\n",
      "epoch 20 | loss: 5.81802 | train_balanced_accuracy: 0.0563  | valid_balanced_accuracy: 0.05632 |  0:23:21s\n",
      "epoch 21 | loss: 5.51212 | train_balanced_accuracy: 0.07658 | valid_balanced_accuracy: 0.07755 |  0:24:27s\n",
      "epoch 22 | loss: 5.20992 | train_balanced_accuracy: 0.0606  | valid_balanced_accuracy: 0.06009 |  0:25:34s\n",
      "epoch 23 | loss: 4.97705 | train_balanced_accuracy: 0.07115 | valid_balanced_accuracy: 0.07203 |  0:26:40s\n",
      "epoch 24 | loss: 4.77075 | train_balanced_accuracy: 0.07256 | valid_balanced_accuracy: 0.07337 |  0:27:46s\n",
      "epoch 25 | loss: 4.55422 | train_balanced_accuracy: 0.06041 | valid_balanced_accuracy: 0.06019 |  0:28:53s\n",
      "epoch 26 | loss: 4.37102 | train_balanced_accuracy: 0.06939 | valid_balanced_accuracy: 0.06904 |  0:29:59s\n",
      "epoch 27 | loss: 4.20843 | train_balanced_accuracy: 0.08257 | valid_balanced_accuracy: 0.08255 |  0:31:05s\n",
      "epoch 28 | loss: 4.05248 | train_balanced_accuracy: 0.07686 | valid_balanced_accuracy: 0.0752  |  0:32:12s\n",
      "epoch 29 | loss: 3.92163 | train_balanced_accuracy: 0.09083 | valid_balanced_accuracy: 0.09049 |  0:33:19s\n",
      "epoch 30 | loss: 3.78763 | train_balanced_accuracy: 0.07113 | valid_balanced_accuracy: 0.07276 |  0:34:26s\n",
      "epoch 31 | loss: 3.67918 | train_balanced_accuracy: 0.12639 | valid_balanced_accuracy: 0.1243  |  0:35:32s\n",
      "epoch 32 | loss: 3.58518 | train_balanced_accuracy: 0.09878 | valid_balanced_accuracy: 0.09827 |  0:36:38s\n",
      "epoch 33 | loss: 3.49927 | train_balanced_accuracy: 0.10324 | valid_balanced_accuracy: 0.10252 |  0:37:44s\n",
      "epoch 34 | loss: 3.4187  | train_balanced_accuracy: 0.11454 | valid_balanced_accuracy: 0.11126 |  0:38:50s\n",
      "epoch 35 | loss: 3.33487 | train_balanced_accuracy: 0.12006 | valid_balanced_accuracy: 0.1181  |  0:39:56s\n",
      "epoch 36 | loss: 3.2607  | train_balanced_accuracy: 0.10951 | valid_balanced_accuracy: 0.10763 |  0:41:02s\n",
      "epoch 37 | loss: 3.19007 | train_balanced_accuracy: 0.12892 | valid_balanced_accuracy: 0.12803 |  0:42:08s\n",
      "epoch 38 | loss: 3.13357 | train_balanced_accuracy: 0.12602 | valid_balanced_accuracy: 0.12402 |  0:43:14s\n",
      "epoch 39 | loss: 3.06991 | train_balanced_accuracy: 0.11146 | valid_balanced_accuracy: 0.11192 |  0:44:20s\n",
      "epoch 40 | loss: 3.02218 | train_balanced_accuracy: 0.13872 | valid_balanced_accuracy: 0.13922 |  0:45:26s\n",
      "epoch 41 | loss: 2.96359 | train_balanced_accuracy: 0.10655 | valid_balanced_accuracy: 0.10609 |  0:46:32s\n",
      "epoch 42 | loss: 2.91875 | train_balanced_accuracy: 0.13939 | valid_balanced_accuracy: 0.13789 |  0:47:39s\n",
      "epoch 43 | loss: 2.88357 | train_balanced_accuracy: 0.14483 | valid_balanced_accuracy: 0.14314 |  0:48:45s\n",
      "epoch 44 | loss: 2.84292 | train_balanced_accuracy: 0.12941 | valid_balanced_accuracy: 0.12778 |  0:49:51s\n",
      "epoch 45 | loss: 2.81108 | train_balanced_accuracy: 0.13124 | valid_balanced_accuracy: 0.12951 |  0:50:57s\n",
      "epoch 46 | loss: 2.76728 | train_balanced_accuracy: 0.13884 | valid_balanced_accuracy: 0.13671 |  0:52:05s\n",
      "epoch 47 | loss: 2.72629 | train_balanced_accuracy: 0.12498 | valid_balanced_accuracy: 0.12364 |  0:53:13s\n",
      "epoch 48 | loss: 2.70237 | train_balanced_accuracy: 0.15277 | valid_balanced_accuracy: 0.1516  |  0:54:21s\n",
      "epoch 49 | loss: 2.66716 | train_balanced_accuracy: 0.14306 | valid_balanced_accuracy: 0.14191 |  0:55:29s\n",
      "epoch 50 | loss: 2.64122 | train_balanced_accuracy: 0.14016 | valid_balanced_accuracy: 0.1387  |  0:56:36s\n",
      "epoch 51 | loss: 2.61422 | train_balanced_accuracy: 0.14982 | valid_balanced_accuracy: 0.14733 |  0:57:43s\n",
      "epoch 52 | loss: 2.59278 | train_balanced_accuracy: 0.13149 | valid_balanced_accuracy: 0.13088 |  0:58:50s\n",
      "epoch 53 | loss: 2.56371 | train_balanced_accuracy: 0.14404 | valid_balanced_accuracy: 0.1418  |  0:59:56s\n",
      "epoch 54 | loss: 2.53996 | train_balanced_accuracy: 0.12523 | valid_balanced_accuracy: 0.12156 |  1:01:02s\n",
      "epoch 55 | loss: 2.51491 | train_balanced_accuracy: 0.14004 | valid_balanced_accuracy: 0.13937 |  1:02:09s\n",
      "epoch 56 | loss: 2.48867 | train_balanced_accuracy: 0.1544  | valid_balanced_accuracy: 0.15015 |  1:03:15s\n",
      "epoch 57 | loss: 2.4586  | train_balanced_accuracy: 0.14411 | valid_balanced_accuracy: 0.14133 |  1:04:21s\n",
      "epoch 58 | loss: 2.44556 | train_balanced_accuracy: 0.13733 | valid_balanced_accuracy: 0.13291 |  1:05:27s\n",
      "\n",
      "Early stopping occured at epoch 58 with best_epoch = 48 and best_valid_balanced_accuracy = 0.1516\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'gamma': 4}, optimizer_params={'lr': 1e-05}, n_steps=13, n_shared=1, n_independent=3, n_a=64, momentum=1, lambda_sparse=0.0001, clip_value=0.5, total=65.8min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'gamma': 4}, optimizer_params={'lr': 1e-05}, n_steps=13, n_shared=1, n_independent=3, n_a=64, momentum=1, lambda_sparse=0.0001, clip_value=0.5 \n",
      "epoch 0  | loss: 21.17828| train_balanced_accuracy: 0.04077 | valid_balanced_accuracy: 0.04134 |  0:01:06s\n",
      "epoch 1  | loss: 20.07085| train_balanced_accuracy: 0.0419  | valid_balanced_accuracy: 0.04128 |  0:02:13s\n",
      "epoch 2  | loss: 18.92465| train_balanced_accuracy: 0.04918 | valid_balanced_accuracy: 0.0484  |  0:03:20s\n",
      "epoch 3  | loss: 17.80872| train_balanced_accuracy: 0.03953 | valid_balanced_accuracy: 0.0406  |  0:04:25s\n",
      "epoch 4  | loss: 16.66028| train_balanced_accuracy: 0.04306 | valid_balanced_accuracy: 0.04319 |  0:05:31s\n",
      "epoch 5  | loss: 15.73859| train_balanced_accuracy: 0.04576 | valid_balanced_accuracy: 0.04723 |  0:06:37s\n",
      "epoch 6  | loss: 14.82281| train_balanced_accuracy: 0.04098 | valid_balanced_accuracy: 0.04389 |  0:07:43s\n",
      "epoch 7  | loss: 13.89818| train_balanced_accuracy: 0.0382  | valid_balanced_accuracy: 0.03805 |  0:08:48s\n",
      "epoch 8  | loss: 13.05973| train_balanced_accuracy: 0.03458 | valid_balanced_accuracy: 0.03684 |  0:09:54s\n",
      "epoch 9  | loss: 12.29132| train_balanced_accuracy: 0.04337 | valid_balanced_accuracy: 0.04332 |  0:11:00s\n",
      "epoch 10 | loss: 11.5542 | train_balanced_accuracy: 0.04388 | valid_balanced_accuracy: 0.04464 |  0:12:05s\n",
      "epoch 11 | loss: 10.81292| train_balanced_accuracy: 0.03856 | valid_balanced_accuracy: 0.03731 |  0:13:11s\n",
      "epoch 12 | loss: 10.14989| train_balanced_accuracy: 0.04074 | valid_balanced_accuracy: 0.04119 |  0:14:17s\n",
      "\n",
      "Early stopping occured at epoch 12 with best_epoch = 2 and best_valid_balanced_accuracy = 0.0484\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'gamma': 4}, optimizer_params={'lr': 1e-05}, n_steps=13, n_shared=1, n_independent=3, n_a=64, momentum=1, lambda_sparse=0.0001, clip_value=0.5, total=14.6min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'step_size': 100}, optimizer_params={'lr': 0.001}, n_steps=5, n_shared=4, n_independent=9, n_a=64, momentum=0.02, lambda_sparse=0.0001, clip_value=0.5 \n",
      "epoch 0  | loss: 2.80453 | train_balanced_accuracy: 0.15243 | valid_balanced_accuracy: 0.15293 |  0:01:12s\n",
      "epoch 1  | loss: 1.8118  | train_balanced_accuracy: 0.21649 | valid_balanced_accuracy: 0.2182  |  0:02:25s\n",
      "epoch 2  | loss: 1.55679 | train_balanced_accuracy: 0.29129 | valid_balanced_accuracy: 0.29152 |  0:03:39s\n",
      "epoch 3  | loss: 1.42228 | train_balanced_accuracy: 0.37522 | valid_balanced_accuracy: 0.36766 |  0:04:53s\n",
      "epoch 4  | loss: 1.33776 | train_balanced_accuracy: 0.44061 | valid_balanced_accuracy: 0.43153 |  0:06:07s\n",
      "epoch 5  | loss: 1.28372 | train_balanced_accuracy: 0.45903 | valid_balanced_accuracy: 0.45752 |  0:07:21s\n",
      "epoch 6  | loss: 1.24286 | train_balanced_accuracy: 0.50186 | valid_balanced_accuracy: 0.49405 |  0:08:36s\n",
      "epoch 7  | loss: 1.21878 | train_balanced_accuracy: 0.50071 | valid_balanced_accuracy: 0.49764 |  0:09:50s\n",
      "epoch 8  | loss: 1.19688 | train_balanced_accuracy: 0.52173 | valid_balanced_accuracy: 0.51671 |  0:11:03s\n",
      "epoch 9  | loss: 1.17855 | train_balanced_accuracy: 0.54709 | valid_balanced_accuracy: 0.5388  |  0:12:15s\n",
      "epoch 10 | loss: 1.15485 | train_balanced_accuracy: 0.5468  | valid_balanced_accuracy: 0.53397 |  0:13:28s\n",
      "epoch 11 | loss: 1.14023 | train_balanced_accuracy: 0.54238 | valid_balanced_accuracy: 0.53871 |  0:14:40s\n",
      "epoch 12 | loss: 1.12802 | train_balanced_accuracy: 0.56038 | valid_balanced_accuracy: 0.54683 |  0:15:53s\n",
      "epoch 13 | loss: 1.12063 | train_balanced_accuracy: 0.56841 | valid_balanced_accuracy: 0.56458 |  0:17:06s\n",
      "epoch 14 | loss: 1.11245 | train_balanced_accuracy: 0.56093 | valid_balanced_accuracy: 0.55213 |  0:18:19s\n",
      "epoch 15 | loss: 1.10846 | train_balanced_accuracy: 0.57404 | valid_balanced_accuracy: 0.56248 |  0:19:32s\n",
      "epoch 16 | loss: 1.09808 | train_balanced_accuracy: 0.57529 | valid_balanced_accuracy: 0.56401 |  0:20:44s\n",
      "epoch 17 | loss: 1.08961 | train_balanced_accuracy: 0.58533 | valid_balanced_accuracy: 0.5626  |  0:21:57s\n",
      "epoch 18 | loss: 1.07978 | train_balanced_accuracy: 0.57256 | valid_balanced_accuracy: 0.56083 |  0:23:09s\n",
      "epoch 19 | loss: 1.07197 | train_balanced_accuracy: 0.61523 | valid_balanced_accuracy: 0.60582 |  0:24:21s\n",
      "epoch 20 | loss: 1.06569 | train_balanced_accuracy: 0.58349 | valid_balanced_accuracy: 0.57709 |  0:25:35s\n",
      "epoch 21 | loss: 1.05534 | train_balanced_accuracy: 0.61442 | valid_balanced_accuracy: 0.60179 |  0:26:50s\n",
      "epoch 22 | loss: 1.05064 | train_balanced_accuracy: 0.60866 | valid_balanced_accuracy: 0.59475 |  0:28:04s\n",
      "epoch 23 | loss: 1.04517 | train_balanced_accuracy: 0.60162 | valid_balanced_accuracy: 0.58599 |  0:29:18s\n",
      "epoch 24 | loss: 1.04916 | train_balanced_accuracy: 0.61797 | valid_balanced_accuracy: 0.59243 |  0:30:31s\n",
      "epoch 25 | loss: 1.03888 | train_balanced_accuracy: 0.61511 | valid_balanced_accuracy: 0.59931 |  0:31:43s\n",
      "epoch 26 | loss: 1.03441 | train_balanced_accuracy: 0.61772 | valid_balanced_accuracy: 0.60275 |  0:32:56s\n",
      "epoch 27 | loss: 1.02883 | train_balanced_accuracy: 0.62058 | valid_balanced_accuracy: 0.59653 |  0:34:08s\n",
      "epoch 28 | loss: 1.03058 | train_balanced_accuracy: 0.6114  | valid_balanced_accuracy: 0.59395 |  0:35:20s\n",
      "epoch 29 | loss: 1.03282 | train_balanced_accuracy: 0.61305 | valid_balanced_accuracy: 0.58623 |  0:36:32s\n",
      "\n",
      "Early stopping occured at epoch 29 with best_epoch = 19 and best_valid_balanced_accuracy = 0.60582\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'step_size': 100}, optimizer_params={'lr': 0.001}, n_steps=5, n_shared=4, n_independent=9, n_a=64, momentum=0.02, lambda_sparse=0.0001, clip_value=0.5, total=36.9min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'step_size': 100}, optimizer_params={'lr': 0.001}, n_steps=5, n_shared=4, n_independent=9, n_a=64, momentum=0.02, lambda_sparse=0.0001, clip_value=0.5 \n",
      "epoch 0  | loss: 2.74682 | train_balanced_accuracy: 0.17858 | valid_balanced_accuracy: 0.18058 |  0:01:12s\n",
      "epoch 1  | loss: 1.76077 | train_balanced_accuracy: 0.23518 | valid_balanced_accuracy: 0.23516 |  0:02:24s\n",
      "epoch 2  | loss: 1.52101 | train_balanced_accuracy: 0.31878 | valid_balanced_accuracy: 0.32989 |  0:03:36s\n",
      "epoch 3  | loss: 1.38403 | train_balanced_accuracy: 0.4072  | valid_balanced_accuracy: 0.41161 |  0:04:48s\n",
      "epoch 4  | loss: 1.30419 | train_balanced_accuracy: 0.45707 | valid_balanced_accuracy: 0.44706 |  0:06:00s\n",
      "epoch 5  | loss: 1.26562 | train_balanced_accuracy: 0.48552 | valid_balanced_accuracy: 0.47481 |  0:07:12s\n",
      "epoch 6  | loss: 1.22536 | train_balanced_accuracy: 0.49691 | valid_balanced_accuracy: 0.48147 |  0:08:24s\n",
      "epoch 7  | loss: 1.19678 | train_balanced_accuracy: 0.4843  | valid_balanced_accuracy: 0.47    |  0:09:36s\n",
      "epoch 8  | loss: 1.17151 | train_balanced_accuracy: 0.51327 | valid_balanced_accuracy: 0.49634 |  0:10:48s\n",
      "epoch 9  | loss: 1.15416 | train_balanced_accuracy: 0.53422 | valid_balanced_accuracy: 0.52186 |  0:12:00s\n",
      "epoch 10 | loss: 1.1449  | train_balanced_accuracy: 0.54947 | valid_balanced_accuracy: 0.5294  |  0:13:12s\n",
      "epoch 11 | loss: 1.12231 | train_balanced_accuracy: 0.55408 | valid_balanced_accuracy: 0.54194 |  0:14:24s\n",
      "epoch 12 | loss: 1.10742 | train_balanced_accuracy: 0.56805 | valid_balanced_accuracy: 0.54671 |  0:15:37s\n",
      "epoch 13 | loss: 1.10256 | train_balanced_accuracy: 0.5502  | valid_balanced_accuracy: 0.5297  |  0:16:49s\n",
      "epoch 14 | loss: 1.09621 | train_balanced_accuracy: 0.57531 | valid_balanced_accuracy: 0.5575  |  0:18:01s\n",
      "epoch 15 | loss: 1.07805 | train_balanced_accuracy: 0.57826 | valid_balanced_accuracy: 0.56578 |  0:19:15s\n",
      "epoch 16 | loss: 1.06716 | train_balanced_accuracy: 0.56417 | valid_balanced_accuracy: 0.53693 |  0:20:29s\n",
      "epoch 17 | loss: 1.06092 | train_balanced_accuracy: 0.59594 | valid_balanced_accuracy: 0.58738 |  0:21:43s\n",
      "epoch 18 | loss: 1.0507  | train_balanced_accuracy: 0.59706 | valid_balanced_accuracy: 0.57943 |  0:22:57s\n",
      "epoch 19 | loss: 1.04666 | train_balanced_accuracy: 0.61302 | valid_balanced_accuracy: 0.59436 |  0:24:09s\n",
      "epoch 20 | loss: 1.03808 | train_balanced_accuracy: 0.59054 | valid_balanced_accuracy: 0.56973 |  0:25:22s\n",
      "epoch 21 | loss: 1.03737 | train_balanced_accuracy: 0.61206 | valid_balanced_accuracy: 0.58817 |  0:26:35s\n",
      "epoch 22 | loss: 1.02645 | train_balanced_accuracy: 0.61545 | valid_balanced_accuracy: 0.59418 |  0:27:48s\n",
      "epoch 23 | loss: 1.02225 | train_balanced_accuracy: 0.61661 | valid_balanced_accuracy: 0.59824 |  0:29:01s\n",
      "epoch 24 | loss: 1.01952 | train_balanced_accuracy: 0.6065  | valid_balanced_accuracy: 0.58408 |  0:30:15s\n",
      "epoch 25 | loss: 1.01139 | train_balanced_accuracy: 0.62929 | valid_balanced_accuracy: 0.60209 |  0:31:29s\n",
      "epoch 26 | loss: 1.00467 | train_balanced_accuracy: 0.63336 | valid_balanced_accuracy: 0.60459 |  0:32:42s\n",
      "epoch 27 | loss: 1.00268 | train_balanced_accuracy: 0.6231  | valid_balanced_accuracy: 0.5999  |  0:33:55s\n",
      "epoch 28 | loss: 0.99573 | train_balanced_accuracy: 0.61874 | valid_balanced_accuracy: 0.591   |  0:35:07s\n",
      "epoch 29 | loss: 0.98462 | train_balanced_accuracy: 0.63358 | valid_balanced_accuracy: 0.60533 |  0:36:20s\n",
      "epoch 30 | loss: 0.97489 | train_balanced_accuracy: 0.63246 | valid_balanced_accuracy: 0.60511 |  0:37:35s\n",
      "epoch 31 | loss: 0.96627 | train_balanced_accuracy: 0.65449 | valid_balanced_accuracy: 0.62533 |  0:38:49s\n",
      "epoch 32 | loss: 0.9622  | train_balanced_accuracy: 0.65392 | valid_balanced_accuracy: 0.62304 |  0:40:05s\n",
      "epoch 33 | loss: 0.96589 | train_balanced_accuracy: 0.64872 | valid_balanced_accuracy: 0.61882 |  0:41:19s\n",
      "epoch 34 | loss: 0.9596  | train_balanced_accuracy: 0.65641 | valid_balanced_accuracy: 0.62793 |  0:42:34s\n",
      "epoch 35 | loss: 0.95141 | train_balanced_accuracy: 0.65178 | valid_balanced_accuracy: 0.62207 |  0:43:49s\n",
      "epoch 36 | loss: 0.95382 | train_balanced_accuracy: 0.65309 | valid_balanced_accuracy: 0.63193 |  0:45:05s\n",
      "epoch 37 | loss: 0.94797 | train_balanced_accuracy: 0.66416 | valid_balanced_accuracy: 0.62958 |  0:46:20s\n",
      "epoch 38 | loss: 0.94681 | train_balanced_accuracy: 0.66045 | valid_balanced_accuracy: 0.6284  |  0:47:35s\n",
      "epoch 39 | loss: 0.94051 | train_balanced_accuracy: 0.64729 | valid_balanced_accuracy: 0.61532 |  0:48:50s\n",
      "epoch 40 | loss: 0.93188 | train_balanced_accuracy: 0.66015 | valid_balanced_accuracy: 0.62822 |  0:50:04s\n",
      "epoch 41 | loss: 0.92402 | train_balanced_accuracy: 0.66636 | valid_balanced_accuracy: 0.63778 |  0:51:19s\n",
      "epoch 42 | loss: 0.91842 | train_balanced_accuracy: 0.6679  | valid_balanced_accuracy: 0.6364  |  0:52:35s\n",
      "epoch 43 | loss: 0.91512 | train_balanced_accuracy: 0.67356 | valid_balanced_accuracy: 0.64093 |  0:53:50s\n",
      "epoch 44 | loss: 0.91074 | train_balanced_accuracy: 0.66655 | valid_balanced_accuracy: 0.63673 |  0:55:05s\n",
      "epoch 45 | loss: 0.90362 | train_balanced_accuracy: 0.67292 | valid_balanced_accuracy: 0.64213 |  0:56:20s\n",
      "epoch 46 | loss: 0.90598 | train_balanced_accuracy: 0.67704 | valid_balanced_accuracy: 0.64428 |  0:57:36s\n",
      "epoch 47 | loss: 0.89855 | train_balanced_accuracy: 0.67739 | valid_balanced_accuracy: 0.64853 |  0:58:51s\n",
      "epoch 48 | loss: 0.89615 | train_balanced_accuracy: 0.6686  | valid_balanced_accuracy: 0.63093 |  1:00:07s\n",
      "epoch 49 | loss: 0.89163 | train_balanced_accuracy: 0.67135 | valid_balanced_accuracy: 0.64168 |  1:01:22s\n",
      "epoch 50 | loss: 0.89082 | train_balanced_accuracy: 0.67752 | valid_balanced_accuracy: 0.64244 |  1:02:38s\n",
      "epoch 51 | loss: 0.89978 | train_balanced_accuracy: 0.68137 | valid_balanced_accuracy: 0.641   |  1:03:53s\n",
      "epoch 52 | loss: 0.8846  | train_balanced_accuracy: 0.68158 | valid_balanced_accuracy: 0.6427  |  1:05:09s\n",
      "epoch 53 | loss: 0.87824 | train_balanced_accuracy: 0.67311 | valid_balanced_accuracy: 0.63597 |  1:06:24s\n",
      "epoch 54 | loss: 0.87592 | train_balanced_accuracy: 0.69045 | valid_balanced_accuracy: 0.65354 |  1:07:39s\n",
      "epoch 55 | loss: 0.87478 | train_balanced_accuracy: 0.68384 | valid_balanced_accuracy: 0.64657 |  1:08:55s\n",
      "epoch 56 | loss: 0.86937 | train_balanced_accuracy: 0.69255 | valid_balanced_accuracy: 0.65461 |  1:10:10s\n",
      "epoch 57 | loss: 0.86791 | train_balanced_accuracy: 0.68526 | valid_balanced_accuracy: 0.64859 |  1:11:25s\n",
      "epoch 58 | loss: 0.86388 | train_balanced_accuracy: 0.69992 | valid_balanced_accuracy: 0.65558 |  1:12:41s\n",
      "epoch 59 | loss: 0.86016 | train_balanced_accuracy: 0.69486 | valid_balanced_accuracy: 0.65258 |  1:13:56s\n",
      "epoch 60 | loss: 0.86101 | train_balanced_accuracy: 0.68846 | valid_balanced_accuracy: 0.64564 |  1:15:11s\n",
      "epoch 61 | loss: 0.85656 | train_balanced_accuracy: 0.6958  | valid_balanced_accuracy: 0.65606 |  1:16:26s\n",
      "epoch 62 | loss: 0.85459 | train_balanced_accuracy: 0.69964 | valid_balanced_accuracy: 0.65932 |  1:17:41s\n",
      "epoch 63 | loss: 0.8485  | train_balanced_accuracy: 0.69671 | valid_balanced_accuracy: 0.65485 |  1:18:57s\n",
      "epoch 64 | loss: 0.84068 | train_balanced_accuracy: 0.70639 | valid_balanced_accuracy: 0.6568  |  1:20:12s\n",
      "epoch 65 | loss: 0.84055 | train_balanced_accuracy: 0.69448 | valid_balanced_accuracy: 0.64615 |  1:21:27s\n",
      "epoch 66 | loss: 0.84017 | train_balanced_accuracy: 0.69563 | valid_balanced_accuracy: 0.64928 |  1:22:42s\n",
      "epoch 67 | loss: 0.83501 | train_balanced_accuracy: 0.70221 | valid_balanced_accuracy: 0.65426 |  1:23:57s\n",
      "epoch 68 | loss: 0.83395 | train_balanced_accuracy: 0.70853 | valid_balanced_accuracy: 0.65862 |  1:25:12s\n",
      "epoch 69 | loss: 0.82778 | train_balanced_accuracy: 0.70543 | valid_balanced_accuracy: 0.65674 |  1:26:27s\n",
      "epoch 70 | loss: 0.82521 | train_balanced_accuracy: 0.71064 | valid_balanced_accuracy: 0.64936 |  1:27:42s\n",
      "epoch 71 | loss: 0.82211 | train_balanced_accuracy: 0.70492 | valid_balanced_accuracy: 0.64882 |  1:28:57s\n",
      "epoch 72 | loss: 0.81656 | train_balanced_accuracy: 0.69987 | valid_balanced_accuracy: 0.65629 |  1:30:12s\n",
      "\n",
      "Early stopping occured at epoch 72 with best_epoch = 62 and best_valid_balanced_accuracy = 0.65932\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'step_size': 100}, optimizer_params={'lr': 0.001}, n_steps=5, n_shared=4, n_independent=9, n_a=64, momentum=0.02, lambda_sparse=0.0001, clip_value=0.5, total=90.6min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'step_size': 100}, optimizer_params={'lr': 0.001}, n_steps=5, n_shared=4, n_independent=9, n_a=64, momentum=0.02, lambda_sparse=0.0001, clip_value=0.5 \n",
      "epoch 0  | loss: 2.75232 | train_balanced_accuracy: 0.16376 | valid_balanced_accuracy: 0.16676 |  0:01:14s\n",
      "epoch 1  | loss: 1.78302 | train_balanced_accuracy: 0.21819 | valid_balanced_accuracy: 0.2167  |  0:02:30s\n",
      "epoch 2  | loss: 1.60166 | train_balanced_accuracy: 0.25746 | valid_balanced_accuracy: 0.25377 |  0:03:45s\n",
      "epoch 3  | loss: 1.50743 | train_balanced_accuracy: 0.30495 | valid_balanced_accuracy: 0.29532 |  0:04:58s\n",
      "epoch 4  | loss: 1.43316 | train_balanced_accuracy: 0.3475  | valid_balanced_accuracy: 0.34384 |  0:06:10s\n",
      "epoch 5  | loss: 1.3793  | train_balanced_accuracy: 0.41519 | valid_balanced_accuracy: 0.41176 |  0:07:22s\n",
      "epoch 6  | loss: 1.33332 | train_balanced_accuracy: 0.42024 | valid_balanced_accuracy: 0.40131 |  0:08:35s\n",
      "epoch 7  | loss: 1.30311 | train_balanced_accuracy: 0.42933 | valid_balanced_accuracy: 0.41389 |  0:09:48s\n",
      "epoch 8  | loss: 1.28775 | train_balanced_accuracy: 0.46276 | valid_balanced_accuracy: 0.44751 |  0:11:00s\n",
      "epoch 9  | loss: 1.27018 | train_balanced_accuracy: 0.4749  | valid_balanced_accuracy: 0.48178 |  0:12:13s\n",
      "epoch 10 | loss: 1.24816 | train_balanced_accuracy: 0.48547 | valid_balanced_accuracy: 0.47829 |  0:13:25s\n",
      "epoch 11 | loss: 1.21977 | train_balanced_accuracy: 0.48579 | valid_balanced_accuracy: 0.50754 |  0:14:38s\n",
      "epoch 12 | loss: 1.19799 | train_balanced_accuracy: 0.5159  | valid_balanced_accuracy: 0.52022 |  0:15:50s\n",
      "epoch 13 | loss: 1.17759 | train_balanced_accuracy: 0.51717 | valid_balanced_accuracy: 0.5012  |  0:17:02s\n",
      "epoch 14 | loss: 1.15574 | train_balanced_accuracy: 0.55895 | valid_balanced_accuracy: 0.55444 |  0:18:15s\n",
      "epoch 15 | loss: 1.13545 | train_balanced_accuracy: 0.56387 | valid_balanced_accuracy: 0.56632 |  0:19:27s\n",
      "epoch 16 | loss: 1.11737 | train_balanced_accuracy: 0.56145 | valid_balanced_accuracy: 0.56032 |  0:20:43s\n",
      "epoch 17 | loss: 1.10082 | train_balanced_accuracy: 0.58516 | valid_balanced_accuracy: 0.57852 |  0:21:58s\n",
      "epoch 18 | loss: 1.08527 | train_balanced_accuracy: 0.59209 | valid_balanced_accuracy: 0.58768 |  0:23:14s\n",
      "epoch 19 | loss: 1.07357 | train_balanced_accuracy: 0.60934 | valid_balanced_accuracy: 0.59437 |  0:24:29s\n",
      "epoch 20 | loss: 1.06546 | train_balanced_accuracy: 0.58933 | valid_balanced_accuracy: 0.58883 |  0:25:43s\n",
      "epoch 21 | loss: 1.06301 | train_balanced_accuracy: 0.61236 | valid_balanced_accuracy: 0.60505 |  0:26:58s\n",
      "epoch 22 | loss: 1.05221 | train_balanced_accuracy: 0.61953 | valid_balanced_accuracy: 0.6162  |  0:28:13s\n",
      "epoch 23 | loss: 1.04031 | train_balanced_accuracy: 0.61879 | valid_balanced_accuracy: 0.61633 |  0:29:29s\n",
      "epoch 24 | loss: 1.02845 | train_balanced_accuracy: 0.63455 | valid_balanced_accuracy: 0.62791 |  0:30:44s\n",
      "epoch 25 | loss: 1.01815 | train_balanced_accuracy: 0.6418  | valid_balanced_accuracy: 0.63094 |  0:31:57s\n",
      "epoch 26 | loss: 1.00985 | train_balanced_accuracy: 0.64169 | valid_balanced_accuracy: 0.62741 |  0:33:10s\n",
      "epoch 27 | loss: 1.00218 | train_balanced_accuracy: 0.63337 | valid_balanced_accuracy: 0.61846 |  0:34:23s\n",
      "epoch 28 | loss: 0.99621 | train_balanced_accuracy: 0.64702 | valid_balanced_accuracy: 0.63827 |  0:35:36s\n",
      "epoch 29 | loss: 0.98641 | train_balanced_accuracy: 0.64339 | valid_balanced_accuracy: 0.63519 |  0:36:49s\n",
      "epoch 30 | loss: 0.98446 | train_balanced_accuracy: 0.64457 | valid_balanced_accuracy: 0.62431 |  0:38:02s\n",
      "epoch 31 | loss: 0.9866  | train_balanced_accuracy: 0.65606 | valid_balanced_accuracy: 0.64197 |  0:39:14s\n",
      "epoch 32 | loss: 0.97412 | train_balanced_accuracy: 0.66799 | valid_balanced_accuracy: 0.64551 |  0:40:26s\n",
      "epoch 33 | loss: 0.97028 | train_balanced_accuracy: 0.65922 | valid_balanced_accuracy: 0.64182 |  0:41:39s\n",
      "epoch 34 | loss: 0.96927 | train_balanced_accuracy: 0.65801 | valid_balanced_accuracy: 0.63857 |  0:42:54s\n",
      "epoch 35 | loss: 0.97001 | train_balanced_accuracy: 0.66865 | valid_balanced_accuracy: 0.65008 |  0:44:08s\n",
      "epoch 36 | loss: 0.96348 | train_balanced_accuracy: 0.67434 | valid_balanced_accuracy: 0.65139 |  0:45:23s\n",
      "epoch 37 | loss: 0.95374 | train_balanced_accuracy: 0.67941 | valid_balanced_accuracy: 0.65968 |  0:46:38s\n",
      "epoch 38 | loss: 0.95131 | train_balanced_accuracy: 0.67523 | valid_balanced_accuracy: 0.64777 |  0:47:53s\n",
      "epoch 39 | loss: 0.9486  | train_balanced_accuracy: 0.68597 | valid_balanced_accuracy: 0.64846 |  0:49:08s\n",
      "epoch 40 | loss: 0.93772 | train_balanced_accuracy: 0.68376 | valid_balanced_accuracy: 0.65845 |  0:50:22s\n",
      "epoch 41 | loss: 0.9324  | train_balanced_accuracy: 0.69212 | valid_balanced_accuracy: 0.6484  |  0:51:34s\n",
      "epoch 42 | loss: 0.93425 | train_balanced_accuracy: 0.6892  | valid_balanced_accuracy: 0.65638 |  0:52:46s\n",
      "epoch 43 | loss: 0.92914 | train_balanced_accuracy: 0.69058 | valid_balanced_accuracy: 0.65598 |  0:53:59s\n",
      "epoch 44 | loss: 0.92704 | train_balanced_accuracy: 0.68218 | valid_balanced_accuracy: 0.64886 |  0:55:11s\n",
      "epoch 45 | loss: 0.92414 | train_balanced_accuracy: 0.69666 | valid_balanced_accuracy: 0.65389 |  0:56:24s\n",
      "epoch 46 | loss: 0.91925 | train_balanced_accuracy: 0.69515 | valid_balanced_accuracy: 0.65721 |  0:57:36s\n",
      "epoch 47 | loss: 0.91483 | train_balanced_accuracy: 0.69789 | valid_balanced_accuracy: 0.65937 |  0:58:48s\n",
      "\n",
      "Early stopping occured at epoch 47 with best_epoch = 37 and best_valid_balanced_accuracy = 0.65968\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'step_size': 100}, optimizer_params={'lr': 0.001}, n_steps=5, n_shared=4, n_independent=9, n_a=64, momentum=0.02, lambda_sparse=0.0001, clip_value=0.5, total=59.2min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'step_size': 100}, optimizer_params={'lr': 0.001}, n_steps=5, n_shared=4, n_independent=9, n_a=64, momentum=0.02, lambda_sparse=0.0001, clip_value=0.5 \n",
      "epoch 0  | loss: 2.81885 | train_balanced_accuracy: 0.17858 | valid_balanced_accuracy: 0.179   |  0:01:12s\n",
      "epoch 1  | loss: 1.85369 | train_balanced_accuracy: 0.21289 | valid_balanced_accuracy: 0.21004 |  0:02:24s\n",
      "epoch 2  | loss: 1.61138 | train_balanced_accuracy: 0.25285 | valid_balanced_accuracy: 0.25031 |  0:03:36s\n",
      "epoch 3  | loss: 1.49838 | train_balanced_accuracy: 0.29058 | valid_balanced_accuracy: 0.2875  |  0:04:49s\n",
      "epoch 4  | loss: 1.42974 | train_balanced_accuracy: 0.36522 | valid_balanced_accuracy: 0.35366 |  0:06:01s\n",
      "epoch 5  | loss: 1.37708 | train_balanced_accuracy: 0.40261 | valid_balanced_accuracy: 0.38478 |  0:07:16s\n",
      "epoch 6  | loss: 1.34954 | train_balanced_accuracy: 0.42618 | valid_balanced_accuracy: 0.41563 |  0:08:30s\n",
      "epoch 7  | loss: 1.30954 | train_balanced_accuracy: 0.43144 | valid_balanced_accuracy: 0.42247 |  0:09:44s\n",
      "epoch 8  | loss: 1.28478 | train_balanced_accuracy: 0.45255 | valid_balanced_accuracy: 0.4442  |  0:10:58s\n",
      "epoch 9  | loss: 1.26017 | train_balanced_accuracy: 0.46228 | valid_balanced_accuracy: 0.44303 |  0:12:12s\n",
      "epoch 10 | loss: 1.23625 | train_balanced_accuracy: 0.482   | valid_balanced_accuracy: 0.46054 |  0:13:26s\n",
      "epoch 11 | loss: 1.21519 | train_balanced_accuracy: 0.478   | valid_balanced_accuracy: 0.46022 |  0:14:40s\n",
      "epoch 12 | loss: 1.20036 | train_balanced_accuracy: 0.51341 | valid_balanced_accuracy: 0.50061 |  0:15:54s\n",
      "epoch 13 | loss: 1.18186 | train_balanced_accuracy: 0.53174 | valid_balanced_accuracy: 0.50696 |  0:17:08s\n",
      "epoch 14 | loss: 1.16754 | train_balanced_accuracy: 0.54499 | valid_balanced_accuracy: 0.52247 |  0:18:22s\n",
      "epoch 15 | loss: 1.15092 | train_balanced_accuracy: 0.55315 | valid_balanced_accuracy: 0.53163 |  0:19:36s\n",
      "epoch 16 | loss: 1.14003 | train_balanced_accuracy: 0.54467 | valid_balanced_accuracy: 0.52795 |  0:20:50s\n",
      "epoch 17 | loss: 1.13329 | train_balanced_accuracy: 0.56682 | valid_balanced_accuracy: 0.52991 |  0:22:05s\n",
      "epoch 18 | loss: 1.122   | train_balanced_accuracy: 0.56772 | valid_balanced_accuracy: 0.54817 |  0:23:19s\n",
      "epoch 19 | loss: 1.1119  | train_balanced_accuracy: 0.57009 | valid_balanced_accuracy: 0.55752 |  0:24:33s\n",
      "epoch 20 | loss: 1.10057 | train_balanced_accuracy: 0.58757 | valid_balanced_accuracy: 0.56009 |  0:25:47s\n",
      "epoch 21 | loss: 1.08852 | train_balanced_accuracy: 0.59052 | valid_balanced_accuracy: 0.5625  |  0:27:01s\n",
      "epoch 22 | loss: 1.08468 | train_balanced_accuracy: 0.60565 | valid_balanced_accuracy: 0.57657 |  0:28:15s\n",
      "epoch 23 | loss: 1.0745  | train_balanced_accuracy: 0.6159  | valid_balanced_accuracy: 0.5908  |  0:29:29s\n",
      "epoch 24 | loss: 1.06379 | train_balanced_accuracy: 0.6051  | valid_balanced_accuracy: 0.58777 |  0:30:43s\n",
      "epoch 25 | loss: 1.06507 | train_balanced_accuracy: 0.60067 | valid_balanced_accuracy: 0.57398 |  0:31:58s\n",
      "epoch 26 | loss: 1.06351 | train_balanced_accuracy: 0.60437 | valid_balanced_accuracy: 0.58315 |  0:33:12s\n",
      "epoch 27 | loss: 1.05534 | train_balanced_accuracy: 0.61245 | valid_balanced_accuracy: 0.58177 |  0:34:26s\n",
      "epoch 28 | loss: 1.04879 | train_balanced_accuracy: 0.62003 | valid_balanced_accuracy: 0.57314 |  0:35:40s\n",
      "epoch 29 | loss: 1.03931 | train_balanced_accuracy: 0.62837 | valid_balanced_accuracy: 0.5909  |  0:36:54s\n",
      "epoch 30 | loss: 1.03058 | train_balanced_accuracy: 0.61857 | valid_balanced_accuracy: 0.58758 |  0:38:08s\n",
      "epoch 31 | loss: 1.02467 | train_balanced_accuracy: 0.63461 | valid_balanced_accuracy: 0.6029  |  0:39:22s\n",
      "epoch 32 | loss: 1.02069 | train_balanced_accuracy: 0.63548 | valid_balanced_accuracy: 0.58219 |  0:40:36s\n",
      "epoch 33 | loss: 1.00461 | train_balanced_accuracy: 0.65039 | valid_balanced_accuracy: 0.60842 |  0:41:50s\n",
      "epoch 34 | loss: 1.00427 | train_balanced_accuracy: 0.64092 | valid_balanced_accuracy: 0.60746 |  0:43:05s\n",
      "epoch 35 | loss: 1.0035  | train_balanced_accuracy: 0.64334 | valid_balanced_accuracy: 0.60203 |  0:44:19s\n",
      "epoch 36 | loss: 1.00314 | train_balanced_accuracy: 0.64861 | valid_balanced_accuracy: 0.6217  |  0:45:33s\n",
      "epoch 37 | loss: 1.0008  | train_balanced_accuracy: 0.6414  | valid_balanced_accuracy: 0.60984 |  0:46:47s\n",
      "epoch 38 | loss: 0.99514 | train_balanced_accuracy: 0.66705 | valid_balanced_accuracy: 0.63397 |  0:48:01s\n",
      "epoch 39 | loss: 0.98353 | train_balanced_accuracy: 0.66428 | valid_balanced_accuracy: 0.6266  |  0:49:15s\n",
      "epoch 40 | loss: 0.97288 | train_balanced_accuracy: 0.66628 | valid_balanced_accuracy: 0.62478 |  0:50:30s\n",
      "epoch 41 | loss: 0.96766 | train_balanced_accuracy: 0.67252 | valid_balanced_accuracy: 0.62609 |  0:51:44s\n",
      "epoch 42 | loss: 0.95954 | train_balanced_accuracy: 0.68674 | valid_balanced_accuracy: 0.64751 |  0:52:58s\n",
      "epoch 43 | loss: 0.95757 | train_balanced_accuracy: 0.66869 | valid_balanced_accuracy: 0.62606 |  0:54:12s\n",
      "epoch 44 | loss: 0.95209 | train_balanced_accuracy: 0.67683 | valid_balanced_accuracy: 0.62996 |  0:55:26s\n",
      "epoch 45 | loss: 0.94735 | train_balanced_accuracy: 0.68511 | valid_balanced_accuracy: 0.6333  |  0:56:41s\n",
      "epoch 46 | loss: 0.94159 | train_balanced_accuracy: 0.69639 | valid_balanced_accuracy: 0.64226 |  0:57:55s\n",
      "epoch 47 | loss: 0.93383 | train_balanced_accuracy: 0.69315 | valid_balanced_accuracy: 0.63953 |  0:59:08s\n",
      "epoch 48 | loss: 0.93434 | train_balanced_accuracy: 0.68578 | valid_balanced_accuracy: 0.6257  |  1:00:20s\n",
      "epoch 49 | loss: 0.92408 | train_balanced_accuracy: 0.6995  | valid_balanced_accuracy: 0.64733 |  1:01:32s\n",
      "epoch 50 | loss: 0.91915 | train_balanced_accuracy: 0.69904 | valid_balanced_accuracy: 0.64344 |  1:02:46s\n",
      "epoch 51 | loss: 0.91792 | train_balanced_accuracy: 0.70632 | valid_balanced_accuracy: 0.65693 |  1:03:59s\n",
      "epoch 52 | loss: 0.91263 | train_balanced_accuracy: 0.70661 | valid_balanced_accuracy: 0.64955 |  1:05:12s\n",
      "epoch 53 | loss: 0.90944 | train_balanced_accuracy: 0.71907 | valid_balanced_accuracy: 0.66034 |  1:06:24s\n",
      "epoch 54 | loss: 0.90534 | train_balanced_accuracy: 0.71565 | valid_balanced_accuracy: 0.65691 |  1:07:37s\n",
      "epoch 55 | loss: 0.90471 | train_balanced_accuracy: 0.70887 | valid_balanced_accuracy: 0.65568 |  1:08:49s\n",
      "epoch 56 | loss: 0.91008 | train_balanced_accuracy: 0.70055 | valid_balanced_accuracy: 0.65264 |  1:10:01s\n",
      "epoch 57 | loss: 0.90057 | train_balanced_accuracy: 0.70306 | valid_balanced_accuracy: 0.65169 |  1:11:15s\n",
      "epoch 58 | loss: 0.89309 | train_balanced_accuracy: 0.71055 | valid_balanced_accuracy: 0.64717 |  1:12:28s\n",
      "epoch 59 | loss: 0.88986 | train_balanced_accuracy: 0.70925 | valid_balanced_accuracy: 0.65184 |  1:13:41s\n",
      "epoch 60 | loss: 0.88116 | train_balanced_accuracy: 0.73036 | valid_balanced_accuracy: 0.65923 |  1:14:53s\n",
      "epoch 61 | loss: 0.88304 | train_balanced_accuracy: 0.71932 | valid_balanced_accuracy: 0.65401 |  1:16:06s\n",
      "epoch 62 | loss: 0.87818 | train_balanced_accuracy: 0.7277  | valid_balanced_accuracy: 0.65831 |  1:17:19s\n",
      "epoch 63 | loss: 0.87465 | train_balanced_accuracy: 0.72468 | valid_balanced_accuracy: 0.65688 |  1:18:31s\n",
      "\n",
      "Early stopping occured at epoch 63 with best_epoch = 53 and best_valid_balanced_accuracy = 0.66034\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'step_size': 100}, optimizer_params={'lr': 0.001}, n_steps=5, n_shared=4, n_independent=9, n_a=64, momentum=0.02, lambda_sparse=0.0001, clip_value=0.5, total=78.9min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'step_size': 100}, optimizer_params={'lr': 0.001}, n_steps=5, n_shared=4, n_independent=9, n_a=64, momentum=0.02, lambda_sparse=0.0001, clip_value=0.5 \n",
      "epoch 0  | loss: 2.80081 | train_balanced_accuracy: 0.16612 | valid_balanced_accuracy: 0.1658  |  0:01:13s\n",
      "epoch 1  | loss: 1.79312 | train_balanced_accuracy: 0.21764 | valid_balanced_accuracy: 0.21479 |  0:02:26s\n",
      "epoch 2  | loss: 1.61027 | train_balanced_accuracy: 0.23162 | valid_balanced_accuracy: 0.22928 |  0:03:39s\n",
      "epoch 3  | loss: 1.52208 | train_balanced_accuracy: 0.29437 | valid_balanced_accuracy: 0.29099 |  0:04:52s\n",
      "epoch 4  | loss: 1.45386 | train_balanced_accuracy: 0.34845 | valid_balanced_accuracy: 0.34003 |  0:06:04s\n",
      "epoch 5  | loss: 1.38616 | train_balanced_accuracy: 0.35963 | valid_balanced_accuracy: 0.35761 |  0:07:17s\n",
      "epoch 6  | loss: 1.33101 | train_balanced_accuracy: 0.43294 | valid_balanced_accuracy: 0.42553 |  0:08:29s\n",
      "epoch 7  | loss: 1.28416 | train_balanced_accuracy: 0.46112 | valid_balanced_accuracy: 0.45401 |  0:09:42s\n",
      "epoch 8  | loss: 1.24472 | train_balanced_accuracy: 0.46348 | valid_balanced_accuracy: 0.45214 |  0:10:56s\n",
      "epoch 9  | loss: 1.21434 | train_balanced_accuracy: 0.50213 | valid_balanced_accuracy: 0.49935 |  0:12:10s\n",
      "epoch 10 | loss: 1.18569 | train_balanced_accuracy: 0.51433 | valid_balanced_accuracy: 0.51211 |  0:13:25s\n",
      "epoch 11 | loss: 1.16644 | train_balanced_accuracy: 0.52054 | valid_balanced_accuracy: 0.51406 |  0:14:41s\n",
      "epoch 12 | loss: 1.14347 | train_balanced_accuracy: 0.54455 | valid_balanced_accuracy: 0.53777 |  0:15:56s\n",
      "epoch 13 | loss: 1.1241  | train_balanced_accuracy: 0.55172 | valid_balanced_accuracy: 0.54719 |  0:17:11s\n",
      "epoch 14 | loss: 1.11316 | train_balanced_accuracy: 0.53649 | valid_balanced_accuracy: 0.51973 |  0:18:26s\n",
      "epoch 15 | loss: 1.10422 | train_balanced_accuracy: 0.54275 | valid_balanced_accuracy: 0.52943 |  0:19:41s\n",
      "epoch 16 | loss: 1.08817 | train_balanced_accuracy: 0.54835 | valid_balanced_accuracy: 0.53849 |  0:20:56s\n",
      "epoch 17 | loss: 1.07893 | train_balanced_accuracy: 0.57985 | valid_balanced_accuracy: 0.56417 |  0:22:12s\n",
      "epoch 18 | loss: 1.06817 | train_balanced_accuracy: 0.61144 | valid_balanced_accuracy: 0.59023 |  0:23:27s\n",
      "epoch 19 | loss: 1.05895 | train_balanced_accuracy: 0.56181 | valid_balanced_accuracy: 0.53648 |  0:24:42s\n",
      "epoch 20 | loss: 1.04984 | train_balanced_accuracy: 0.61204 | valid_balanced_accuracy: 0.58461 |  0:25:57s\n",
      "epoch 21 | loss: 1.03913 | train_balanced_accuracy: 0.59412 | valid_balanced_accuracy: 0.57549 |  0:27:12s\n",
      "epoch 22 | loss: 1.02984 | train_balanced_accuracy: 0.62645 | valid_balanced_accuracy: 0.60111 |  0:28:27s\n",
      "epoch 23 | loss: 1.02248 | train_balanced_accuracy: 0.61474 | valid_balanced_accuracy: 0.58977 |  0:29:43s\n",
      "epoch 24 | loss: 1.02018 | train_balanced_accuracy: 0.62949 | valid_balanced_accuracy: 0.59444 |  0:30:56s\n",
      "epoch 25 | loss: 1.01528 | train_balanced_accuracy: 0.63752 | valid_balanced_accuracy: 0.60487 |  0:32:09s\n",
      "epoch 26 | loss: 1.00351 | train_balanced_accuracy: 0.63145 | valid_balanced_accuracy: 0.59089 |  0:33:21s\n",
      "epoch 27 | loss: 0.99932 | train_balanced_accuracy: 0.64715 | valid_balanced_accuracy: 0.60694 |  0:34:34s\n",
      "epoch 28 | loss: 0.99226 | train_balanced_accuracy: 0.64272 | valid_balanced_accuracy: 0.60115 |  0:35:46s\n",
      "epoch 29 | loss: 0.98513 | train_balanced_accuracy: 0.64997 | valid_balanced_accuracy: 0.61058 |  0:36:57s\n",
      "epoch 30 | loss: 0.97923 | train_balanced_accuracy: 0.64231 | valid_balanced_accuracy: 0.60164 |  0:38:09s\n",
      "epoch 31 | loss: 0.97739 | train_balanced_accuracy: 0.65135 | valid_balanced_accuracy: 0.61631 |  0:39:21s\n",
      "epoch 32 | loss: 0.97259 | train_balanced_accuracy: 0.65564 | valid_balanced_accuracy: 0.61418 |  0:40:33s\n",
      "epoch 33 | loss: 0.96686 | train_balanced_accuracy: 0.64903 | valid_balanced_accuracy: 0.59868 |  0:41:45s\n",
      "epoch 34 | loss: 0.96751 | train_balanced_accuracy: 0.66157 | valid_balanced_accuracy: 0.61514 |  0:42:56s\n",
      "epoch 35 | loss: 0.95599 | train_balanced_accuracy: 0.66793 | valid_balanced_accuracy: 0.61203 |  0:44:08s\n",
      "epoch 36 | loss: 0.95041 | train_balanced_accuracy: 0.67283 | valid_balanced_accuracy: 0.62184 |  0:45:20s\n",
      "epoch 37 | loss: 0.94632 | train_balanced_accuracy: 0.67306 | valid_balanced_accuracy: 0.61862 |  0:46:32s\n",
      "epoch 38 | loss: 0.94572 | train_balanced_accuracy: 0.68424 | valid_balanced_accuracy: 0.63011 |  0:47:44s\n",
      "epoch 39 | loss: 0.93997 | train_balanced_accuracy: 0.67803 | valid_balanced_accuracy: 0.62706 |  0:48:56s\n",
      "epoch 40 | loss: 0.94213 | train_balanced_accuracy: 0.65864 | valid_balanced_accuracy: 0.61367 |  0:50:09s\n",
      "epoch 41 | loss: 0.93259 | train_balanced_accuracy: 0.68533 | valid_balanced_accuracy: 0.62954 |  0:51:23s\n",
      "epoch 42 | loss: 0.92983 | train_balanced_accuracy: 0.67575 | valid_balanced_accuracy: 0.62895 |  0:52:37s\n",
      "epoch 43 | loss: 0.92565 | train_balanced_accuracy: 0.68101 | valid_balanced_accuracy: 0.63779 |  0:53:50s\n",
      "epoch 44 | loss: 0.91992 | train_balanced_accuracy: 0.66685 | valid_balanced_accuracy: 0.62233 |  0:55:02s\n",
      "epoch 45 | loss: 0.9171  | train_balanced_accuracy: 0.68533 | valid_balanced_accuracy: 0.63722 |  0:56:16s\n",
      "epoch 46 | loss: 0.91281 | train_balanced_accuracy: 0.68213 | valid_balanced_accuracy: 0.63231 |  0:57:30s\n",
      "epoch 47 | loss: 0.91744 | train_balanced_accuracy: 0.6738  | valid_balanced_accuracy: 0.62279 |  0:58:42s\n",
      "epoch 48 | loss: 0.91688 | train_balanced_accuracy: 0.68322 | valid_balanced_accuracy: 0.62986 |  0:59:55s\n",
      "epoch 49 | loss: 0.92    | train_balanced_accuracy: 0.68277 | valid_balanced_accuracy: 0.63318 |  1:01:10s\n",
      "epoch 50 | loss: 0.91368 | train_balanced_accuracy: 0.68134 | valid_balanced_accuracy: 0.63239 |  1:02:24s\n",
      "epoch 51 | loss: 0.90237 | train_balanced_accuracy: 0.69223 | valid_balanced_accuracy: 0.63751 |  1:03:38s\n",
      "epoch 52 | loss: 0.89972 | train_balanced_accuracy: 0.70196 | valid_balanced_accuracy: 0.64523 |  1:04:52s\n",
      "epoch 53 | loss: 0.89198 | train_balanced_accuracy: 0.69484 | valid_balanced_accuracy: 0.64126 |  1:06:06s\n",
      "epoch 54 | loss: 0.88631 | train_balanced_accuracy: 0.72125 | valid_balanced_accuracy: 0.65337 |  1:07:19s\n",
      "epoch 55 | loss: 0.88294 | train_balanced_accuracy: 0.71704 | valid_balanced_accuracy: 0.65453 |  1:08:33s\n",
      "epoch 56 | loss: 0.88186 | train_balanced_accuracy: 0.72493 | valid_balanced_accuracy: 0.65457 |  1:09:47s\n",
      "epoch 57 | loss: 0.87368 | train_balanced_accuracy: 0.70391 | valid_balanced_accuracy: 0.65081 |  1:11:01s\n",
      "epoch 58 | loss: 0.87318 | train_balanced_accuracy: 0.71115 | valid_balanced_accuracy: 0.6448  |  1:12:16s\n",
      "epoch 59 | loss: 0.87319 | train_balanced_accuracy: 0.73006 | valid_balanced_accuracy: 0.64889 |  1:13:29s\n",
      "epoch 60 | loss: 0.86668 | train_balanced_accuracy: 0.7159  | valid_balanced_accuracy: 0.651   |  1:14:44s\n",
      "epoch 61 | loss: 0.86311 | train_balanced_accuracy: 0.71699 | valid_balanced_accuracy: 0.6446  |  1:15:58s\n",
      "epoch 62 | loss: 0.86034 | train_balanced_accuracy: 0.71813 | valid_balanced_accuracy: 0.64609 |  1:17:13s\n",
      "epoch 63 | loss: 0.85833 | train_balanced_accuracy: 0.70687 | valid_balanced_accuracy: 0.64261 |  1:18:27s\n",
      "epoch 64 | loss: 0.85274 | train_balanced_accuracy: 0.73173 | valid_balanced_accuracy: 0.66438 |  1:19:41s\n",
      "epoch 65 | loss: 0.85336 | train_balanced_accuracy: 0.71448 | valid_balanced_accuracy: 0.64908 |  1:20:54s\n",
      "epoch 66 | loss: 0.85068 | train_balanced_accuracy: 0.73518 | valid_balanced_accuracy: 0.65649 |  1:22:08s\n",
      "epoch 67 | loss: 0.85866 | train_balanced_accuracy: 0.70994 | valid_balanced_accuracy: 0.6472  |  1:23:21s\n",
      "epoch 68 | loss: 0.84756 | train_balanced_accuracy: 0.73407 | valid_balanced_accuracy: 0.65543 |  1:24:33s\n",
      "epoch 69 | loss: 0.84344 | train_balanced_accuracy: 0.73875 | valid_balanced_accuracy: 0.65984 |  1:25:45s\n",
      "epoch 70 | loss: 0.84529 | train_balanced_accuracy: 0.71763 | valid_balanced_accuracy: 0.65035 |  1:26:57s\n",
      "epoch 71 | loss: 0.83562 | train_balanced_accuracy: 0.73373 | valid_balanced_accuracy: 0.65655 |  1:28:09s\n",
      "epoch 72 | loss: 0.83546 | train_balanced_accuracy: 0.73976 | valid_balanced_accuracy: 0.65789 |  1:29:22s\n",
      "epoch 73 | loss: 0.83566 | train_balanced_accuracy: 0.7309  | valid_balanced_accuracy: 0.66088 |  1:30:34s\n",
      "epoch 74 | loss: 0.82956 | train_balanced_accuracy: 0.72241 | valid_balanced_accuracy: 0.64356 |  1:31:46s\n",
      "\n",
      "Early stopping occured at epoch 74 with best_epoch = 64 and best_valid_balanced_accuracy = 0.66438\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'step_size': 100}, optimizer_params={'lr': 0.001}, n_steps=5, n_shared=4, n_independent=9, n_a=64, momentum=0.02, lambda_sparse=0.0001, clip_value=0.5, total=92.1min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'step_size': 10}, optimizer_params={'lr': 1e-05}, n_steps=13, n_shared=3, n_independent=7, n_a=4, momentum=0.02, lambda_sparse=0.01, clip_value=1.0 \n",
      "epoch 0  | loss: 7.21561 | train_balanced_accuracy: 0.04499 | valid_balanced_accuracy: 0.04511 |  0:02:12s\n",
      "epoch 1  | loss: 7.17246 | train_balanced_accuracy: 0.04509 | valid_balanced_accuracy: 0.04482 |  0:04:29s\n",
      "epoch 2  | loss: 7.13129 | train_balanced_accuracy: 0.04531 | valid_balanced_accuracy: 0.04517 |  0:06:44s\n",
      "epoch 3  | loss: 7.09037 | train_balanced_accuracy: 0.04527 | valid_balanced_accuracy: 0.04549 |  0:09:00s\n",
      "epoch 4  | loss: 7.03906 | train_balanced_accuracy: 0.04475 | valid_balanced_accuracy: 0.04472 |  0:11:16s\n",
      "epoch 5  | loss: 7.00695 | train_balanced_accuracy: 0.0448  | valid_balanced_accuracy: 0.04452 |  0:13:33s\n",
      "epoch 6  | loss: 6.96334 | train_balanced_accuracy: 0.04485 | valid_balanced_accuracy: 0.0452  |  0:15:50s\n",
      "epoch 7  | loss: 6.93258 | train_balanced_accuracy: 0.04496 | valid_balanced_accuracy: 0.04497 |  0:18:06s\n",
      "epoch 8  | loss: 6.88866 | train_balanced_accuracy: 0.04509 | valid_balanced_accuracy: 0.04504 |  0:20:22s\n",
      "epoch 9  | loss: 6.85006 | train_balanced_accuracy: 0.04477 | valid_balanced_accuracy: 0.04519 |  0:22:39s\n",
      "epoch 10 | loss: 6.81506 | train_balanced_accuracy: 0.04454 | valid_balanced_accuracy: 0.04458 |  0:24:55s\n",
      "epoch 11 | loss: 6.77877 | train_balanced_accuracy: 0.04437 | valid_balanced_accuracy: 0.04445 |  0:27:12s\n",
      "epoch 12 | loss: 6.75102 | train_balanced_accuracy: 0.04472 | valid_balanced_accuracy: 0.04514 |  0:29:26s\n",
      "epoch 13 | loss: 6.70367 | train_balanced_accuracy: 0.04396 | valid_balanced_accuracy: 0.04477 |  0:31:38s\n",
      "\n",
      "Early stopping occured at epoch 13 with best_epoch = 3 and best_valid_balanced_accuracy = 0.04549\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'step_size': 10}, optimizer_params={'lr': 1e-05}, n_steps=13, n_shared=3, n_independent=7, n_a=4, momentum=0.02, lambda_sparse=0.01, clip_value=1.0, total=32.3min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'step_size': 10}, optimizer_params={'lr': 1e-05}, n_steps=13, n_shared=3, n_independent=7, n_a=4, momentum=0.02, lambda_sparse=0.01, clip_value=1.0 \n",
      "epoch 0  | loss: 7.21328 | train_balanced_accuracy: 0.04471 | valid_balanced_accuracy: 0.04499 |  0:02:12s\n",
      "epoch 1  | loss: 7.16518 | train_balanced_accuracy: 0.0448  | valid_balanced_accuracy: 0.04497 |  0:04:27s\n",
      "epoch 2  | loss: 7.13417 | train_balanced_accuracy: 0.04537 | valid_balanced_accuracy: 0.04566 |  0:06:44s\n",
      "epoch 3  | loss: 7.09335 | train_balanced_accuracy: 0.0451  | valid_balanced_accuracy: 0.04526 |  0:09:01s\n",
      "epoch 4  | loss: 7.05831 | train_balanced_accuracy: 0.04508 | valid_balanced_accuracy: 0.04532 |  0:11:18s\n",
      "epoch 5  | loss: 7.00979 | train_balanced_accuracy: 0.04459 | valid_balanced_accuracy: 0.04455 |  0:13:34s\n",
      "epoch 6  | loss: 6.98588 | train_balanced_accuracy: 0.04473 | valid_balanced_accuracy: 0.04473 |  0:15:51s\n",
      "epoch 7  | loss: 6.94427 | train_balanced_accuracy: 0.04482 | valid_balanced_accuracy: 0.04478 |  0:18:09s\n",
      "epoch 8  | loss: 6.89536 | train_balanced_accuracy: 0.04475 | valid_balanced_accuracy: 0.04507 |  0:20:26s\n",
      "epoch 9  | loss: 6.86347 | train_balanced_accuracy: 0.04504 | valid_balanced_accuracy: 0.04521 |  0:22:44s\n",
      "epoch 10 | loss: 6.83703 | train_balanced_accuracy: 0.04448 | valid_balanced_accuracy: 0.04452 |  0:25:01s\n",
      "epoch 11 | loss: 6.78256 | train_balanced_accuracy: 0.04466 | valid_balanced_accuracy: 0.04434 |  0:27:19s\n",
      "epoch 12 | loss: 6.7593  | train_balanced_accuracy: 0.04502 | valid_balanced_accuracy: 0.04536 |  0:29:36s\n",
      "\n",
      "Early stopping occured at epoch 12 with best_epoch = 2 and best_valid_balanced_accuracy = 0.04566\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'step_size': 10}, optimizer_params={'lr': 1e-05}, n_steps=13, n_shared=3, n_independent=7, n_a=4, momentum=0.02, lambda_sparse=0.01, clip_value=1.0, total=30.3min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'step_size': 10}, optimizer_params={'lr': 1e-05}, n_steps=13, n_shared=3, n_independent=7, n_a=4, momentum=0.02, lambda_sparse=0.01, clip_value=1.0 \n",
      "epoch 0  | loss: 7.2125  | train_balanced_accuracy: 0.0448  | valid_balanced_accuracy: 0.04505 |  0:02:17s\n",
      "epoch 1  | loss: 7.18218 | train_balanced_accuracy: 0.04529 | valid_balanced_accuracy: 0.04529 |  0:04:34s\n",
      "epoch 2  | loss: 7.13755 | train_balanced_accuracy: 0.04541 | valid_balanced_accuracy: 0.04508 |  0:06:47s\n",
      "epoch 3  | loss: 7.09973 | train_balanced_accuracy: 0.04451 | valid_balanced_accuracy: 0.04485 |  0:08:58s\n",
      "epoch 4  | loss: 7.06873 | train_balanced_accuracy: 0.04465 | valid_balanced_accuracy: 0.04501 |  0:11:14s\n",
      "epoch 5  | loss: 7.02689 | train_balanced_accuracy: 0.0445  | valid_balanced_accuracy: 0.04472 |  0:13:30s\n",
      "epoch 6  | loss: 7.00349 | train_balanced_accuracy: 0.04491 | valid_balanced_accuracy: 0.04501 |  0:15:43s\n",
      "epoch 7  | loss: 6.96216 | train_balanced_accuracy: 0.04487 | valid_balanced_accuracy: 0.04522 |  0:17:56s\n",
      "epoch 8  | loss: 6.92414 | train_balanced_accuracy: 0.04475 | valid_balanced_accuracy: 0.04514 |  0:20:08s\n",
      "epoch 9  | loss: 6.87162 | train_balanced_accuracy: 0.04493 | valid_balanced_accuracy: 0.04519 |  0:22:21s\n",
      "epoch 10 | loss: 6.84102 | train_balanced_accuracy: 0.04485 | valid_balanced_accuracy: 0.04475 |  0:24:34s\n",
      "epoch 11 | loss: 6.80625 | train_balanced_accuracy: 0.04419 | valid_balanced_accuracy: 0.04442 |  0:26:47s\n",
      "\n",
      "Early stopping occured at epoch 11 with best_epoch = 1 and best_valid_balanced_accuracy = 0.04529\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'step_size': 10}, optimizer_params={'lr': 1e-05}, n_steps=13, n_shared=3, n_independent=7, n_a=4, momentum=0.02, lambda_sparse=0.01, clip_value=1.0, total=27.4min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'step_size': 10}, optimizer_params={'lr': 1e-05}, n_steps=13, n_shared=3, n_independent=7, n_a=4, momentum=0.02, lambda_sparse=0.01, clip_value=1.0 \n",
      "epoch 0  | loss: 7.21525 | train_balanced_accuracy: 0.04485 | valid_balanced_accuracy: 0.04488 |  0:02:13s\n",
      "epoch 1  | loss: 7.16202 | train_balanced_accuracy: 0.04463 | valid_balanced_accuracy: 0.04461 |  0:04:27s\n",
      "epoch 2  | loss: 7.13453 | train_balanced_accuracy: 0.04474 | valid_balanced_accuracy: 0.04502 |  0:06:44s\n",
      "epoch 3  | loss: 7.08802 | train_balanced_accuracy: 0.04451 | valid_balanced_accuracy: 0.04456 |  0:09:00s\n",
      "epoch 4  | loss: 7.05089 | train_balanced_accuracy: 0.04452 | valid_balanced_accuracy: 0.04456 |  0:11:17s\n",
      "epoch 5  | loss: 7.01825 | train_balanced_accuracy: 0.04447 | valid_balanced_accuracy: 0.04445 |  0:13:35s\n",
      "epoch 6  | loss: 7.0015  | train_balanced_accuracy: 0.04424 | valid_balanced_accuracy: 0.0443  |  0:15:52s\n",
      "epoch 7  | loss: 6.95143 | train_balanced_accuracy: 0.04471 | valid_balanced_accuracy: 0.0446  |  0:18:09s\n",
      "epoch 8  | loss: 6.91726 | train_balanced_accuracy: 0.04472 | valid_balanced_accuracy: 0.04466 |  0:20:26s\n",
      "epoch 9  | loss: 6.86773 | train_balanced_accuracy: 0.04417 | valid_balanced_accuracy: 0.04382 |  0:22:43s\n",
      "epoch 10 | loss: 6.83784 | train_balanced_accuracy: 0.045   | valid_balanced_accuracy: 0.04487 |  0:25:01s\n",
      "epoch 11 | loss: 6.79603 | train_balanced_accuracy: 0.04492 | valid_balanced_accuracy: 0.04504 |  0:27:19s\n",
      "epoch 12 | loss: 6.77793 | train_balanced_accuracy: 0.04377 | valid_balanced_accuracy: 0.04391 |  0:29:38s\n",
      "epoch 13 | loss: 6.72527 | train_balanced_accuracy: 0.04491 | valid_balanced_accuracy: 0.04475 |  0:31:57s\n",
      "epoch 14 | loss: 6.68926 | train_balanced_accuracy: 0.04392 | valid_balanced_accuracy: 0.04393 |  0:34:15s\n",
      "epoch 15 | loss: 6.64896 | train_balanced_accuracy: 0.04445 | valid_balanced_accuracy: 0.04414 |  0:36:28s\n",
      "epoch 16 | loss: 6.62744 | train_balanced_accuracy: 0.04429 | valid_balanced_accuracy: 0.04439 |  0:38:45s\n",
      "epoch 17 | loss: 6.59761 | train_balanced_accuracy: 0.04524 | valid_balanced_accuracy: 0.04533 |  0:41:02s\n",
      "epoch 18 | loss: 6.56117 | train_balanced_accuracy: 0.04426 | valid_balanced_accuracy: 0.04423 |  0:43:18s\n",
      "epoch 19 | loss: 6.53333 | train_balanced_accuracy: 0.04363 | valid_balanced_accuracy: 0.04356 |  0:45:36s\n",
      "epoch 20 | loss: 6.49957 | train_balanced_accuracy: 0.04351 | valid_balanced_accuracy: 0.04351 |  0:47:52s\n",
      "epoch 21 | loss: 6.47006 | train_balanced_accuracy: 0.04438 | valid_balanced_accuracy: 0.0441  |  0:50:06s\n",
      "epoch 22 | loss: 6.42765 | train_balanced_accuracy: 0.04457 | valid_balanced_accuracy: 0.04474 |  0:52:19s\n",
      "epoch 23 | loss: 6.41894 | train_balanced_accuracy: 0.04422 | valid_balanced_accuracy: 0.04421 |  0:54:31s\n",
      "epoch 24 | loss: 6.38281 | train_balanced_accuracy: 0.04338 | valid_balanced_accuracy: 0.04339 |  0:56:46s\n",
      "epoch 25 | loss: 6.34188 | train_balanced_accuracy: 0.04305 | valid_balanced_accuracy: 0.04272 |  0:58:59s\n",
      "epoch 26 | loss: 6.3112  | train_balanced_accuracy: 0.04403 | valid_balanced_accuracy: 0.04393 |  1:01:10s\n",
      "epoch 27 | loss: 6.26809 | train_balanced_accuracy: 0.04372 | valid_balanced_accuracy: 0.04358 |  1:03:24s\n",
      "\n",
      "Early stopping occured at epoch 27 with best_epoch = 17 and best_valid_balanced_accuracy = 0.04533\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'step_size': 10}, optimizer_params={'lr': 1e-05}, n_steps=13, n_shared=3, n_independent=7, n_a=4, momentum=0.02, lambda_sparse=0.01, clip_value=1.0, total=64.1min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'step_size': 10}, optimizer_params={'lr': 1e-05}, n_steps=13, n_shared=3, n_independent=7, n_a=4, momentum=0.02, lambda_sparse=0.01, clip_value=1.0 \n",
      "epoch 0  | loss: 7.21413 | train_balanced_accuracy: 0.04502 | valid_balanced_accuracy: 0.04511 |  0:02:12s\n",
      "epoch 1  | loss: 7.15655 | train_balanced_accuracy: 0.04502 | valid_balanced_accuracy: 0.04531 |  0:04:27s\n",
      "epoch 2  | loss: 7.12784 | train_balanced_accuracy: 0.04473 | valid_balanced_accuracy: 0.04491 |  0:06:44s\n",
      "epoch 3  | loss: 7.0754  | train_balanced_accuracy: 0.04517 | valid_balanced_accuracy: 0.04481 |  0:09:01s\n",
      "epoch 4  | loss: 7.05809 | train_balanced_accuracy: 0.04451 | valid_balanced_accuracy: 0.04469 |  0:11:17s\n",
      "epoch 5  | loss: 7.02173 | train_balanced_accuracy: 0.04438 | valid_balanced_accuracy: 0.04456 |  0:13:30s\n",
      "epoch 6  | loss: 6.97697 | train_balanced_accuracy: 0.04484 | valid_balanced_accuracy: 0.04511 |  0:15:43s\n",
      "epoch 7  | loss: 6.94525 | train_balanced_accuracy: 0.04428 | valid_balanced_accuracy: 0.04459 |  0:17:54s\n",
      "epoch 8  | loss: 6.91044 | train_balanced_accuracy: 0.04492 | valid_balanced_accuracy: 0.0448  |  0:20:06s\n",
      "epoch 9  | loss: 6.88169 | train_balanced_accuracy: 0.04493 | valid_balanced_accuracy: 0.04503 |  0:22:21s\n",
      "epoch 10 | loss: 6.83781 | train_balanced_accuracy: 0.04444 | valid_balanced_accuracy: 0.04437 |  0:24:36s\n",
      "epoch 11 | loss: 6.81234 | train_balanced_accuracy: 0.04472 | valid_balanced_accuracy: 0.04528 |  0:26:52s\n",
      "\n",
      "Early stopping occured at epoch 11 with best_epoch = 1 and best_valid_balanced_accuracy = 0.04531\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'step_size': 10}, optimizer_params={'lr': 1e-05}, n_steps=13, n_shared=3, n_independent=7, n_a=4, momentum=0.02, lambda_sparse=0.01, clip_value=1.0, total=27.6min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'step_size': 100}, optimizer_params={'lr': 1e-05}, n_steps=11, n_shared=1, n_independent=2, n_a=64, momentum=0.05, lambda_sparse=1, clip_value=0.5 \n",
      "epoch 0  | loss: 14.30515| train_balanced_accuracy: 0.06716 | valid_balanced_accuracy: 0.07742 |  0:00:49s\n",
      "epoch 1  | loss: 13.49798| train_balanced_accuracy: 0.07122 | valid_balanced_accuracy: 0.07519 |  0:01:39s\n",
      "epoch 2  | loss: 12.68394| train_balanced_accuracy: 0.06726 | valid_balanced_accuracy: 0.07443 |  0:02:27s\n",
      "epoch 3  | loss: 11.93278| train_balanced_accuracy: 0.07099 | valid_balanced_accuracy: 0.08427 |  0:03:15s\n",
      "epoch 4  | loss: 11.19768| train_balanced_accuracy: 0.06492 | valid_balanced_accuracy: 0.08014 |  0:04:04s\n",
      "epoch 5  | loss: 10.50301| train_balanced_accuracy: 0.0637  | valid_balanced_accuracy: 0.07892 |  0:04:52s\n",
      "epoch 6  | loss: 9.80108 | train_balanced_accuracy: 0.09236 | valid_balanced_accuracy: 0.10192 |  0:05:40s\n",
      "epoch 7  | loss: 9.20026 | train_balanced_accuracy: 0.08259 | valid_balanced_accuracy: 0.09995 |  0:06:27s\n",
      "epoch 8  | loss: 8.60155 | train_balanced_accuracy: 0.09645 | valid_balanced_accuracy: 0.09125 |  0:07:15s\n",
      "epoch 9  | loss: 8.05761 | train_balanced_accuracy: 0.0922  | valid_balanced_accuracy: 0.10491 |  0:08:02s\n",
      "epoch 10 | loss: 7.51804 | train_balanced_accuracy: 0.0976  | valid_balanced_accuracy: 0.10748 |  0:08:50s\n",
      "epoch 11 | loss: 7.03592 | train_balanced_accuracy: 0.10427 | valid_balanced_accuracy: 0.11528 |  0:09:38s\n",
      "epoch 12 | loss: 6.56257 | train_balanced_accuracy: 0.09267 | valid_balanced_accuracy: 0.09846 |  0:10:25s\n",
      "epoch 13 | loss: 6.17107 | train_balanced_accuracy: 0.10035 | valid_balanced_accuracy: 0.08512 |  0:11:13s\n",
      "epoch 14 | loss: 5.79077 | train_balanced_accuracy: 0.09483 | valid_balanced_accuracy: 0.09839 |  0:12:01s\n",
      "epoch 15 | loss: 5.45729 | train_balanced_accuracy: 0.101   | valid_balanced_accuracy: 0.0912  |  0:12:49s\n",
      "epoch 16 | loss: 5.13543 | train_balanced_accuracy: 0.10241 | valid_balanced_accuracy: 0.09341 |  0:13:36s\n",
      "epoch 17 | loss: 4.8773  | train_balanced_accuracy: 0.10759 | valid_balanced_accuracy: 0.0986  |  0:14:24s\n",
      "epoch 18 | loss: 4.6413  | train_balanced_accuracy: 0.11224 | valid_balanced_accuracy: 0.09854 |  0:15:12s\n",
      "epoch 19 | loss: 4.41466 | train_balanced_accuracy: 0.11062 | valid_balanced_accuracy: 0.10248 |  0:15:59s\n",
      "epoch 20 | loss: 4.24538 | train_balanced_accuracy: 0.10736 | valid_balanced_accuracy: 0.10296 |  0:16:47s\n",
      "epoch 21 | loss: 4.07163 | train_balanced_accuracy: 0.11755 | valid_balanced_accuracy: 0.1217  |  0:17:35s\n",
      "epoch 22 | loss: 3.91064 | train_balanced_accuracy: 0.11302 | valid_balanced_accuracy: 0.10875 |  0:18:22s\n",
      "epoch 23 | loss: 3.80461 | train_balanced_accuracy: 0.1208  | valid_balanced_accuracy: 0.1152  |  0:19:10s\n",
      "epoch 24 | loss: 3.69007 | train_balanced_accuracy: 0.11983 | valid_balanced_accuracy: 0.11553 |  0:19:57s\n",
      "epoch 25 | loss: 3.59534 | train_balanced_accuracy: 0.12981 | valid_balanced_accuracy: 0.1209  |  0:20:45s\n",
      "epoch 26 | loss: 3.51829 | train_balanced_accuracy: 0.12569 | valid_balanced_accuracy: 0.12242 |  0:21:33s\n",
      "epoch 27 | loss: 3.43289 | train_balanced_accuracy: 0.13861 | valid_balanced_accuracy: 0.12908 |  0:22:20s\n",
      "epoch 28 | loss: 3.36972 | train_balanced_accuracy: 0.13485 | valid_balanced_accuracy: 0.13152 |  0:23:08s\n",
      "epoch 29 | loss: 3.31368 | train_balanced_accuracy: 0.13705 | valid_balanced_accuracy: 0.13158 |  0:23:55s\n",
      "epoch 30 | loss: 3.25609 | train_balanced_accuracy: 0.13836 | valid_balanced_accuracy: 0.13581 |  0:24:43s\n",
      "epoch 31 | loss: 3.19777 | train_balanced_accuracy: 0.14486 | valid_balanced_accuracy: 0.13439 |  0:25:30s\n",
      "epoch 32 | loss: 3.15855 | train_balanced_accuracy: 0.14761 | valid_balanced_accuracy: 0.1436  |  0:26:18s\n",
      "epoch 33 | loss: 3.10623 | train_balanced_accuracy: 0.1524  | valid_balanced_accuracy: 0.14619 |  0:27:05s\n",
      "epoch 34 | loss: 3.06189 | train_balanced_accuracy: 0.15127 | valid_balanced_accuracy: 0.14767 |  0:27:52s\n",
      "epoch 35 | loss: 3.03292 | train_balanced_accuracy: 0.15232 | valid_balanced_accuracy: 0.15232 |  0:28:40s\n",
      "epoch 36 | loss: 2.98635 | train_balanced_accuracy: 0.1542  | valid_balanced_accuracy: 0.1489  |  0:29:27s\n",
      "epoch 37 | loss: 2.95821 | train_balanced_accuracy: 0.16832 | valid_balanced_accuracy: 0.15723 |  0:30:14s\n",
      "epoch 38 | loss: 2.92769 | train_balanced_accuracy: 0.1518  | valid_balanced_accuracy: 0.15235 |  0:31:02s\n",
      "epoch 39 | loss: 2.89503 | train_balanced_accuracy: 0.15955 | valid_balanced_accuracy: 0.15361 |  0:31:49s\n",
      "epoch 40 | loss: 2.86631 | train_balanced_accuracy: 0.15688 | valid_balanced_accuracy: 0.15827 |  0:32:36s\n",
      "epoch 41 | loss: 2.83693 | train_balanced_accuracy: 0.15884 | valid_balanced_accuracy: 0.15687 |  0:33:24s\n",
      "epoch 42 | loss: 2.81911 | train_balanced_accuracy: 0.16083 | valid_balanced_accuracy: 0.15629 |  0:34:11s\n",
      "epoch 43 | loss: 2.78953 | train_balanced_accuracy: 0.16573 | valid_balanced_accuracy: 0.16049 |  0:34:59s\n",
      "epoch 44 | loss: 2.76568 | train_balanced_accuracy: 0.1764  | valid_balanced_accuracy: 0.17255 |  0:35:46s\n",
      "epoch 45 | loss: 2.74908 | train_balanced_accuracy: 0.17043 | valid_balanced_accuracy: 0.17031 |  0:36:34s\n",
      "epoch 46 | loss: 2.72972 | train_balanced_accuracy: 0.16703 | valid_balanced_accuracy: 0.17054 |  0:37:21s\n",
      "epoch 47 | loss: 2.71183 | train_balanced_accuracy: 0.16838 | valid_balanced_accuracy: 0.17056 |  0:38:09s\n",
      "epoch 48 | loss: 2.70007 | train_balanced_accuracy: 0.17618 | valid_balanced_accuracy: 0.17733 |  0:38:56s\n",
      "epoch 49 | loss: 2.68331 | train_balanced_accuracy: 0.1772  | valid_balanced_accuracy: 0.17875 |  0:39:44s\n",
      "epoch 50 | loss: 2.67229 | train_balanced_accuracy: 0.17615 | valid_balanced_accuracy: 0.17699 |  0:40:31s\n",
      "epoch 51 | loss: 2.65821 | train_balanced_accuracy: 0.18393 | valid_balanced_accuracy: 0.18393 |  0:41:19s\n",
      "epoch 52 | loss: 2.63912 | train_balanced_accuracy: 0.18638 | valid_balanced_accuracy: 0.18846 |  0:42:06s\n",
      "epoch 53 | loss: 2.62776 | train_balanced_accuracy: 0.1835  | valid_balanced_accuracy: 0.18302 |  0:42:54s\n",
      "epoch 54 | loss: 2.60965 | train_balanced_accuracy: 0.177   | valid_balanced_accuracy: 0.17872 |  0:43:41s\n",
      "epoch 55 | loss: 2.59865 | train_balanced_accuracy: 0.1818  | valid_balanced_accuracy: 0.18266 |  0:44:29s\n",
      "epoch 56 | loss: 2.58664 | train_balanced_accuracy: 0.18592 | valid_balanced_accuracy: 0.18555 |  0:45:16s\n",
      "epoch 57 | loss: 2.57402 | train_balanced_accuracy: 0.18921 | valid_balanced_accuracy: 0.19276 |  0:46:03s\n",
      "epoch 58 | loss: 2.56784 | train_balanced_accuracy: 0.18972 | valid_balanced_accuracy: 0.19008 |  0:46:51s\n",
      "epoch 59 | loss: 2.55524 | train_balanced_accuracy: 0.18711 | valid_balanced_accuracy: 0.18655 |  0:47:38s\n",
      "epoch 60 | loss: 2.54386 | train_balanced_accuracy: 0.18327 | valid_balanced_accuracy: 0.18448 |  0:48:26s\n",
      "epoch 61 | loss: 2.53324 | train_balanced_accuracy: 0.19058 | valid_balanced_accuracy: 0.19154 |  0:49:13s\n",
      "epoch 62 | loss: 2.52819 | train_balanced_accuracy: 0.19688 | valid_balanced_accuracy: 0.19797 |  0:50:00s\n",
      "epoch 63 | loss: 2.51898 | train_balanced_accuracy: 0.19789 | valid_balanced_accuracy: 0.19986 |  0:50:48s\n",
      "epoch 64 | loss: 2.50589 | train_balanced_accuracy: 0.19805 | valid_balanced_accuracy: 0.19961 |  0:51:35s\n",
      "epoch 65 | loss: 2.50104 | train_balanced_accuracy: 0.20017 | valid_balanced_accuracy: 0.20223 |  0:52:22s\n",
      "epoch 66 | loss: 2.49091 | train_balanced_accuracy: 0.19772 | valid_balanced_accuracy: 0.20253 |  0:53:10s\n",
      "epoch 67 | loss: 2.47873 | train_balanced_accuracy: 0.20195 | valid_balanced_accuracy: 0.20554 |  0:53:57s\n",
      "epoch 68 | loss: 2.47676 | train_balanced_accuracy: 0.20187 | valid_balanced_accuracy: 0.20346 |  0:54:45s\n",
      "epoch 69 | loss: 2.4697  | train_balanced_accuracy: 0.20433 | valid_balanced_accuracy: 0.20317 |  0:55:32s\n",
      "epoch 70 | loss: 2.45988 | train_balanced_accuracy: 0.20828 | valid_balanced_accuracy: 0.20738 |  0:56:21s\n",
      "epoch 71 | loss: 2.45015 | train_balanced_accuracy: 0.20295 | valid_balanced_accuracy: 0.20493 |  0:57:08s\n",
      "epoch 72 | loss: 2.448   | train_balanced_accuracy: 0.20127 | valid_balanced_accuracy: 0.20395 |  0:57:56s\n",
      "epoch 73 | loss: 2.43953 | train_balanced_accuracy: 0.19988 | valid_balanced_accuracy: 0.19929 |  0:58:44s\n",
      "epoch 74 | loss: 2.42881 | train_balanced_accuracy: 0.2112  | valid_balanced_accuracy: 0.21318 |  0:59:33s\n",
      "epoch 75 | loss: 2.42243 | train_balanced_accuracy: 0.20846 | valid_balanced_accuracy: 0.20957 |  1:00:22s\n",
      "epoch 76 | loss: 2.41597 | train_balanced_accuracy: 0.20976 | valid_balanced_accuracy: 0.20961 |  1:01:11s\n",
      "epoch 77 | loss: 2.4095  | train_balanced_accuracy: 0.2089  | valid_balanced_accuracy: 0.21089 |  1:02:00s\n",
      "epoch 78 | loss: 2.40366 | train_balanced_accuracy: 0.20972 | valid_balanced_accuracy: 0.21137 |  1:02:49s\n",
      "epoch 79 | loss: 2.39217 | train_balanced_accuracy: 0.21556 | valid_balanced_accuracy: 0.2197  |  1:03:38s\n",
      "epoch 80 | loss: 2.38658 | train_balanced_accuracy: 0.20648 | valid_balanced_accuracy: 0.2063  |  1:04:27s\n",
      "epoch 81 | loss: 2.37936 | train_balanced_accuracy: 0.21182 | valid_balanced_accuracy: 0.21269 |  1:05:16s\n",
      "epoch 82 | loss: 2.37464 | train_balanced_accuracy: 0.21583 | valid_balanced_accuracy: 0.21422 |  1:06:04s\n",
      "epoch 83 | loss: 2.37353 | train_balanced_accuracy: 0.21071 | valid_balanced_accuracy: 0.21195 |  1:06:52s\n",
      "epoch 84 | loss: 2.36328 | train_balanced_accuracy: 0.21538 | valid_balanced_accuracy: 0.21925 |  1:07:40s\n",
      "epoch 85 | loss: 2.3624  | train_balanced_accuracy: 0.21897 | valid_balanced_accuracy: 0.22064 |  1:08:28s\n",
      "epoch 86 | loss: 2.35134 | train_balanced_accuracy: 0.2211  | valid_balanced_accuracy: 0.22272 |  1:09:16s\n",
      "epoch 87 | loss: 2.34992 | train_balanced_accuracy: 0.21434 | valid_balanced_accuracy: 0.21583 |  1:10:04s\n",
      "epoch 88 | loss: 2.34466 | train_balanced_accuracy: 0.22164 | valid_balanced_accuracy: 0.22067 |  1:10:53s\n",
      "epoch 89 | loss: 2.33696 | train_balanced_accuracy: 0.21946 | valid_balanced_accuracy: 0.22048 |  1:11:42s\n",
      "epoch 90 | loss: 2.32914 | train_balanced_accuracy: 0.21874 | valid_balanced_accuracy: 0.22158 |  1:12:30s\n",
      "epoch 91 | loss: 2.32547 | train_balanced_accuracy: 0.21578 | valid_balanced_accuracy: 0.21595 |  1:13:18s\n",
      "epoch 92 | loss: 2.32024 | train_balanced_accuracy: 0.22312 | valid_balanced_accuracy: 0.2241  |  1:14:05s\n",
      "epoch 93 | loss: 2.31798 | train_balanced_accuracy: 0.22359 | valid_balanced_accuracy: 0.22478 |  1:14:53s\n",
      "epoch 94 | loss: 2.31911 | train_balanced_accuracy: 0.22209 | valid_balanced_accuracy: 0.22218 |  1:15:40s\n",
      "epoch 95 | loss: 2.31054 | train_balanced_accuracy: 0.2207  | valid_balanced_accuracy: 0.22222 |  1:16:28s\n",
      "epoch 96 | loss: 2.30558 | train_balanced_accuracy: 0.2259  | valid_balanced_accuracy: 0.22725 |  1:17:16s\n",
      "epoch 97 | loss: 2.30379 | train_balanced_accuracy: 0.22151 | valid_balanced_accuracy: 0.22243 |  1:18:04s\n",
      "epoch 98 | loss: 2.29736 | train_balanced_accuracy: 0.22635 | valid_balanced_accuracy: 0.22997 |  1:18:52s\n",
      "epoch 99 | loss: 2.29362 | train_balanced_accuracy: 0.22056 | valid_balanced_accuracy: 0.22013 |  1:19:40s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 98 and best_valid_balanced_accuracy = 0.22997\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'step_size': 100}, optimizer_params={'lr': 1e-05}, n_steps=11, n_shared=1, n_independent=2, n_a=64, momentum=0.05, lambda_sparse=1, clip_value=0.5, total=79.9min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'step_size': 100}, optimizer_params={'lr': 1e-05}, n_steps=11, n_shared=1, n_independent=2, n_a=64, momentum=0.05, lambda_sparse=1, clip_value=0.5 \n",
      "epoch 0  | loss: 14.33254| train_balanced_accuracy: 0.05403 | valid_balanced_accuracy: 0.06833 |  0:00:48s\n",
      "epoch 1  | loss: 13.55817| train_balanced_accuracy: 0.0632  | valid_balanced_accuracy: 0.06784 |  0:01:36s\n",
      "epoch 2  | loss: 12.81092| train_balanced_accuracy: 0.04984 | valid_balanced_accuracy: 0.07517 |  0:02:25s\n",
      "epoch 3  | loss: 11.99153| train_balanced_accuracy: 0.05655 | valid_balanced_accuracy: 0.07282 |  0:03:15s\n",
      "epoch 4  | loss: 11.22488| train_balanced_accuracy: 0.05947 | valid_balanced_accuracy: 0.07626 |  0:04:04s\n",
      "epoch 5  | loss: 10.56241| train_balanced_accuracy: 0.06823 | valid_balanced_accuracy: 0.08313 |  0:04:53s\n",
      "epoch 6  | loss: 9.92786 | train_balanced_accuracy: 0.08698 | valid_balanced_accuracy: 0.09186 |  0:05:42s\n",
      "epoch 7  | loss: 9.30339 | train_balanced_accuracy: 0.08359 | valid_balanced_accuracy: 0.08856 |  0:06:32s\n",
      "epoch 8  | loss: 8.702   | train_balanced_accuracy: 0.07755 | valid_balanced_accuracy: 0.09123 |  0:07:21s\n",
      "epoch 9  | loss: 8.13197 | train_balanced_accuracy: 0.08621 | valid_balanced_accuracy: 0.0931  |  0:08:09s\n",
      "epoch 10 | loss: 7.57818 | train_balanced_accuracy: 0.08217 | valid_balanced_accuracy: 0.09718 |  0:08:58s\n",
      "epoch 11 | loss: 7.11321 | train_balanced_accuracy: 0.09004 | valid_balanced_accuracy: 0.10386 |  0:09:47s\n",
      "epoch 12 | loss: 6.5967  | train_balanced_accuracy: 0.09337 | valid_balanced_accuracy: 0.10618 |  0:10:36s\n",
      "epoch 13 | loss: 6.20157 | train_balanced_accuracy: 0.08926 | valid_balanced_accuracy: 0.1027  |  0:11:25s\n",
      "epoch 14 | loss: 5.84735 | train_balanced_accuracy: 0.09562 | valid_balanced_accuracy: 0.10748 |  0:12:13s\n",
      "epoch 15 | loss: 5.51573 | train_balanced_accuracy: 0.09604 | valid_balanced_accuracy: 0.10784 |  0:13:02s\n",
      "epoch 16 | loss: 5.1956  | train_balanced_accuracy: 0.09272 | valid_balanced_accuracy: 0.09263 |  0:13:51s\n",
      "epoch 17 | loss: 4.93355 | train_balanced_accuracy: 0.1025  | valid_balanced_accuracy: 0.11303 |  0:14:39s\n",
      "epoch 18 | loss: 4.69292 | train_balanced_accuracy: 0.10725 | valid_balanced_accuracy: 0.09762 |  0:15:28s\n",
      "epoch 19 | loss: 4.4821  | train_balanced_accuracy: 0.10835 | valid_balanced_accuracy: 0.09816 |  0:16:17s\n",
      "epoch 20 | loss: 4.30834 | train_balanced_accuracy: 0.10956 | valid_balanced_accuracy: 0.09903 |  0:17:05s\n",
      "epoch 21 | loss: 4.13384 | train_balanced_accuracy: 0.11214 | valid_balanced_accuracy: 0.10108 |  0:17:54s\n",
      "epoch 22 | loss: 3.9831  | train_balanced_accuracy: 0.11756 | valid_balanced_accuracy: 0.10622 |  0:18:42s\n",
      "epoch 23 | loss: 3.86917 | train_balanced_accuracy: 0.11708 | valid_balanced_accuracy: 0.10475 |  0:19:31s\n",
      "epoch 24 | loss: 3.76181 | train_balanced_accuracy: 0.12208 | valid_balanced_accuracy: 0.11086 |  0:20:20s\n",
      "epoch 25 | loss: 3.66155 | train_balanced_accuracy: 0.11512 | valid_balanced_accuracy: 0.11385 |  0:21:08s\n",
      "epoch 26 | loss: 3.5829  | train_balanced_accuracy: 0.11425 | valid_balanced_accuracy: 0.11214 |  0:21:57s\n",
      "epoch 27 | loss: 3.5032  | train_balanced_accuracy: 0.1342  | valid_balanced_accuracy: 0.12387 |  0:22:45s\n",
      "epoch 28 | loss: 3.42805 | train_balanced_accuracy: 0.13153 | valid_balanced_accuracy: 0.1213  |  0:23:34s\n",
      "epoch 29 | loss: 3.36803 | train_balanced_accuracy: 0.1251  | valid_balanced_accuracy: 0.12396 |  0:24:23s\n",
      "epoch 30 | loss: 3.30156 | train_balanced_accuracy: 0.12857 | valid_balanced_accuracy: 0.127   |  0:25:11s\n",
      "epoch 31 | loss: 3.25288 | train_balanced_accuracy: 0.13291 | valid_balanced_accuracy: 0.13276 |  0:26:00s\n",
      "epoch 32 | loss: 3.20511 | train_balanced_accuracy: 0.13312 | valid_balanced_accuracy: 0.13297 |  0:26:48s\n",
      "epoch 33 | loss: 3.15614 | train_balanced_accuracy: 0.13242 | valid_balanced_accuracy: 0.1321  |  0:27:37s\n",
      "epoch 34 | loss: 3.11768 | train_balanced_accuracy: 0.13688 | valid_balanced_accuracy: 0.13614 |  0:28:25s\n",
      "epoch 35 | loss: 3.07756 | train_balanced_accuracy: 0.13694 | valid_balanced_accuracy: 0.13577 |  0:29:14s\n",
      "epoch 36 | loss: 3.03738 | train_balanced_accuracy: 0.14342 | valid_balanced_accuracy: 0.1442  |  0:30:02s\n",
      "epoch 37 | loss: 3.00938 | train_balanced_accuracy: 0.14182 | valid_balanced_accuracy: 0.1402  |  0:30:51s\n",
      "epoch 38 | loss: 2.97076 | train_balanced_accuracy: 0.14584 | valid_balanced_accuracy: 0.1453  |  0:31:39s\n",
      "epoch 39 | loss: 2.93661 | train_balanced_accuracy: 0.15187 | valid_balanced_accuracy: 0.15184 |  0:32:28s\n",
      "epoch 40 | loss: 2.91416 | train_balanced_accuracy: 0.14799 | valid_balanced_accuracy: 0.14622 |  0:33:16s\n",
      "epoch 41 | loss: 2.88339 | train_balanced_accuracy: 0.15288 | valid_balanced_accuracy: 0.15283 |  0:34:05s\n",
      "epoch 42 | loss: 2.85242 | train_balanced_accuracy: 0.15544 | valid_balanced_accuracy: 0.154   |  0:34:53s\n",
      "epoch 43 | loss: 2.82736 | train_balanced_accuracy: 0.15774 | valid_balanced_accuracy: 0.15859 |  0:35:42s\n",
      "epoch 44 | loss: 2.81105 | train_balanced_accuracy: 0.16374 | valid_balanced_accuracy: 0.16412 |  0:36:30s\n",
      "epoch 45 | loss: 2.77973 | train_balanced_accuracy: 0.16303 | valid_balanced_accuracy: 0.16272 |  0:37:19s\n",
      "epoch 46 | loss: 2.76848 | train_balanced_accuracy: 0.16422 | valid_balanced_accuracy: 0.16489 |  0:38:07s\n",
      "epoch 47 | loss: 2.73484 | train_balanced_accuracy: 0.17195 | valid_balanced_accuracy: 0.1731  |  0:38:56s\n",
      "epoch 48 | loss: 2.72251 | train_balanced_accuracy: 0.16487 | valid_balanced_accuracy: 0.16573 |  0:39:45s\n",
      "epoch 49 | loss: 2.70219 | train_balanced_accuracy: 0.17441 | valid_balanced_accuracy: 0.17354 |  0:40:33s\n",
      "epoch 50 | loss: 2.6783  | train_balanced_accuracy: 0.17608 | valid_balanced_accuracy: 0.17725 |  0:41:22s\n",
      "epoch 51 | loss: 2.66629 | train_balanced_accuracy: 0.17586 | valid_balanced_accuracy: 0.17738 |  0:42:10s\n",
      "epoch 52 | loss: 2.65078 | train_balanced_accuracy: 0.18235 | valid_balanced_accuracy: 0.18248 |  0:42:58s\n",
      "epoch 53 | loss: 2.63533 | train_balanced_accuracy: 0.17682 | valid_balanced_accuracy: 0.17871 |  0:43:47s\n",
      "epoch 54 | loss: 2.62731 | train_balanced_accuracy: 0.18064 | valid_balanced_accuracy: 0.18065 |  0:44:35s\n",
      "epoch 55 | loss: 2.6081  | train_balanced_accuracy: 0.17387 | valid_balanced_accuracy: 0.17697 |  0:45:24s\n",
      "epoch 56 | loss: 2.59328 | train_balanced_accuracy: 0.18308 | valid_balanced_accuracy: 0.18584 |  0:46:13s\n",
      "epoch 57 | loss: 2.57813 | train_balanced_accuracy: 0.18254 | valid_balanced_accuracy: 0.18351 |  0:47:02s\n",
      "epoch 58 | loss: 2.56969 | train_balanced_accuracy: 0.18784 | valid_balanced_accuracy: 0.19081 |  0:47:51s\n",
      "epoch 59 | loss: 2.55424 | train_balanced_accuracy: 0.19044 | valid_balanced_accuracy: 0.1889  |  0:48:40s\n",
      "epoch 60 | loss: 2.54346 | train_balanced_accuracy: 0.18861 | valid_balanced_accuracy: 0.19107 |  0:49:29s\n",
      "epoch 61 | loss: 2.5372  | train_balanced_accuracy: 0.18629 | valid_balanced_accuracy: 0.19035 |  0:50:17s\n",
      "epoch 62 | loss: 2.52475 | train_balanced_accuracy: 0.19473 | valid_balanced_accuracy: 0.19592 |  0:51:06s\n",
      "epoch 63 | loss: 2.51439 | train_balanced_accuracy: 0.19694 | valid_balanced_accuracy: 0.19738 |  0:51:54s\n",
      "epoch 64 | loss: 2.50875 | train_balanced_accuracy: 0.19251 | valid_balanced_accuracy: 0.19403 |  0:52:43s\n",
      "epoch 65 | loss: 2.49683 | train_balanced_accuracy: 0.19872 | valid_balanced_accuracy: 0.20207 |  0:53:31s\n",
      "epoch 66 | loss: 2.48669 | train_balanced_accuracy: 0.19695 | valid_balanced_accuracy: 0.19949 |  0:54:19s\n",
      "epoch 67 | loss: 2.475   | train_balanced_accuracy: 0.19825 | valid_balanced_accuracy: 0.19976 |  0:55:08s\n",
      "epoch 68 | loss: 2.4662  | train_balanced_accuracy: 0.20129 | valid_balanced_accuracy: 0.20146 |  0:55:56s\n",
      "epoch 69 | loss: 2.4618  | train_balanced_accuracy: 0.20125 | valid_balanced_accuracy: 0.2034  |  0:56:43s\n",
      "epoch 70 | loss: 2.45739 | train_balanced_accuracy: 0.2013  | valid_balanced_accuracy: 0.20482 |  0:57:33s\n",
      "epoch 71 | loss: 2.44583 | train_balanced_accuracy: 0.20244 | valid_balanced_accuracy: 0.20359 |  0:58:22s\n",
      "epoch 72 | loss: 2.43903 | train_balanced_accuracy: 0.20436 | valid_balanced_accuracy: 0.20439 |  0:59:11s\n",
      "epoch 73 | loss: 2.42641 | train_balanced_accuracy: 0.20622 | valid_balanced_accuracy: 0.20784 |  1:00:00s\n",
      "epoch 74 | loss: 2.41796 | train_balanced_accuracy: 0.20332 | valid_balanced_accuracy: 0.20068 |  1:00:49s\n",
      "epoch 75 | loss: 2.41815 | train_balanced_accuracy: 0.21112 | valid_balanced_accuracy: 0.21179 |  1:01:38s\n",
      "epoch 76 | loss: 2.40821 | train_balanced_accuracy: 0.20975 | valid_balanced_accuracy: 0.21324 |  1:02:28s\n",
      "epoch 77 | loss: 2.40287 | train_balanced_accuracy: 0.21284 | valid_balanced_accuracy: 0.21184 |  1:03:17s\n",
      "epoch 78 | loss: 2.39478 | train_balanced_accuracy: 0.20958 | valid_balanced_accuracy: 0.21185 |  1:04:06s\n",
      "epoch 79 | loss: 2.38749 | train_balanced_accuracy: 0.20831 | valid_balanced_accuracy: 0.20844 |  1:04:55s\n",
      "epoch 80 | loss: 2.38121 | train_balanced_accuracy: 0.21016 | valid_balanced_accuracy: 0.21134 |  1:05:45s\n",
      "epoch 81 | loss: 2.37618 | train_balanced_accuracy: 0.21303 | valid_balanced_accuracy: 0.21484 |  1:06:33s\n",
      "epoch 82 | loss: 2.37189 | train_balanced_accuracy: 0.21394 | valid_balanced_accuracy: 0.21834 |  1:07:20s\n",
      "epoch 83 | loss: 2.36737 | train_balanced_accuracy: 0.21552 | valid_balanced_accuracy: 0.21321 |  1:08:09s\n",
      "epoch 84 | loss: 2.356   | train_balanced_accuracy: 0.2188  | valid_balanced_accuracy: 0.21877 |  1:08:57s\n",
      "epoch 85 | loss: 2.34993 | train_balanced_accuracy: 0.21562 | valid_balanced_accuracy: 0.21507 |  1:09:45s\n",
      "epoch 86 | loss: 2.34669 | train_balanced_accuracy: 0.21716 | valid_balanced_accuracy: 0.218   |  1:10:34s\n",
      "epoch 87 | loss: 2.34057 | train_balanced_accuracy: 0.2215  | valid_balanced_accuracy: 0.21999 |  1:11:23s\n",
      "epoch 88 | loss: 2.33895 | train_balanced_accuracy: 0.21502 | valid_balanced_accuracy: 0.2188  |  1:12:13s\n",
      "epoch 89 | loss: 2.33087 | train_balanced_accuracy: 0.21784 | valid_balanced_accuracy: 0.21847 |  1:13:02s\n",
      "epoch 90 | loss: 2.32599 | train_balanced_accuracy: 0.21513 | valid_balanced_accuracy: 0.21673 |  1:13:51s\n",
      "epoch 91 | loss: 2.32391 | train_balanced_accuracy: 0.22401 | valid_balanced_accuracy: 0.22355 |  1:14:40s\n",
      "epoch 92 | loss: 2.319   | train_balanced_accuracy: 0.21924 | valid_balanced_accuracy: 0.21981 |  1:15:28s\n",
      "epoch 93 | loss: 2.31399 | train_balanced_accuracy: 0.21718 | valid_balanced_accuracy: 0.21773 |  1:16:16s\n",
      "epoch 94 | loss: 2.31303 | train_balanced_accuracy: 0.21845 | valid_balanced_accuracy: 0.2193  |  1:17:04s\n",
      "epoch 95 | loss: 2.30256 | train_balanced_accuracy: 0.22035 | valid_balanced_accuracy: 0.22304 |  1:17:53s\n",
      "epoch 96 | loss: 2.29968 | train_balanced_accuracy: 0.22459 | valid_balanced_accuracy: 0.22359 |  1:18:42s\n",
      "epoch 97 | loss: 2.29336 | train_balanced_accuracy: 0.22733 | valid_balanced_accuracy: 0.22779 |  1:19:31s\n",
      "epoch 98 | loss: 2.29166 | train_balanced_accuracy: 0.21987 | valid_balanced_accuracy: 0.22037 |  1:20:20s\n",
      "epoch 99 | loss: 2.28625 | train_balanced_accuracy: 0.22396 | valid_balanced_accuracy: 0.22547 |  1:21:08s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 97 and best_valid_balanced_accuracy = 0.22779\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'step_size': 100}, optimizer_params={'lr': 1e-05}, n_steps=11, n_shared=1, n_independent=2, n_a=64, momentum=0.05, lambda_sparse=1, clip_value=0.5, total=81.4min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'step_size': 100}, optimizer_params={'lr': 1e-05}, n_steps=11, n_shared=1, n_independent=2, n_a=64, momentum=0.05, lambda_sparse=1, clip_value=0.5 \n",
      "epoch 0  | loss: 14.29688| train_balanced_accuracy: 0.07378 | valid_balanced_accuracy: 0.07241 |  0:00:48s\n",
      "epoch 1  | loss: 13.53356| train_balanced_accuracy: 0.06697 | valid_balanced_accuracy: 0.06902 |  0:01:37s\n",
      "epoch 2  | loss: 12.72806| train_balanced_accuracy: 0.07448 | valid_balanced_accuracy: 0.08025 |  0:02:25s\n",
      "epoch 3  | loss: 11.99705| train_balanced_accuracy: 0.06729 | valid_balanced_accuracy: 0.07104 |  0:03:14s\n",
      "epoch 4  | loss: 11.3246 | train_balanced_accuracy: 0.07876 | valid_balanced_accuracy: 0.07566 |  0:04:03s\n",
      "epoch 5  | loss: 10.65489| train_balanced_accuracy: 0.0787  | valid_balanced_accuracy: 0.08194 |  0:04:51s\n",
      "epoch 6  | loss: 9.97113 | train_balanced_accuracy: 0.08881 | valid_balanced_accuracy: 0.09367 |  0:05:40s\n",
      "epoch 7  | loss: 9.33462 | train_balanced_accuracy: 0.10175 | valid_balanced_accuracy: 0.09975 |  0:06:29s\n",
      "epoch 8  | loss: 8.71458 | train_balanced_accuracy: 0.09936 | valid_balanced_accuracy: 0.10244 |  0:07:17s\n",
      "epoch 9  | loss: 8.13679 | train_balanced_accuracy: 0.09843 | valid_balanced_accuracy: 0.10305 |  0:08:05s\n",
      "epoch 10 | loss: 7.61974 | train_balanced_accuracy: 0.10507 | valid_balanced_accuracy: 0.10184 |  0:08:53s\n",
      "epoch 11 | loss: 7.13886 | train_balanced_accuracy: 0.10808 | valid_balanced_accuracy: 0.10498 |  0:09:42s\n",
      "epoch 12 | loss: 6.64663 | train_balanced_accuracy: 0.10661 | valid_balanced_accuracy: 0.1124  |  0:10:30s\n",
      "epoch 13 | loss: 6.22937 | train_balanced_accuracy: 0.10551 | valid_balanced_accuracy: 0.10681 |  0:11:18s\n",
      "epoch 14 | loss: 5.89066 | train_balanced_accuracy: 0.10342 | valid_balanced_accuracy: 0.08515 |  0:12:06s\n",
      "epoch 15 | loss: 5.56249 | train_balanced_accuracy: 0.11497 | valid_balanced_accuracy: 0.11614 |  0:12:54s\n",
      "epoch 16 | loss: 5.23403 | train_balanced_accuracy: 0.10838 | valid_balanced_accuracy: 0.1137  |  0:13:41s\n",
      "epoch 17 | loss: 4.92945 | train_balanced_accuracy: 0.10725 | valid_balanced_accuracy: 0.11582 |  0:14:28s\n",
      "epoch 18 | loss: 4.69977 | train_balanced_accuracy: 0.11758 | valid_balanced_accuracy: 0.09621 |  0:15:16s\n",
      "epoch 19 | loss: 4.48979 | train_balanced_accuracy: 0.11321 | valid_balanced_accuracy: 0.09563 |  0:16:03s\n",
      "epoch 20 | loss: 4.3026  | train_balanced_accuracy: 0.11527 | valid_balanced_accuracy: 0.12019 |  0:16:50s\n",
      "epoch 21 | loss: 4.12924 | train_balanced_accuracy: 0.11839 | valid_balanced_accuracy: 0.12449 |  0:17:37s\n",
      "epoch 22 | loss: 3.96736 | train_balanced_accuracy: 0.12209 | valid_balanced_accuracy: 0.10637 |  0:18:24s\n",
      "epoch 23 | loss: 3.85678 | train_balanced_accuracy: 0.12966 | valid_balanced_accuracy: 0.11263 |  0:19:11s\n",
      "epoch 24 | loss: 3.74233 | train_balanced_accuracy: 0.12964 | valid_balanced_accuracy: 0.11398 |  0:19:59s\n",
      "epoch 25 | loss: 3.66198 | train_balanced_accuracy: 0.1292  | valid_balanced_accuracy: 0.11627 |  0:20:46s\n",
      "epoch 26 | loss: 3.56439 | train_balanced_accuracy: 0.12458 | valid_balanced_accuracy: 0.11578 |  0:21:34s\n",
      "epoch 27 | loss: 3.4913  | train_balanced_accuracy: 0.13989 | valid_balanced_accuracy: 0.12295 |  0:22:21s\n",
      "epoch 28 | loss: 3.42479 | train_balanced_accuracy: 0.13653 | valid_balanced_accuracy: 0.12502 |  0:23:09s\n",
      "epoch 29 | loss: 3.36792 | train_balanced_accuracy: 0.13336 | valid_balanced_accuracy: 0.12584 |  0:23:56s\n",
      "epoch 30 | loss: 3.31063 | train_balanced_accuracy: 0.1447  | valid_balanced_accuracy: 0.13284 |  0:24:44s\n",
      "epoch 31 | loss: 3.24986 | train_balanced_accuracy: 0.1429  | valid_balanced_accuracy: 0.13387 |  0:25:31s\n",
      "epoch 32 | loss: 3.20275 | train_balanced_accuracy: 0.14366 | valid_balanced_accuracy: 0.13618 |  0:26:19s\n",
      "epoch 33 | loss: 3.16187 | train_balanced_accuracy: 0.14151 | valid_balanced_accuracy: 0.13475 |  0:27:06s\n",
      "epoch 34 | loss: 3.11517 | train_balanced_accuracy: 0.145   | valid_balanced_accuracy: 0.13862 |  0:27:54s\n",
      "epoch 35 | loss: 3.07841 | train_balanced_accuracy: 0.15175 | valid_balanced_accuracy: 0.14274 |  0:28:41s\n",
      "epoch 36 | loss: 3.04166 | train_balanced_accuracy: 0.15485 | valid_balanced_accuracy: 0.1476  |  0:29:29s\n",
      "epoch 37 | loss: 3.00342 | train_balanced_accuracy: 0.15729 | valid_balanced_accuracy: 0.14959 |  0:30:17s\n",
      "epoch 38 | loss: 2.97134 | train_balanced_accuracy: 0.15749 | valid_balanced_accuracy: 0.15034 |  0:31:05s\n",
      "epoch 39 | loss: 2.94189 | train_balanced_accuracy: 0.15546 | valid_balanced_accuracy: 0.15309 |  0:31:54s\n",
      "epoch 40 | loss: 2.91634 | train_balanced_accuracy: 0.15677 | valid_balanced_accuracy: 0.15144 |  0:32:43s\n",
      "epoch 41 | loss: 2.88149 | train_balanced_accuracy: 0.15898 | valid_balanced_accuracy: 0.15665 |  0:33:32s\n",
      "epoch 42 | loss: 2.85562 | train_balanced_accuracy: 0.15701 | valid_balanced_accuracy: 0.15445 |  0:34:22s\n",
      "epoch 43 | loss: 2.83017 | train_balanced_accuracy: 0.1556  | valid_balanced_accuracy: 0.15732 |  0:35:10s\n",
      "epoch 44 | loss: 2.80581 | train_balanced_accuracy: 0.16648 | valid_balanced_accuracy: 0.16735 |  0:35:59s\n",
      "epoch 45 | loss: 2.78675 | train_balanced_accuracy: 0.16497 | valid_balanced_accuracy: 0.1691  |  0:36:47s\n",
      "epoch 46 | loss: 2.75944 | train_balanced_accuracy: 0.16858 | valid_balanced_accuracy: 0.1647  |  0:37:35s\n",
      "epoch 47 | loss: 2.74019 | train_balanced_accuracy: 0.17119 | valid_balanced_accuracy: 0.17424 |  0:38:22s\n",
      "epoch 48 | loss: 2.72498 | train_balanced_accuracy: 0.17328 | valid_balanced_accuracy: 0.17356 |  0:39:10s\n",
      "epoch 49 | loss: 2.70214 | train_balanced_accuracy: 0.17353 | valid_balanced_accuracy: 0.17485 |  0:39:57s\n",
      "epoch 50 | loss: 2.68244 | train_balanced_accuracy: 0.17862 | valid_balanced_accuracy: 0.18029 |  0:40:46s\n",
      "epoch 51 | loss: 2.66976 | train_balanced_accuracy: 0.17619 | valid_balanced_accuracy: 0.1801  |  0:41:35s\n",
      "epoch 52 | loss: 2.65583 | train_balanced_accuracy: 0.18061 | valid_balanced_accuracy: 0.18244 |  0:42:24s\n",
      "epoch 53 | loss: 2.64432 | train_balanced_accuracy: 0.17858 | valid_balanced_accuracy: 0.18082 |  0:43:12s\n",
      "epoch 54 | loss: 2.62618 | train_balanced_accuracy: 0.18077 | valid_balanced_accuracy: 0.18233 |  0:44:00s\n",
      "epoch 55 | loss: 2.61206 | train_balanced_accuracy: 0.18395 | valid_balanced_accuracy: 0.18685 |  0:44:48s\n",
      "epoch 56 | loss: 2.60531 | train_balanced_accuracy: 0.1835  | valid_balanced_accuracy: 0.18602 |  0:45:35s\n",
      "epoch 57 | loss: 2.5827  | train_balanced_accuracy: 0.18612 | valid_balanced_accuracy: 0.18847 |  0:46:23s\n",
      "epoch 58 | loss: 2.58019 | train_balanced_accuracy: 0.19219 | valid_balanced_accuracy: 0.19556 |  0:47:13s\n",
      "epoch 59 | loss: 2.56583 | train_balanced_accuracy: 0.1824  | valid_balanced_accuracy: 0.1835  |  0:48:02s\n",
      "epoch 60 | loss: 2.55435 | train_balanced_accuracy: 0.18716 | valid_balanced_accuracy: 0.18997 |  0:48:51s\n",
      "epoch 61 | loss: 2.54637 | train_balanced_accuracy: 0.18691 | valid_balanced_accuracy: 0.1919  |  0:49:38s\n",
      "epoch 62 | loss: 2.53326 | train_balanced_accuracy: 0.18613 | valid_balanced_accuracy: 0.18798 |  0:50:28s\n",
      "epoch 63 | loss: 2.52235 | train_balanced_accuracy: 0.18913 | valid_balanced_accuracy: 0.19099 |  0:51:17s\n",
      "epoch 64 | loss: 2.51179 | train_balanced_accuracy: 0.18989 | valid_balanced_accuracy: 0.19222 |  0:52:06s\n",
      "epoch 65 | loss: 2.49968 | train_balanced_accuracy: 0.19686 | valid_balanced_accuracy: 0.2016  |  0:52:55s\n",
      "epoch 66 | loss: 2.49013 | train_balanced_accuracy: 0.19373 | valid_balanced_accuracy: 0.19587 |  0:53:44s\n",
      "epoch 67 | loss: 2.4836  | train_balanced_accuracy: 0.195   | valid_balanced_accuracy: 0.19735 |  0:54:33s\n",
      "epoch 68 | loss: 2.47649 | train_balanced_accuracy: 0.19028 | valid_balanced_accuracy: 0.19297 |  0:55:22s\n",
      "epoch 69 | loss: 2.46773 | train_balanced_accuracy: 0.19676 | valid_balanced_accuracy: 0.19981 |  0:56:12s\n",
      "epoch 70 | loss: 2.45876 | train_balanced_accuracy: 0.19006 | valid_balanced_accuracy: 0.19443 |  0:57:01s\n",
      "epoch 71 | loss: 2.45215 | train_balanced_accuracy: 0.19288 | valid_balanced_accuracy: 0.19841 |  0:57:49s\n",
      "epoch 72 | loss: 2.44401 | train_balanced_accuracy: 0.1958  | valid_balanced_accuracy: 0.20028 |  0:58:37s\n",
      "epoch 73 | loss: 2.43874 | train_balanced_accuracy: 0.19068 | valid_balanced_accuracy: 0.19381 |  0:59:25s\n",
      "epoch 74 | loss: 2.42987 | train_balanced_accuracy: 0.19878 | valid_balanced_accuracy: 0.20018 |  1:00:13s\n",
      "epoch 75 | loss: 2.4218  | train_balanced_accuracy: 0.19608 | valid_balanced_accuracy: 0.1997  |  1:01:01s\n",
      "\n",
      "Early stopping occured at epoch 75 with best_epoch = 65 and best_valid_balanced_accuracy = 0.2016\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'step_size': 100}, optimizer_params={'lr': 1e-05}, n_steps=11, n_shared=1, n_independent=2, n_a=64, momentum=0.05, lambda_sparse=1, clip_value=0.5, total=61.3min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'step_size': 100}, optimizer_params={'lr': 1e-05}, n_steps=11, n_shared=1, n_independent=2, n_a=64, momentum=0.05, lambda_sparse=1, clip_value=0.5 \n",
      "epoch 0  | loss: 14.3407 | train_balanced_accuracy: 0.06922 | valid_balanced_accuracy: 0.06126 |  0:00:47s\n",
      "epoch 1  | loss: 13.4981 | train_balanced_accuracy: 0.06096 | valid_balanced_accuracy: 0.05964 |  0:01:35s\n",
      "epoch 2  | loss: 12.72445| train_balanced_accuracy: 0.07385 | valid_balanced_accuracy: 0.06852 |  0:02:22s\n",
      "epoch 3  | loss: 11.98032| train_balanced_accuracy: 0.07082 | valid_balanced_accuracy: 0.06397 |  0:03:10s\n",
      "epoch 4  | loss: 11.33786| train_balanced_accuracy: 0.08016 | valid_balanced_accuracy: 0.07978 |  0:03:57s\n",
      "epoch 5  | loss: 10.72756| train_balanced_accuracy: 0.0864  | valid_balanced_accuracy: 0.08101 |  0:04:45s\n",
      "epoch 6  | loss: 10.08167| train_balanced_accuracy: 0.08827 | valid_balanced_accuracy: 0.08211 |  0:05:33s\n",
      "epoch 7  | loss: 9.52214 | train_balanced_accuracy: 0.07843 | valid_balanced_accuracy: 0.07924 |  0:06:20s\n",
      "epoch 8  | loss: 8.88303 | train_balanced_accuracy: 0.09607 | valid_balanced_accuracy: 0.08996 |  0:07:09s\n",
      "epoch 9  | loss: 8.37173 | train_balanced_accuracy: 0.08857 | valid_balanced_accuracy: 0.08817 |  0:07:58s\n",
      "epoch 10 | loss: 7.88013 | train_balanced_accuracy: 0.08707 | valid_balanced_accuracy: 0.08574 |  0:08:47s\n",
      "epoch 11 | loss: 7.35778 | train_balanced_accuracy: 0.09543 | valid_balanced_accuracy: 0.09442 |  0:09:36s\n",
      "epoch 12 | loss: 6.92835 | train_balanced_accuracy: 0.09713 | valid_balanced_accuracy: 0.10263 |  0:10:25s\n",
      "epoch 13 | loss: 6.47159 | train_balanced_accuracy: 0.09076 | valid_balanced_accuracy: 0.09513 |  0:11:12s\n",
      "epoch 14 | loss: 6.03937 | train_balanced_accuracy: 0.09767 | valid_balanced_accuracy: 0.08137 |  0:12:00s\n",
      "epoch 15 | loss: 5.66773 | train_balanced_accuracy: 0.09649 | valid_balanced_accuracy: 0.09178 |  0:12:47s\n",
      "epoch 16 | loss: 5.3492  | train_balanced_accuracy: 0.0977  | valid_balanced_accuracy: 0.08546 |  0:13:34s\n",
      "epoch 17 | loss: 5.08148 | train_balanced_accuracy: 0.10459 | valid_balanced_accuracy: 0.09349 |  0:14:21s\n",
      "epoch 18 | loss: 4.8102  | train_balanced_accuracy: 0.10724 | valid_balanced_accuracy: 0.0961  |  0:15:09s\n",
      "epoch 19 | loss: 4.57085 | train_balanced_accuracy: 0.11196 | valid_balanced_accuracy: 0.11651 |  0:15:56s\n",
      "epoch 20 | loss: 4.36279 | train_balanced_accuracy: 0.12003 | valid_balanced_accuracy: 0.11369 |  0:16:43s\n",
      "epoch 21 | loss: 4.19459 | train_balanced_accuracy: 0.10848 | valid_balanced_accuracy: 0.1175  |  0:17:30s\n",
      "epoch 22 | loss: 4.04781 | train_balanced_accuracy: 0.10674 | valid_balanced_accuracy: 0.12281 |  0:18:18s\n",
      "epoch 23 | loss: 3.93635 | train_balanced_accuracy: 0.10775 | valid_balanced_accuracy: 0.109   |  0:19:05s\n",
      "epoch 24 | loss: 3.81011 | train_balanced_accuracy: 0.12223 | valid_balanced_accuracy: 0.11069 |  0:19:52s\n",
      "epoch 25 | loss: 3.71063 | train_balanced_accuracy: 0.1214  | valid_balanced_accuracy: 0.11568 |  0:20:39s\n",
      "epoch 26 | loss: 3.61332 | train_balanced_accuracy: 0.1296  | valid_balanced_accuracy: 0.12227 |  0:21:27s\n",
      "epoch 27 | loss: 3.52919 | train_balanced_accuracy: 0.11894 | valid_balanced_accuracy: 0.13488 |  0:22:14s\n",
      "epoch 28 | loss: 3.46214 | train_balanced_accuracy: 0.13498 | valid_balanced_accuracy: 0.12599 |  0:23:01s\n",
      "epoch 29 | loss: 3.39822 | train_balanced_accuracy: 0.12408 | valid_balanced_accuracy: 0.12462 |  0:23:48s\n",
      "epoch 30 | loss: 3.33971 | train_balanced_accuracy: 0.14038 | valid_balanced_accuracy: 0.13016 |  0:24:35s\n",
      "epoch 31 | loss: 3.28305 | train_balanced_accuracy: 0.13809 | valid_balanced_accuracy: 0.1337  |  0:25:23s\n",
      "epoch 32 | loss: 3.23229 | train_balanced_accuracy: 0.135   | valid_balanced_accuracy: 0.13033 |  0:26:10s\n",
      "epoch 33 | loss: 3.18205 | train_balanced_accuracy: 0.1471  | valid_balanced_accuracy: 0.13766 |  0:26:57s\n",
      "epoch 34 | loss: 3.13804 | train_balanced_accuracy: 0.13472 | valid_balanced_accuracy: 0.13698 |  0:27:45s\n",
      "epoch 35 | loss: 3.09932 | train_balanced_accuracy: 0.14865 | valid_balanced_accuracy: 0.14325 |  0:28:33s\n",
      "epoch 36 | loss: 3.06467 | train_balanced_accuracy: 0.14303 | valid_balanced_accuracy: 0.14194 |  0:29:20s\n",
      "epoch 37 | loss: 3.02264 | train_balanced_accuracy: 0.14768 | valid_balanced_accuracy: 0.14337 |  0:30:07s\n",
      "epoch 38 | loss: 2.992   | train_balanced_accuracy: 0.14893 | valid_balanced_accuracy: 0.14879 |  0:30:54s\n",
      "epoch 39 | loss: 2.96011 | train_balanced_accuracy: 0.15426 | valid_balanced_accuracy: 0.14866 |  0:31:41s\n",
      "epoch 40 | loss: 2.93593 | train_balanced_accuracy: 0.15408 | valid_balanced_accuracy: 0.15128 |  0:32:28s\n",
      "epoch 41 | loss: 2.90892 | train_balanced_accuracy: 0.15601 | valid_balanced_accuracy: 0.15602 |  0:33:15s\n",
      "epoch 42 | loss: 2.88972 | train_balanced_accuracy: 0.15346 | valid_balanced_accuracy: 0.1474  |  0:34:03s\n",
      "epoch 43 | loss: 2.86139 | train_balanced_accuracy: 0.16546 | valid_balanced_accuracy: 0.15893 |  0:34:51s\n",
      "epoch 44 | loss: 2.83844 | train_balanced_accuracy: 0.16737 | valid_balanced_accuracy: 0.15702 |  0:35:38s\n",
      "epoch 45 | loss: 2.81298 | train_balanced_accuracy: 0.15458 | valid_balanced_accuracy: 0.15588 |  0:36:26s\n",
      "epoch 46 | loss: 2.79485 | train_balanced_accuracy: 0.15857 | valid_balanced_accuracy: 0.15381 |  0:37:13s\n",
      "epoch 47 | loss: 2.77723 | train_balanced_accuracy: 0.16179 | valid_balanced_accuracy: 0.16522 |  0:38:01s\n",
      "epoch 48 | loss: 2.75457 | train_balanced_accuracy: 0.17058 | valid_balanced_accuracy: 0.17047 |  0:38:49s\n",
      "epoch 49 | loss: 2.73798 | train_balanced_accuracy: 0.16978 | valid_balanced_accuracy: 0.17092 |  0:39:36s\n",
      "epoch 50 | loss: 2.71738 | train_balanced_accuracy: 0.16977 | valid_balanced_accuracy: 0.17132 |  0:40:24s\n",
      "epoch 51 | loss: 2.7058  | train_balanced_accuracy: 0.17918 | valid_balanced_accuracy: 0.17408 |  0:41:11s\n",
      "epoch 52 | loss: 2.68601 | train_balanced_accuracy: 0.16747 | valid_balanced_accuracy: 0.16654 |  0:41:59s\n",
      "epoch 53 | loss: 2.67469 | train_balanced_accuracy: 0.17969 | valid_balanced_accuracy: 0.1777  |  0:42:47s\n",
      "epoch 54 | loss: 2.6599  | train_balanced_accuracy: 0.17813 | valid_balanced_accuracy: 0.17848 |  0:43:34s\n",
      "epoch 55 | loss: 2.64355 | train_balanced_accuracy: 0.18118 | valid_balanced_accuracy: 0.1844  |  0:44:22s\n",
      "epoch 56 | loss: 2.63231 | train_balanced_accuracy: 0.18125 | valid_balanced_accuracy: 0.18048 |  0:45:11s\n",
      "epoch 57 | loss: 2.6174  | train_balanced_accuracy: 0.1856  | valid_balanced_accuracy: 0.18832 |  0:46:00s\n",
      "epoch 58 | loss: 2.60508 | train_balanced_accuracy: 0.18166 | valid_balanced_accuracy: 0.18106 |  0:46:49s\n",
      "epoch 59 | loss: 2.59807 | train_balanced_accuracy: 0.17553 | valid_balanced_accuracy: 0.17661 |  0:47:38s\n",
      "epoch 60 | loss: 2.58243 | train_balanced_accuracy: 0.18437 | valid_balanced_accuracy: 0.18665 |  0:48:27s\n",
      "epoch 61 | loss: 2.56787 | train_balanced_accuracy: 0.18212 | valid_balanced_accuracy: 0.18104 |  0:49:16s\n",
      "epoch 62 | loss: 2.56117 | train_balanced_accuracy: 0.18681 | valid_balanced_accuracy: 0.18883 |  0:50:04s\n",
      "epoch 63 | loss: 2.55118 | train_balanced_accuracy: 0.18727 | valid_balanced_accuracy: 0.18617 |  0:50:52s\n",
      "epoch 64 | loss: 2.54009 | train_balanced_accuracy: 0.18949 | valid_balanced_accuracy: 0.19221 |  0:51:39s\n",
      "epoch 65 | loss: 2.52967 | train_balanced_accuracy: 0.19213 | valid_balanced_accuracy: 0.19269 |  0:52:27s\n",
      "epoch 66 | loss: 2.52256 | train_balanced_accuracy: 0.19487 | valid_balanced_accuracy: 0.19444 |  0:53:15s\n",
      "epoch 67 | loss: 2.5098  | train_balanced_accuracy: 0.19416 | valid_balanced_accuracy: 0.19792 |  0:54:03s\n",
      "epoch 68 | loss: 2.50029 | train_balanced_accuracy: 0.20026 | valid_balanced_accuracy: 0.20241 |  0:54:50s\n",
      "epoch 69 | loss: 2.48914 | train_balanced_accuracy: 0.1892  | valid_balanced_accuracy: 0.18915 |  0:55:39s\n",
      "epoch 70 | loss: 2.48545 | train_balanced_accuracy: 0.19291 | valid_balanced_accuracy: 0.1915  |  0:56:28s\n",
      "epoch 71 | loss: 2.47728 | train_balanced_accuracy: 0.20044 | valid_balanced_accuracy: 0.20063 |  0:57:17s\n",
      "epoch 72 | loss: 2.47316 | train_balanced_accuracy: 0.19332 | valid_balanced_accuracy: 0.19197 |  0:58:06s\n",
      "epoch 73 | loss: 2.45834 | train_balanced_accuracy: 0.20228 | valid_balanced_accuracy: 0.20364 |  0:58:54s\n",
      "epoch 74 | loss: 2.45464 | train_balanced_accuracy: 0.19971 | valid_balanced_accuracy: 0.19903 |  0:59:41s\n",
      "epoch 75 | loss: 2.44883 | train_balanced_accuracy: 0.20278 | valid_balanced_accuracy: 0.20402 |  1:00:29s\n",
      "epoch 76 | loss: 2.44084 | train_balanced_accuracy: 0.20757 | valid_balanced_accuracy: 0.20957 |  1:01:17s\n",
      "epoch 77 | loss: 2.43171 | train_balanced_accuracy: 0.20379 | valid_balanced_accuracy: 0.20528 |  1:02:05s\n",
      "epoch 78 | loss: 2.4269  | train_balanced_accuracy: 0.21021 | valid_balanced_accuracy: 0.20967 |  1:02:52s\n",
      "epoch 79 | loss: 2.42068 | train_balanced_accuracy: 0.20976 | valid_balanced_accuracy: 0.2092  |  1:03:40s\n",
      "epoch 80 | loss: 2.41723 | train_balanced_accuracy: 0.2041  | valid_balanced_accuracy: 0.20653 |  1:04:28s\n",
      "epoch 81 | loss: 2.40912 | train_balanced_accuracy: 0.21535 | valid_balanced_accuracy: 0.21427 |  1:05:15s\n",
      "epoch 82 | loss: 2.40053 | train_balanced_accuracy: 0.20912 | valid_balanced_accuracy: 0.21115 |  1:06:03s\n",
      "epoch 83 | loss: 2.39683 | train_balanced_accuracy: 0.21343 | valid_balanced_accuracy: 0.21151 |  1:06:51s\n",
      "epoch 84 | loss: 2.39139 | train_balanced_accuracy: 0.21173 | valid_balanced_accuracy: 0.21442 |  1:07:40s\n",
      "epoch 85 | loss: 2.38485 | train_balanced_accuracy: 0.20779 | valid_balanced_accuracy: 0.21069 |  1:08:30s\n",
      "epoch 86 | loss: 2.38314 | train_balanced_accuracy: 0.21446 | valid_balanced_accuracy: 0.21081 |  1:09:20s\n",
      "epoch 87 | loss: 2.37326 | train_balanced_accuracy: 0.21444 | valid_balanced_accuracy: 0.21274 |  1:10:10s\n",
      "epoch 88 | loss: 2.37243 | train_balanced_accuracy: 0.21318 | valid_balanced_accuracy: 0.21373 |  1:11:00s\n",
      "epoch 89 | loss: 2.36298 | train_balanced_accuracy: 0.22064 | valid_balanced_accuracy: 0.2201  |  1:11:49s\n",
      "epoch 90 | loss: 2.35616 | train_balanced_accuracy: 0.21854 | valid_balanced_accuracy: 0.21703 |  1:12:38s\n",
      "epoch 91 | loss: 2.35075 | train_balanced_accuracy: 0.21745 | valid_balanced_accuracy: 0.21577 |  1:13:28s\n",
      "epoch 92 | loss: 2.34902 | train_balanced_accuracy: 0.21317 | valid_balanced_accuracy: 0.21349 |  1:14:18s\n",
      "epoch 93 | loss: 2.34515 | train_balanced_accuracy: 0.22083 | valid_balanced_accuracy: 0.21997 |  1:15:07s\n",
      "epoch 94 | loss: 2.34048 | train_balanced_accuracy: 0.22511 | valid_balanced_accuracy: 0.22686 |  1:15:56s\n",
      "epoch 95 | loss: 2.34049 | train_balanced_accuracy: 0.2255  | valid_balanced_accuracy: 0.22419 |  1:16:45s\n",
      "epoch 96 | loss: 2.33384 | train_balanced_accuracy: 0.22782 | valid_balanced_accuracy: 0.22725 |  1:17:35s\n",
      "epoch 97 | loss: 2.32564 | train_balanced_accuracy: 0.22771 | valid_balanced_accuracy: 0.22596 |  1:18:23s\n",
      "epoch 98 | loss: 2.327   | train_balanced_accuracy: 0.22309 | valid_balanced_accuracy: 0.22305 |  1:19:12s\n",
      "epoch 99 | loss: 2.32021 | train_balanced_accuracy: 0.22521 | valid_balanced_accuracy: 0.22646 |  1:20:01s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 96 and best_valid_balanced_accuracy = 0.22725\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'step_size': 100}, optimizer_params={'lr': 1e-05}, n_steps=11, n_shared=1, n_independent=2, n_a=64, momentum=0.05, lambda_sparse=1, clip_value=0.5, total=80.3min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'step_size': 100}, optimizer_params={'lr': 1e-05}, n_steps=11, n_shared=1, n_independent=2, n_a=64, momentum=0.05, lambda_sparse=1, clip_value=0.5 \n",
      "epoch 0  | loss: 14.32041| train_balanced_accuracy: 0.07834 | valid_balanced_accuracy: 0.04725 |  0:00:49s\n",
      "epoch 1  | loss: 13.47881| train_balanced_accuracy: 0.06819 | valid_balanced_accuracy: 0.04116 |  0:01:38s\n",
      "epoch 2  | loss: 12.6853 | train_balanced_accuracy: 0.07375 | valid_balanced_accuracy: 0.05513 |  0:02:28s\n",
      "epoch 3  | loss: 11.90481| train_balanced_accuracy: 0.07279 | valid_balanced_accuracy: 0.05229 |  0:03:17s\n",
      "epoch 4  | loss: 11.15533| train_balanced_accuracy: 0.08482 | valid_balanced_accuracy: 0.06379 |  0:04:07s\n",
      "epoch 5  | loss: 10.50754| train_balanced_accuracy: 0.0882  | valid_balanced_accuracy: 0.06033 |  0:04:56s\n",
      "epoch 6  | loss: 9.89675 | train_balanced_accuracy: 0.09098 | valid_balanced_accuracy: 0.06542 |  0:05:44s\n",
      "epoch 7  | loss: 9.27343 | train_balanced_accuracy: 0.08684 | valid_balanced_accuracy: 0.05895 |  0:06:32s\n",
      "epoch 8  | loss: 8.6927  | train_balanced_accuracy: 0.09874 | valid_balanced_accuracy: 0.07514 |  0:07:20s\n",
      "epoch 9  | loss: 8.12396 | train_balanced_accuracy: 0.09629 | valid_balanced_accuracy: 0.074   |  0:08:08s\n",
      "epoch 10 | loss: 7.55816 | train_balanced_accuracy: 0.09506 | valid_balanced_accuracy: 0.07574 |  0:08:57s\n",
      "epoch 11 | loss: 7.03845 | train_balanced_accuracy: 0.10758 | valid_balanced_accuracy: 0.07934 |  0:09:45s\n",
      "epoch 12 | loss: 6.56141 | train_balanced_accuracy: 0.09591 | valid_balanced_accuracy: 0.08174 |  0:10:34s\n",
      "epoch 13 | loss: 6.09366 | train_balanced_accuracy: 0.102   | valid_balanced_accuracy: 0.08353 |  0:11:22s\n",
      "epoch 14 | loss: 5.72878 | train_balanced_accuracy: 0.09078 | valid_balanced_accuracy: 0.08728 |  0:12:12s\n",
      "epoch 15 | loss: 5.36902 | train_balanced_accuracy: 0.10265 | valid_balanced_accuracy: 0.09309 |  0:13:01s\n",
      "epoch 16 | loss: 5.06681 | train_balanced_accuracy: 0.09896 | valid_balanced_accuracy: 0.09363 |  0:13:50s\n",
      "epoch 17 | loss: 4.83995 | train_balanced_accuracy: 0.11445 | valid_balanced_accuracy: 0.09639 |  0:14:40s\n",
      "epoch 18 | loss: 4.62151 | train_balanced_accuracy: 0.10056 | valid_balanced_accuracy: 0.09871 |  0:15:29s\n",
      "epoch 19 | loss: 4.43058 | train_balanced_accuracy: 0.10419 | valid_balanced_accuracy: 0.10065 |  0:16:18s\n",
      "epoch 20 | loss: 4.24985 | train_balanced_accuracy: 0.11458 | valid_balanced_accuracy: 0.09791 |  0:17:06s\n",
      "epoch 21 | loss: 4.08124 | train_balanced_accuracy: 0.12314 | valid_balanced_accuracy: 0.10314 |  0:17:54s\n",
      "epoch 22 | loss: 3.94565 | train_balanced_accuracy: 0.10749 | valid_balanced_accuracy: 0.10447 |  0:18:42s\n",
      "epoch 23 | loss: 3.81754 | train_balanced_accuracy: 0.12749 | valid_balanced_accuracy: 0.11437 |  0:19:30s\n",
      "epoch 24 | loss: 3.71657 | train_balanced_accuracy: 0.13603 | valid_balanced_accuracy: 0.11715 |  0:20:18s\n",
      "epoch 25 | loss: 3.61046 | train_balanced_accuracy: 0.12959 | valid_balanced_accuracy: 0.12031 |  0:21:06s\n",
      "epoch 26 | loss: 3.51814 | train_balanced_accuracy: 0.13065 | valid_balanced_accuracy: 0.12663 |  0:21:54s\n",
      "epoch 27 | loss: 3.45245 | train_balanced_accuracy: 0.13562 | valid_balanced_accuracy: 0.12818 |  0:22:42s\n",
      "epoch 28 | loss: 3.39385 | train_balanced_accuracy: 0.13436 | valid_balanced_accuracy: 0.12962 |  0:23:30s\n",
      "epoch 29 | loss: 3.32413 | train_balanced_accuracy: 0.13879 | valid_balanced_accuracy: 0.13529 |  0:24:18s\n",
      "epoch 30 | loss: 3.27721 | train_balanced_accuracy: 0.14761 | valid_balanced_accuracy: 0.1381  |  0:25:07s\n",
      "epoch 31 | loss: 3.2164  | train_balanced_accuracy: 0.14209 | valid_balanced_accuracy: 0.14027 |  0:25:55s\n",
      "epoch 32 | loss: 3.18025 | train_balanced_accuracy: 0.15448 | valid_balanced_accuracy: 0.14459 |  0:26:44s\n",
      "epoch 33 | loss: 3.13476 | train_balanced_accuracy: 0.16098 | valid_balanced_accuracy: 0.14863 |  0:27:33s\n",
      "epoch 34 | loss: 3.08513 | train_balanced_accuracy: 0.15482 | valid_balanced_accuracy: 0.14949 |  0:28:21s\n",
      "epoch 35 | loss: 3.04854 | train_balanced_accuracy: 0.15688 | valid_balanced_accuracy: 0.14831 |  0:29:09s\n",
      "epoch 36 | loss: 3.0114  | train_balanced_accuracy: 0.15566 | valid_balanced_accuracy: 0.15495 |  0:29:59s\n",
      "epoch 37 | loss: 2.97285 | train_balanced_accuracy: 0.16564 | valid_balanced_accuracy: 0.1605  |  0:30:49s\n",
      "epoch 38 | loss: 2.94535 | train_balanced_accuracy: 0.15803 | valid_balanced_accuracy: 0.1606  |  0:31:38s\n",
      "epoch 39 | loss: 2.90723 | train_balanced_accuracy: 0.15793 | valid_balanced_accuracy: 0.15684 |  0:32:28s\n",
      "epoch 40 | loss: 2.8817  | train_balanced_accuracy: 0.16655 | valid_balanced_accuracy: 0.1665  |  0:33:17s\n",
      "epoch 41 | loss: 2.85412 | train_balanced_accuracy: 0.16861 | valid_balanced_accuracy: 0.16791 |  0:34:06s\n",
      "epoch 42 | loss: 2.83193 | train_balanced_accuracy: 0.17116 | valid_balanced_accuracy: 0.16981 |  0:34:56s\n",
      "epoch 43 | loss: 2.80552 | train_balanced_accuracy: 0.16859 | valid_balanced_accuracy: 0.16751 |  0:35:45s\n",
      "epoch 44 | loss: 2.78803 | train_balanced_accuracy: 0.17323 | valid_balanced_accuracy: 0.17228 |  0:36:35s\n",
      "epoch 45 | loss: 2.75831 | train_balanced_accuracy: 0.1713  | valid_balanced_accuracy: 0.16939 |  0:37:24s\n",
      "epoch 46 | loss: 2.74129 | train_balanced_accuracy: 0.17489 | valid_balanced_accuracy: 0.17486 |  0:38:14s\n",
      "epoch 47 | loss: 2.72182 | train_balanced_accuracy: 0.17953 | valid_balanced_accuracy: 0.17739 |  0:39:04s\n",
      "epoch 48 | loss: 2.7009  | train_balanced_accuracy: 0.17821 | valid_balanced_accuracy: 0.17709 |  0:39:52s\n",
      "epoch 49 | loss: 2.68699 | train_balanced_accuracy: 0.18015 | valid_balanced_accuracy: 0.18127 |  0:40:41s\n",
      "epoch 50 | loss: 2.66765 | train_balanced_accuracy: 0.18453 | valid_balanced_accuracy: 0.18641 |  0:41:29s\n",
      "epoch 51 | loss: 2.65134 | train_balanced_accuracy: 0.18035 | valid_balanced_accuracy: 0.17965 |  0:42:18s\n",
      "epoch 52 | loss: 2.63831 | train_balanced_accuracy: 0.18265 | valid_balanced_accuracy: 0.18434 |  0:43:06s\n",
      "epoch 53 | loss: 2.62038 | train_balanced_accuracy: 0.18354 | valid_balanced_accuracy: 0.18325 |  0:43:54s\n",
      "epoch 54 | loss: 2.60388 | train_balanced_accuracy: 0.18228 | valid_balanced_accuracy: 0.18236 |  0:44:43s\n",
      "epoch 55 | loss: 2.59997 | train_balanced_accuracy: 0.18724 | valid_balanced_accuracy: 0.1875  |  0:45:31s\n",
      "epoch 56 | loss: 2.58883 | train_balanced_accuracy: 0.18726 | valid_balanced_accuracy: 0.18819 |  0:46:19s\n",
      "epoch 57 | loss: 2.5719  | train_balanced_accuracy: 0.19163 | valid_balanced_accuracy: 0.19029 |  0:47:07s\n",
      "epoch 58 | loss: 2.57321 | train_balanced_accuracy: 0.192   | valid_balanced_accuracy: 0.19229 |  0:47:56s\n",
      "epoch 59 | loss: 2.55068 | train_balanced_accuracy: 0.1912  | valid_balanced_accuracy: 0.19243 |  0:48:44s\n",
      "epoch 60 | loss: 2.54448 | train_balanced_accuracy: 0.19064 | valid_balanced_accuracy: 0.19018 |  0:49:32s\n",
      "epoch 61 | loss: 2.52859 | train_balanced_accuracy: 0.19514 | valid_balanced_accuracy: 0.19418 |  0:50:20s\n",
      "epoch 62 | loss: 2.51786 | train_balanced_accuracy: 0.19559 | valid_balanced_accuracy: 0.19395 |  0:51:08s\n",
      "epoch 63 | loss: 2.51306 | train_balanced_accuracy: 0.19764 | valid_balanced_accuracy: 0.19618 |  0:51:56s\n",
      "epoch 64 | loss: 2.49959 | train_balanced_accuracy: 0.20242 | valid_balanced_accuracy: 0.20232 |  0:52:44s\n",
      "epoch 65 | loss: 2.48867 | train_balanced_accuracy: 0.19914 | valid_balanced_accuracy: 0.19593 |  0:53:32s\n",
      "epoch 66 | loss: 2.4794  | train_balanced_accuracy: 0.20056 | valid_balanced_accuracy: 0.20166 |  0:54:19s\n",
      "epoch 67 | loss: 2.47076 | train_balanced_accuracy: 0.20058 | valid_balanced_accuracy: 0.20181 |  0:55:07s\n",
      "epoch 68 | loss: 2.45736 | train_balanced_accuracy: 0.20217 | valid_balanced_accuracy: 0.20274 |  0:55:56s\n",
      "epoch 69 | loss: 2.45266 | train_balanced_accuracy: 0.20105 | valid_balanced_accuracy: 0.1991  |  0:56:46s\n",
      "epoch 70 | loss: 2.4443  | train_balanced_accuracy: 0.20034 | valid_balanced_accuracy: 0.19872 |  0:57:35s\n",
      "epoch 71 | loss: 2.43702 | train_balanced_accuracy: 0.20844 | valid_balanced_accuracy: 0.20867 |  0:58:25s\n",
      "epoch 72 | loss: 2.43234 | train_balanced_accuracy: 0.20477 | valid_balanced_accuracy: 0.20504 |  0:59:14s\n",
      "epoch 73 | loss: 2.42596 | train_balanced_accuracy: 0.21173 | valid_balanced_accuracy: 0.21173 |  1:00:04s\n",
      "epoch 74 | loss: 2.41427 | train_balanced_accuracy: 0.20761 | valid_balanced_accuracy: 0.20574 |  1:00:53s\n",
      "epoch 75 | loss: 2.41114 | train_balanced_accuracy: 0.20848 | valid_balanced_accuracy: 0.20718 |  1:01:43s\n",
      "epoch 76 | loss: 2.40227 | train_balanced_accuracy: 0.20495 | valid_balanced_accuracy: 0.2027  |  1:02:32s\n",
      "epoch 77 | loss: 2.39824 | train_balanced_accuracy: 0.21472 | valid_balanced_accuracy: 0.21322 |  1:03:22s\n",
      "epoch 78 | loss: 2.39253 | train_balanced_accuracy: 0.21209 | valid_balanced_accuracy: 0.20996 |  1:04:11s\n",
      "epoch 79 | loss: 2.38686 | train_balanced_accuracy: 0.21377 | valid_balanced_accuracy: 0.20934 |  1:05:01s\n",
      "epoch 80 | loss: 2.38391 | train_balanced_accuracy: 0.21298 | valid_balanced_accuracy: 0.21411 |  1:05:50s\n",
      "epoch 81 | loss: 2.37597 | train_balanced_accuracy: 0.21422 | valid_balanced_accuracy: 0.21253 |  1:06:40s\n",
      "epoch 82 | loss: 2.3727  | train_balanced_accuracy: 0.20964 | valid_balanced_accuracy: 0.20824 |  1:07:29s\n",
      "epoch 83 | loss: 2.36671 | train_balanced_accuracy: 0.2128  | valid_balanced_accuracy: 0.21312 |  1:08:19s\n",
      "epoch 84 | loss: 2.36279 | train_balanced_accuracy: 0.21528 | valid_balanced_accuracy: 0.21325 |  1:09:08s\n",
      "epoch 85 | loss: 2.36048 | train_balanced_accuracy: 0.21432 | valid_balanced_accuracy: 0.21298 |  1:09:58s\n",
      "epoch 86 | loss: 2.35341 | train_balanced_accuracy: 0.21677 | valid_balanced_accuracy: 0.21605 |  1:10:48s\n",
      "epoch 87 | loss: 2.34888 | train_balanced_accuracy: 0.21675 | valid_balanced_accuracy: 0.21447 |  1:11:38s\n",
      "epoch 88 | loss: 2.34478 | train_balanced_accuracy: 0.21388 | valid_balanced_accuracy: 0.21146 |  1:12:28s\n",
      "epoch 89 | loss: 2.33871 | train_balanced_accuracy: 0.21264 | valid_balanced_accuracy: 0.21148 |  1:13:18s\n",
      "epoch 90 | loss: 2.33958 | train_balanced_accuracy: 0.22028 | valid_balanced_accuracy: 0.21807 |  1:14:07s\n",
      "epoch 91 | loss: 2.33401 | train_balanced_accuracy: 0.2164  | valid_balanced_accuracy: 0.21573 |  1:14:57s\n",
      "epoch 92 | loss: 2.33072 | train_balanced_accuracy: 0.21246 | valid_balanced_accuracy: 0.21477 |  1:15:47s\n",
      "epoch 93 | loss: 2.3208  | train_balanced_accuracy: 0.22019 | valid_balanced_accuracy: 0.21987 |  1:16:36s\n",
      "epoch 94 | loss: 2.31925 | train_balanced_accuracy: 0.21198 | valid_balanced_accuracy: 0.2149  |  1:17:25s\n",
      "epoch 95 | loss: 2.31481 | train_balanced_accuracy: 0.22038 | valid_balanced_accuracy: 0.22093 |  1:18:13s\n",
      "epoch 96 | loss: 2.31015 | train_balanced_accuracy: 0.21872 | valid_balanced_accuracy: 0.2194  |  1:19:02s\n",
      "epoch 97 | loss: 2.30359 | train_balanced_accuracy: 0.22199 | valid_balanced_accuracy: 0.21989 |  1:19:51s\n",
      "epoch 98 | loss: 2.29737 | train_balanced_accuracy: 0.22079 | valid_balanced_accuracy: 0.21969 |  1:20:40s\n",
      "epoch 99 | loss: 2.29778 | train_balanced_accuracy: 0.22345 | valid_balanced_accuracy: 0.22165 |  1:21:29s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_valid_balanced_accuracy = 0.22165\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'step_size': 100}, optimizer_params={'lr': 1e-05}, n_steps=11, n_shared=1, n_independent=2, n_a=64, momentum=0.05, lambda_sparse=1, clip_value=0.5, total=81.7min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'gamma': 2.2}, optimizer_params={'lr': 0.0001}, n_steps=11, n_shared=4, n_independent=9, n_a=4, momentum=1, lambda_sparse=0.1, clip_value=10 \n",
      "epoch 0  | loss: 5.63844 | train_balanced_accuracy: 0.03414 | valid_balanced_accuracy: 0.03442 |  0:02:26s\n",
      "epoch 1  | loss: 5.32328 | train_balanced_accuracy: 0.03365 | valid_balanced_accuracy: 0.03209 |  0:04:51s\n",
      "epoch 2  | loss: 5.06671 | train_balanced_accuracy: 0.05958 | valid_balanced_accuracy: 0.0591  |  0:07:16s\n",
      "epoch 3  | loss: 4.9197  | train_balanced_accuracy: 0.05381 | valid_balanced_accuracy: 0.05917 |  0:09:43s\n",
      "epoch 4  | loss: 4.731   | train_balanced_accuracy: 0.03601 | valid_balanced_accuracy: 0.03247 |  0:12:08s\n",
      "epoch 5  | loss: 4.57345 | train_balanced_accuracy: 0.06242 | valid_balanced_accuracy: 0.05962 |  0:14:31s\n",
      "epoch 6  | loss: 4.43509 | train_balanced_accuracy: 0.03584 | valid_balanced_accuracy: 0.03687 |  0:16:54s\n",
      "epoch 7  | loss: 4.28019 | train_balanced_accuracy: 0.04546 | valid_balanced_accuracy: 0.0398  |  0:19:16s\n",
      "epoch 8  | loss: 4.14292 | train_balanced_accuracy: 0.04499 | valid_balanced_accuracy: 0.04315 |  0:21:38s\n",
      "epoch 9  | loss: 4.01531 | train_balanced_accuracy: 0.06091 | valid_balanced_accuracy: 0.06041 |  0:23:59s\n",
      "epoch 10 | loss: 3.89176 | train_balanced_accuracy: 0.06088 | valid_balanced_accuracy: 0.06174 |  0:26:21s\n",
      "epoch 11 | loss: 3.79969 | train_balanced_accuracy: 0.05427 | valid_balanced_accuracy: 0.05617 |  0:28:43s\n",
      "epoch 12 | loss: 3.72251 | train_balanced_accuracy: 0.05153 | valid_balanced_accuracy: 0.05226 |  0:31:09s\n",
      "epoch 13 | loss: 3.64804 | train_balanced_accuracy: 0.04555 | valid_balanced_accuracy: 0.04775 |  0:33:35s\n",
      "epoch 14 | loss: 3.57525 | train_balanced_accuracy: 0.04504 | valid_balanced_accuracy: 0.04976 |  0:36:02s\n",
      "epoch 15 | loss: 3.51035 | train_balanced_accuracy: 0.05073 | valid_balanced_accuracy: 0.05199 |  0:38:29s\n",
      "epoch 16 | loss: 3.44717 | train_balanced_accuracy: 0.05101 | valid_balanced_accuracy: 0.05523 |  0:40:54s\n",
      "epoch 17 | loss: 3.38884 | train_balanced_accuracy: 0.03694 | valid_balanced_accuracy: 0.03356 |  0:43:17s\n",
      "epoch 18 | loss: 3.34149 | train_balanced_accuracy: 0.06062 | valid_balanced_accuracy: 0.05789 |  0:45:42s\n",
      "epoch 19 | loss: 3.30268 | train_balanced_accuracy: 0.05752 | valid_balanced_accuracy: 0.05818 |  0:48:05s\n",
      "epoch 20 | loss: 3.25745 | train_balanced_accuracy: 0.05093 | valid_balanced_accuracy: 0.04903 |  0:50:28s\n",
      "\n",
      "Early stopping occured at epoch 20 with best_epoch = 10 and best_valid_balanced_accuracy = 0.06174\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'gamma': 2.2}, optimizer_params={'lr': 0.0001}, n_steps=11, n_shared=4, n_independent=9, n_a=4, momentum=1, lambda_sparse=0.1, clip_value=10, total=51.2min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'gamma': 2.2}, optimizer_params={'lr': 0.0001}, n_steps=11, n_shared=4, n_independent=9, n_a=4, momentum=1, lambda_sparse=0.1, clip_value=10 \n",
      "epoch 0  | loss: 5.63524 | train_balanced_accuracy: 0.04423 | valid_balanced_accuracy: 0.04062 |  0:02:25s\n",
      "epoch 1  | loss: 5.33183 | train_balanced_accuracy: 0.05617 | valid_balanced_accuracy: 0.05769 |  0:04:50s\n",
      "epoch 2  | loss: 5.10086 | train_balanced_accuracy: 0.05437 | valid_balanced_accuracy: 0.05038 |  0:07:17s\n",
      "epoch 3  | loss: 4.87249 | train_balanced_accuracy: 0.0656  | valid_balanced_accuracy: 0.06332 |  0:09:40s\n",
      "epoch 4  | loss: 4.65638 | train_balanced_accuracy: 0.04835 | valid_balanced_accuracy: 0.05054 |  0:12:04s\n",
      "epoch 5  | loss: 4.48049 | train_balanced_accuracy: 0.05633 | valid_balanced_accuracy: 0.05837 |  0:14:27s\n",
      "epoch 6  | loss: 4.31446 | train_balanced_accuracy: 0.05399 | valid_balanced_accuracy: 0.04975 |  0:16:50s\n",
      "epoch 7  | loss: 4.15913 | train_balanced_accuracy: 0.0572  | valid_balanced_accuracy: 0.05436 |  0:19:13s\n",
      "epoch 8  | loss: 4.00234 | train_balanced_accuracy: 0.05294 | valid_balanced_accuracy: 0.05568 |  0:21:36s\n",
      "epoch 9  | loss: 3.86532 | train_balanced_accuracy: 0.06204 | valid_balanced_accuracy: 0.06651 |  0:23:59s\n",
      "epoch 10 | loss: 3.76402 | train_balanced_accuracy: 0.06502 | valid_balanced_accuracy: 0.06304 |  0:26:21s\n",
      "epoch 11 | loss: 3.67602 | train_balanced_accuracy: 0.06122 | valid_balanced_accuracy: 0.06033 |  0:28:45s\n",
      "epoch 12 | loss: 3.60244 | train_balanced_accuracy: 0.05276 | valid_balanced_accuracy: 0.04757 |  0:31:11s\n",
      "epoch 13 | loss: 3.53158 | train_balanced_accuracy: 0.04622 | valid_balanced_accuracy: 0.04511 |  0:33:35s\n",
      "epoch 14 | loss: 3.47499 | train_balanced_accuracy: 0.02354 | valid_balanced_accuracy: 0.03296 |  0:35:57s\n",
      "epoch 15 | loss: 3.43179 | train_balanced_accuracy: 0.05346 | valid_balanced_accuracy: 0.05514 |  0:38:19s\n",
      "epoch 16 | loss: 3.38396 | train_balanced_accuracy: 0.06301 | valid_balanced_accuracy: 0.05814 |  0:40:42s\n",
      "epoch 17 | loss: 3.33554 | train_balanced_accuracy: 0.06123 | valid_balanced_accuracy: 0.06129 |  0:43:04s\n",
      "epoch 18 | loss: 3.27966 | train_balanced_accuracy: 0.06352 | valid_balanced_accuracy: 0.06005 |  0:45:28s\n",
      "epoch 19 | loss: 3.239   | train_balanced_accuracy: 0.07186 | valid_balanced_accuracy: 0.06666 |  0:47:51s\n",
      "epoch 20 | loss: 3.2024  | train_balanced_accuracy: 0.06053 | valid_balanced_accuracy: 0.06048 |  0:50:13s\n",
      "epoch 21 | loss: 3.17118 | train_balanced_accuracy: 0.07315 | valid_balanced_accuracy: 0.07131 |  0:52:35s\n",
      "epoch 22 | loss: 3.12464 | train_balanced_accuracy: 0.07869 | valid_balanced_accuracy: 0.08268 |  0:54:59s\n",
      "epoch 23 | loss: 3.09022 | train_balanced_accuracy: 0.05809 | valid_balanced_accuracy: 0.05796 |  0:57:22s\n",
      "epoch 24 | loss: 3.05557 | train_balanced_accuracy: 0.06156 | valid_balanced_accuracy: 0.05836 |  0:59:44s\n",
      "epoch 25 | loss: 3.02108 | train_balanced_accuracy: 0.05515 | valid_balanced_accuracy: 0.05582 |  1:02:08s\n",
      "epoch 26 | loss: 2.99789 | train_balanced_accuracy: 0.07145 | valid_balanced_accuracy: 0.06982 |  1:04:31s\n",
      "epoch 27 | loss: 2.9694  | train_balanced_accuracy: 0.05488 | valid_balanced_accuracy: 0.05393 |  1:06:55s\n",
      "epoch 28 | loss: 2.93519 | train_balanced_accuracy: 0.05996 | valid_balanced_accuracy: 0.05825 |  1:09:19s\n",
      "epoch 29 | loss: 2.90206 | train_balanced_accuracy: 0.0609  | valid_balanced_accuracy: 0.06239 |  1:11:43s\n",
      "epoch 30 | loss: 2.87859 | train_balanced_accuracy: 0.06103 | valid_balanced_accuracy: 0.06313 |  1:14:08s\n",
      "epoch 31 | loss: 2.8548  | train_balanced_accuracy: 0.06337 | valid_balanced_accuracy: 0.05963 |  1:16:33s\n",
      "epoch 32 | loss: 2.83176 | train_balanced_accuracy: 0.05097 | valid_balanced_accuracy: 0.04998 |  1:18:58s\n",
      "\n",
      "Early stopping occured at epoch 32 with best_epoch = 22 and best_valid_balanced_accuracy = 0.08268\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'gamma': 2.2}, optimizer_params={'lr': 0.0001}, n_steps=11, n_shared=4, n_independent=9, n_a=4, momentum=1, lambda_sparse=0.1, clip_value=10, total=79.7min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'gamma': 2.2}, optimizer_params={'lr': 0.0001}, n_steps=11, n_shared=4, n_independent=9, n_a=4, momentum=1, lambda_sparse=0.1, clip_value=10 \n",
      "epoch 0  | loss: 5.66286 | train_balanced_accuracy: 0.05334 | valid_balanced_accuracy: 0.05705 |  0:02:25s\n",
      "epoch 1  | loss: 5.3785  | train_balanced_accuracy: 0.05436 | valid_balanced_accuracy: 0.05675 |  0:04:48s\n",
      "epoch 2  | loss: 5.1068  | train_balanced_accuracy: 0.05608 | valid_balanced_accuracy: 0.05038 |  0:07:11s\n",
      "epoch 3  | loss: 4.92795 | train_balanced_accuracy: 0.04892 | valid_balanced_accuracy: 0.05006 |  0:09:34s\n",
      "epoch 4  | loss: 4.79435 | train_balanced_accuracy: 0.05517 | valid_balanced_accuracy: 0.05703 |  0:11:56s\n",
      "epoch 5  | loss: 4.65401 | train_balanced_accuracy: 0.06128 | valid_balanced_accuracy: 0.06407 |  0:14:22s\n",
      "epoch 6  | loss: 4.49869 | train_balanced_accuracy: 0.05978 | valid_balanced_accuracy: 0.05993 |  0:16:50s\n",
      "epoch 7  | loss: 4.34702 | train_balanced_accuracy: 0.05706 | valid_balanced_accuracy: 0.05053 |  0:19:17s\n",
      "epoch 8  | loss: 4.20116 | train_balanced_accuracy: 0.04099 | valid_balanced_accuracy: 0.0353  |  0:21:44s\n",
      "epoch 9  | loss: 4.09931 | train_balanced_accuracy: 0.04846 | valid_balanced_accuracy: 0.04643 |  0:24:12s\n",
      "epoch 10 | loss: 3.99147 | train_balanced_accuracy: 0.04845 | valid_balanced_accuracy: 0.04741 |  0:26:40s\n",
      "epoch 11 | loss: 3.87492 | train_balanced_accuracy: 0.04951 | valid_balanced_accuracy: 0.04774 |  0:29:09s\n",
      "epoch 12 | loss: 3.78154 | train_balanced_accuracy: 0.05158 | valid_balanced_accuracy: 0.04929 |  0:31:36s\n",
      "epoch 13 | loss: 3.69903 | train_balanced_accuracy: 0.05495 | valid_balanced_accuracy: 0.05334 |  0:34:02s\n",
      "epoch 14 | loss: 3.62261 | train_balanced_accuracy: 0.04816 | valid_balanced_accuracy: 0.04998 |  0:36:26s\n",
      "epoch 15 | loss: 3.55993 | train_balanced_accuracy: 0.06114 | valid_balanced_accuracy: 0.06128 |  0:38:52s\n",
      "\n",
      "Early stopping occured at epoch 15 with best_epoch = 5 and best_valid_balanced_accuracy = 0.06407\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'gamma': 2.2}, optimizer_params={'lr': 0.0001}, n_steps=11, n_shared=4, n_independent=9, n_a=4, momentum=1, lambda_sparse=0.1, clip_value=10, total=39.6min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'gamma': 2.2}, optimizer_params={'lr': 0.0001}, n_steps=11, n_shared=4, n_independent=9, n_a=4, momentum=1, lambda_sparse=0.1, clip_value=10 \n",
      "epoch 0  | loss: 5.67347 | train_balanced_accuracy: 0.04513 | valid_balanced_accuracy: 0.04237 |  0:02:23s\n",
      "epoch 1  | loss: 5.37988 | train_balanced_accuracy: 0.05078 | valid_balanced_accuracy: 0.05058 |  0:04:46s\n",
      "epoch 2  | loss: 5.10932 | train_balanced_accuracy: 0.05456 | valid_balanced_accuracy: 0.05052 |  0:07:12s\n",
      "epoch 3  | loss: 4.88627 | train_balanced_accuracy: 0.05971 | valid_balanced_accuracy: 0.05818 |  0:09:39s\n",
      "epoch 4  | loss: 4.68778 | train_balanced_accuracy: 0.05125 | valid_balanced_accuracy: 0.0522  |  0:12:06s\n",
      "epoch 5  | loss: 4.51933 | train_balanced_accuracy: 0.04687 | valid_balanced_accuracy: 0.05196 |  0:14:33s\n",
      "epoch 6  | loss: 4.38476 | train_balanced_accuracy: 0.06155 | valid_balanced_accuracy: 0.06346 |  0:17:00s\n",
      "epoch 7  | loss: 4.25545 | train_balanced_accuracy: 0.05574 | valid_balanced_accuracy: 0.05654 |  0:19:26s\n",
      "epoch 8  | loss: 4.14338 | train_balanced_accuracy: 0.05251 | valid_balanced_accuracy: 0.04936 |  0:21:53s\n",
      "epoch 9  | loss: 4.03659 | train_balanced_accuracy: 0.0484  | valid_balanced_accuracy: 0.04578 |  0:24:21s\n",
      "epoch 10 | loss: 3.93849 | train_balanced_accuracy: 0.05144 | valid_balanced_accuracy: 0.04645 |  0:26:47s\n",
      "epoch 11 | loss: 3.84168 | train_balanced_accuracy: 0.0528  | valid_balanced_accuracy: 0.05123 |  0:29:10s\n",
      "epoch 12 | loss: 3.74295 | train_balanced_accuracy: 0.05231 | valid_balanced_accuracy: 0.05078 |  0:31:35s\n",
      "epoch 13 | loss: 3.66931 | train_balanced_accuracy: 0.05038 | valid_balanced_accuracy: 0.04728 |  0:33:59s\n",
      "epoch 14 | loss: 3.60008 | train_balanced_accuracy: 0.04497 | valid_balanced_accuracy: 0.04371 |  0:36:25s\n",
      "epoch 15 | loss: 3.52497 | train_balanced_accuracy: 0.04937 | valid_balanced_accuracy: 0.04667 |  0:38:50s\n",
      "epoch 16 | loss: 3.45878 | train_balanced_accuracy: 0.05702 | valid_balanced_accuracy: 0.05464 |  0:41:16s\n",
      "\n",
      "Early stopping occured at epoch 16 with best_epoch = 6 and best_valid_balanced_accuracy = 0.06346\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'gamma': 2.2}, optimizer_params={'lr': 0.0001}, n_steps=11, n_shared=4, n_independent=9, n_a=4, momentum=1, lambda_sparse=0.1, clip_value=10, total=42.0min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'gamma': 2.2}, optimizer_params={'lr': 0.0001}, n_steps=11, n_shared=4, n_independent=9, n_a=4, momentum=1, lambda_sparse=0.1, clip_value=10 \n",
      "epoch 0  | loss: 5.64625 | train_balanced_accuracy: 0.05841 | valid_balanced_accuracy: 0.05599 |  0:02:26s\n",
      "epoch 1  | loss: 5.40247 | train_balanced_accuracy: 0.04152 | valid_balanced_accuracy: 0.03854 |  0:04:53s\n",
      "epoch 2  | loss: 5.12484 | train_balanced_accuracy: 0.03046 | valid_balanced_accuracy: 0.03728 |  0:07:19s\n",
      "epoch 3  | loss: 4.85822 | train_balanced_accuracy: 0.04517 | valid_balanced_accuracy: 0.04575 |  0:09:46s\n",
      "epoch 4  | loss: 4.68036 | train_balanced_accuracy: 0.05175 | valid_balanced_accuracy: 0.05656 |  0:12:08s\n",
      "epoch 5  | loss: 4.50736 | train_balanced_accuracy: 0.0459  | valid_balanced_accuracy: 0.0487  |  0:14:31s\n",
      "epoch 6  | loss: 4.31646 | train_balanced_accuracy: 0.05456 | valid_balanced_accuracy: 0.0574  |  0:16:54s\n",
      "epoch 7  | loss: 4.16474 | train_balanced_accuracy: 0.06512 | valid_balanced_accuracy: 0.06419 |  0:19:16s\n",
      "epoch 8  | loss: 4.03989 | train_balanced_accuracy: 0.06814 | valid_balanced_accuracy: 0.06846 |  0:21:41s\n",
      "epoch 9  | loss: 3.91966 | train_balanced_accuracy: 0.04179 | valid_balanced_accuracy: 0.04363 |  0:24:03s\n",
      "epoch 10 | loss: 3.82499 | train_balanced_accuracy: 0.06283 | valid_balanced_accuracy: 0.06458 |  0:26:30s\n",
      "epoch 11 | loss: 3.73242 | train_balanced_accuracy: 0.06039 | valid_balanced_accuracy: 0.06033 |  0:28:57s\n",
      "epoch 12 | loss: 3.6537  | train_balanced_accuracy: 0.04392 | valid_balanced_accuracy: 0.0455  |  0:31:24s\n",
      "epoch 13 | loss: 3.59008 | train_balanced_accuracy: 0.05721 | valid_balanced_accuracy: 0.05746 |  0:33:52s\n",
      "epoch 14 | loss: 3.52585 | train_balanced_accuracy: 0.05098 | valid_balanced_accuracy: 0.04907 |  0:36:19s\n",
      "epoch 15 | loss: 3.47444 | train_balanced_accuracy: 0.05317 | valid_balanced_accuracy: 0.05235 |  0:38:47s\n",
      "epoch 16 | loss: 3.40503 | train_balanced_accuracy: 0.04841 | valid_balanced_accuracy: 0.05246 |  0:41:14s\n",
      "epoch 17 | loss: 3.35333 | train_balanced_accuracy: 0.06744 | valid_balanced_accuracy: 0.06654 |  0:43:41s\n",
      "epoch 18 | loss: 3.30754 | train_balanced_accuracy: 0.04902 | valid_balanced_accuracy: 0.05557 |  0:46:08s\n",
      "\n",
      "Early stopping occured at epoch 18 with best_epoch = 8 and best_valid_balanced_accuracy = 0.06846\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'gamma': 2.2}, optimizer_params={'lr': 0.0001}, n_steps=11, n_shared=4, n_independent=9, n_a=4, momentum=1, lambda_sparse=0.1, clip_value=10, total=46.9min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'step_size': 10}, optimizer_params={'lr': 0.01}, n_steps=8, n_shared=5, n_independent=4, n_a=32, momentum=0.02, lambda_sparse=0.001, clip_value=10 \n",
      "epoch 0  | loss: 2.05187 | train_balanced_accuracy: 0.2202  | valid_balanced_accuracy: 0.22095 |  0:01:21s\n",
      "epoch 1  | loss: 1.63149 | train_balanced_accuracy: 0.26558 | valid_balanced_accuracy: 0.26841 |  0:02:42s\n",
      "epoch 2  | loss: 1.56425 | train_balanced_accuracy: 0.25911 | valid_balanced_accuracy: 0.25878 |  0:04:02s\n",
      "epoch 3  | loss: 1.56777 | train_balanced_accuracy: 0.2685  | valid_balanced_accuracy: 0.27088 |  0:05:23s\n",
      "epoch 4  | loss: 1.51435 | train_balanced_accuracy: 0.31168 | valid_balanced_accuracy: 0.3119  |  0:06:43s\n",
      "epoch 5  | loss: 1.4854  | train_balanced_accuracy: 0.32713 | valid_balanced_accuracy: 0.33035 |  0:08:03s\n",
      "epoch 6  | loss: 1.45127 | train_balanced_accuracy: 0.39561 | valid_balanced_accuracy: 0.39782 |  0:09:24s\n",
      "epoch 7  | loss: 1.41209 | train_balanced_accuracy: 0.42293 | valid_balanced_accuracy: 0.42398 |  0:10:44s\n",
      "epoch 8  | loss: 1.38522 | train_balanced_accuracy: 0.43102 | valid_balanced_accuracy: 0.4239  |  0:12:05s\n",
      "epoch 9  | loss: 1.36289 | train_balanced_accuracy: 0.43343 | valid_balanced_accuracy: 0.43551 |  0:13:25s\n",
      "epoch 10 | loss: 1.34081 | train_balanced_accuracy: 0.43886 | valid_balanced_accuracy: 0.43418 |  0:14:46s\n",
      "epoch 11 | loss: 1.33969 | train_balanced_accuracy: 0.42288 | valid_balanced_accuracy: 0.41791 |  0:16:06s\n",
      "epoch 12 | loss: 1.32656 | train_balanced_accuracy: 0.45242 | valid_balanced_accuracy: 0.43829 |  0:17:27s\n",
      "epoch 13 | loss: 1.30712 | train_balanced_accuracy: 0.47468 | valid_balanced_accuracy: 0.46044 |  0:18:47s\n",
      "epoch 14 | loss: 1.30087 | train_balanced_accuracy: 0.46673 | valid_balanced_accuracy: 0.46942 |  0:20:08s\n",
      "epoch 15 | loss: 1.32887 | train_balanced_accuracy: 0.47316 | valid_balanced_accuracy: 0.45938 |  0:21:28s\n",
      "epoch 16 | loss: 1.28379 | train_balanced_accuracy: 0.49171 | valid_balanced_accuracy: 0.48016 |  0:22:49s\n",
      "epoch 17 | loss: 1.26534 | train_balanced_accuracy: 0.5005  | valid_balanced_accuracy: 0.49724 |  0:24:09s\n",
      "epoch 18 | loss: 1.2514  | train_balanced_accuracy: 0.49633 | valid_balanced_accuracy: 0.48219 |  0:25:30s\n",
      "epoch 19 | loss: 1.2382  | train_balanced_accuracy: 0.50892 | valid_balanced_accuracy: 0.50418 |  0:26:50s\n",
      "epoch 20 | loss: 1.21855 | train_balanced_accuracy: 0.49166 | valid_balanced_accuracy: 0.47564 |  0:28:11s\n",
      "epoch 21 | loss: 1.2087  | train_balanced_accuracy: 0.5152  | valid_balanced_accuracy: 0.50545 |  0:29:31s\n",
      "epoch 22 | loss: 1.20176 | train_balanced_accuracy: 0.52571 | valid_balanced_accuracy: 0.51778 |  0:30:52s\n",
      "epoch 23 | loss: 1.20329 | train_balanced_accuracy: 0.51046 | valid_balanced_accuracy: 0.50578 |  0:32:12s\n",
      "epoch 24 | loss: 1.19255 | train_balanced_accuracy: 0.52674 | valid_balanced_accuracy: 0.50474 |  0:33:32s\n",
      "epoch 25 | loss: 1.17992 | train_balanced_accuracy: 0.51393 | valid_balanced_accuracy: 0.48043 |  0:34:52s\n",
      "epoch 26 | loss: 1.18012 | train_balanced_accuracy: 0.49369 | valid_balanced_accuracy: 0.47939 |  0:36:12s\n",
      "epoch 27 | loss: 1.174   | train_balanced_accuracy: 0.52674 | valid_balanced_accuracy: 0.51319 |  0:37:33s\n",
      "epoch 28 | loss: 1.17224 | train_balanced_accuracy: 0.50957 | valid_balanced_accuracy: 0.49672 |  0:38:53s\n",
      "epoch 29 | loss: 1.20014 | train_balanced_accuracy: 0.53755 | valid_balanced_accuracy: 0.52957 |  0:40:13s\n",
      "epoch 30 | loss: 1.16962 | train_balanced_accuracy: 0.52397 | valid_balanced_accuracy: 0.51279 |  0:41:34s\n",
      "epoch 31 | loss: 1.18549 | train_balanced_accuracy: 0.50578 | valid_balanced_accuracy: 0.48394 |  0:42:54s\n",
      "epoch 32 | loss: 1.17995 | train_balanced_accuracy: 0.54304 | valid_balanced_accuracy: 0.53468 |  0:44:15s\n",
      "epoch 33 | loss: 1.19705 | train_balanced_accuracy: 0.51394 | valid_balanced_accuracy: 0.48496 |  0:45:35s\n",
      "epoch 34 | loss: 1.16148 | train_balanced_accuracy: 0.57106 | valid_balanced_accuracy: 0.54795 |  0:46:55s\n",
      "epoch 35 | loss: 1.14337 | train_balanced_accuracy: 0.55986 | valid_balanced_accuracy: 0.55397 |  0:48:16s\n",
      "epoch 36 | loss: 1.12909 | train_balanced_accuracy: 0.57033 | valid_balanced_accuracy: 0.53345 |  0:49:36s\n",
      "epoch 37 | loss: 1.12864 | train_balanced_accuracy: 0.57399 | valid_balanced_accuracy: 0.54992 |  0:50:57s\n",
      "epoch 38 | loss: 1.12586 | train_balanced_accuracy: 0.55224 | valid_balanced_accuracy: 0.51652 |  0:52:17s\n",
      "epoch 39 | loss: 1.15088 | train_balanced_accuracy: 0.51951 | valid_balanced_accuracy: 0.50328 |  0:53:37s\n",
      "epoch 40 | loss: 1.12809 | train_balanced_accuracy: 0.5447  | valid_balanced_accuracy: 0.51113 |  0:54:58s\n",
      "epoch 41 | loss: 1.11809 | train_balanced_accuracy: 0.56543 | valid_balanced_accuracy: 0.52252 |  0:56:18s\n",
      "epoch 42 | loss: 1.12565 | train_balanced_accuracy: 0.52896 | valid_balanced_accuracy: 0.51196 |  0:57:37s\n",
      "epoch 43 | loss: 1.13842 | train_balanced_accuracy: 0.56384 | valid_balanced_accuracy: 0.54894 |  0:58:56s\n",
      "epoch 44 | loss: 1.11462 | train_balanced_accuracy: 0.54873 | valid_balanced_accuracy: 0.52267 |  1:00:13s\n",
      "epoch 45 | loss: 1.11365 | train_balanced_accuracy: 0.55665 | valid_balanced_accuracy: 0.53521 |  1:01:33s\n",
      "\n",
      "Early stopping occured at epoch 45 with best_epoch = 35 and best_valid_balanced_accuracy = 0.55397\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'step_size': 10}, optimizer_params={'lr': 0.01}, n_steps=8, n_shared=5, n_independent=4, n_a=32, momentum=0.02, lambda_sparse=0.001, clip_value=10, total=62.0min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'step_size': 10}, optimizer_params={'lr': 0.01}, n_steps=8, n_shared=5, n_independent=4, n_a=32, momentum=0.02, lambda_sparse=0.001, clip_value=10 \n",
      "epoch 0  | loss: 2.00422 | train_balanced_accuracy: 0.22896 | valid_balanced_accuracy: 0.23389 |  0:01:18s\n",
      "epoch 1  | loss: 1.60754 | train_balanced_accuracy: 0.25897 | valid_balanced_accuracy: 0.24715 |  0:02:35s\n",
      "epoch 2  | loss: 1.55031 | train_balanced_accuracy: 0.26741 | valid_balanced_accuracy: 0.26825 |  0:03:53s\n",
      "epoch 3  | loss: 1.51742 | train_balanced_accuracy: 0.29472 | valid_balanced_accuracy: 0.29784 |  0:05:11s\n",
      "epoch 4  | loss: 1.48239 | train_balanced_accuracy: 0.35832 | valid_balanced_accuracy: 0.35791 |  0:06:29s\n",
      "epoch 5  | loss: 1.42418 | train_balanced_accuracy: 0.37152 | valid_balanced_accuracy: 0.36879 |  0:07:46s\n",
      "epoch 6  | loss: 1.38329 | train_balanced_accuracy: 0.41891 | valid_balanced_accuracy: 0.4121  |  0:09:05s\n",
      "epoch 7  | loss: 1.32351 | train_balanced_accuracy: 0.45939 | valid_balanced_accuracy: 0.45164 |  0:10:24s\n",
      "epoch 8  | loss: 1.26772 | train_balanced_accuracy: 0.46486 | valid_balanced_accuracy: 0.46153 |  0:11:43s\n",
      "epoch 9  | loss: 1.25134 | train_balanced_accuracy: 0.46155 | valid_balanced_accuracy: 0.4602  |  0:13:02s\n",
      "epoch 10 | loss: 1.23638 | train_balanced_accuracy: 0.50447 | valid_balanced_accuracy: 0.48965 |  0:14:21s\n",
      "epoch 11 | loss: 1.22401 | train_balanced_accuracy: 0.46166 | valid_balanced_accuracy: 0.45698 |  0:15:40s\n",
      "epoch 12 | loss: 1.19741 | train_balanced_accuracy: 0.49531 | valid_balanced_accuracy: 0.48698 |  0:16:59s\n",
      "epoch 13 | loss: 1.1754  | train_balanced_accuracy: 0.51332 | valid_balanced_accuracy: 0.50301 |  0:18:18s\n",
      "epoch 14 | loss: 1.15898 | train_balanced_accuracy: 0.53986 | valid_balanced_accuracy: 0.51505 |  0:19:36s\n",
      "epoch 15 | loss: 1.14298 | train_balanced_accuracy: 0.5543  | valid_balanced_accuracy: 0.53863 |  0:20:54s\n",
      "epoch 16 | loss: 1.14221 | train_balanced_accuracy: 0.53015 | valid_balanced_accuracy: 0.5094  |  0:22:12s\n",
      "epoch 17 | loss: 1.13472 | train_balanced_accuracy: 0.56906 | valid_balanced_accuracy: 0.55183 |  0:23:30s\n",
      "epoch 18 | loss: 1.16204 | train_balanced_accuracy: 0.54453 | valid_balanced_accuracy: 0.53485 |  0:24:48s\n",
      "epoch 19 | loss: 1.13025 | train_balanced_accuracy: 0.535   | valid_balanced_accuracy: 0.52173 |  0:26:06s\n",
      "epoch 20 | loss: 1.14996 | train_balanced_accuracy: 0.54571 | valid_balanced_accuracy: 0.53995 |  0:27:24s\n",
      "epoch 21 | loss: 1.14156 | train_balanced_accuracy: 0.53656 | valid_balanced_accuracy: 0.51862 |  0:28:41s\n",
      "epoch 22 | loss: 1.12485 | train_balanced_accuracy: 0.55939 | valid_balanced_accuracy: 0.53871 |  0:29:59s\n",
      "epoch 23 | loss: 1.12646 | train_balanced_accuracy: 0.57041 | valid_balanced_accuracy: 0.55516 |  0:31:18s\n",
      "epoch 24 | loss: 1.11001 | train_balanced_accuracy: 0.57338 | valid_balanced_accuracy: 0.56365 |  0:32:36s\n",
      "epoch 25 | loss: 1.11372 | train_balanced_accuracy: 0.52651 | valid_balanced_accuracy: 0.50981 |  0:33:56s\n",
      "epoch 26 | loss: 1.10302 | train_balanced_accuracy: 0.58808 | valid_balanced_accuracy: 0.57528 |  0:35:16s\n",
      "epoch 27 | loss: 1.09797 | train_balanced_accuracy: 0.5373  | valid_balanced_accuracy: 0.51487 |  0:36:35s\n",
      "epoch 28 | loss: 1.0893  | train_balanced_accuracy: 0.57279 | valid_balanced_accuracy: 0.56176 |  0:37:53s\n",
      "epoch 29 | loss: 1.08744 | train_balanced_accuracy: 0.59803 | valid_balanced_accuracy: 0.58029 |  0:39:11s\n",
      "epoch 30 | loss: 1.08793 | train_balanced_accuracy: 0.57718 | valid_balanced_accuracy: 0.54996 |  0:40:31s\n",
      "epoch 31 | loss: 1.08095 | train_balanced_accuracy: 0.60805 | valid_balanced_accuracy: 0.58684 |  0:41:51s\n",
      "epoch 32 | loss: 1.07651 | train_balanced_accuracy: 0.59341 | valid_balanced_accuracy: 0.58403 |  0:43:12s\n",
      "epoch 33 | loss: 1.09734 | train_balanced_accuracy: 0.59953 | valid_balanced_accuracy: 0.57554 |  0:44:34s\n",
      "epoch 34 | loss: 1.07362 | train_balanced_accuracy: 0.5936  | valid_balanced_accuracy: 0.57809 |  0:45:53s\n",
      "epoch 35 | loss: 1.07015 | train_balanced_accuracy: 0.59741 | valid_balanced_accuracy: 0.57816 |  0:47:11s\n",
      "epoch 36 | loss: 1.06177 | train_balanced_accuracy: 0.592   | valid_balanced_accuracy: 0.57522 |  0:48:30s\n",
      "epoch 37 | loss: 1.06181 | train_balanced_accuracy: 0.60875 | valid_balanced_accuracy: 0.58674 |  0:49:48s\n",
      "epoch 38 | loss: 1.05566 | train_balanced_accuracy: 0.60524 | valid_balanced_accuracy: 0.59278 |  0:51:07s\n",
      "epoch 39 | loss: 1.05939 | train_balanced_accuracy: 0.61712 | valid_balanced_accuracy: 0.59959 |  0:52:25s\n",
      "epoch 40 | loss: 1.05214 | train_balanced_accuracy: 0.62035 | valid_balanced_accuracy: 0.60429 |  0:53:43s\n",
      "epoch 41 | loss: 1.04878 | train_balanced_accuracy: 0.59892 | valid_balanced_accuracy: 0.58908 |  0:55:01s\n",
      "epoch 42 | loss: 1.05172 | train_balanced_accuracy: 0.60564 | valid_balanced_accuracy: 0.57914 |  0:56:20s\n",
      "epoch 43 | loss: 1.056   | train_balanced_accuracy: 0.61176 | valid_balanced_accuracy: 0.59434 |  0:57:38s\n",
      "epoch 44 | loss: 1.05654 | train_balanced_accuracy: 0.61576 | valid_balanced_accuracy: 0.592   |  0:58:58s\n",
      "epoch 45 | loss: 1.05317 | train_balanced_accuracy: 0.62058 | valid_balanced_accuracy: 0.59761 |  1:00:18s\n",
      "epoch 46 | loss: 1.04361 | train_balanced_accuracy: 0.58266 | valid_balanced_accuracy: 0.56449 |  1:01:38s\n",
      "epoch 47 | loss: 1.04596 | train_balanced_accuracy: 0.60367 | valid_balanced_accuracy: 0.59163 |  1:02:58s\n",
      "epoch 48 | loss: 1.04047 | train_balanced_accuracy: 0.61243 | valid_balanced_accuracy: 0.59509 |  1:04:18s\n",
      "epoch 49 | loss: 1.03408 | train_balanced_accuracy: 0.62306 | valid_balanced_accuracy: 0.61042 |  1:05:38s\n",
      "epoch 50 | loss: 1.03381 | train_balanced_accuracy: 0.57866 | valid_balanced_accuracy: 0.55695 |  1:06:57s\n",
      "epoch 51 | loss: 1.03261 | train_balanced_accuracy: 0.60967 | valid_balanced_accuracy: 0.59073 |  1:08:17s\n",
      "epoch 52 | loss: 1.03279 | train_balanced_accuracy: 0.58034 | valid_balanced_accuracy: 0.55863 |  1:09:36s\n",
      "epoch 53 | loss: 1.02695 | train_balanced_accuracy: 0.63699 | valid_balanced_accuracy: 0.60519 |  1:10:56s\n",
      "epoch 54 | loss: 1.0252  | train_balanced_accuracy: 0.62242 | valid_balanced_accuracy: 0.59584 |  1:12:16s\n",
      "epoch 55 | loss: 1.02491 | train_balanced_accuracy: 0.61769 | valid_balanced_accuracy: 0.59519 |  1:13:35s\n",
      "epoch 56 | loss: 1.02314 | train_balanced_accuracy: 0.6177  | valid_balanced_accuracy: 0.58585 |  1:14:54s\n",
      "epoch 57 | loss: 1.01694 | train_balanced_accuracy: 0.61487 | valid_balanced_accuracy: 0.6026  |  1:16:13s\n",
      "epoch 58 | loss: 1.01946 | train_balanced_accuracy: 0.6216  | valid_balanced_accuracy: 0.59453 |  1:17:33s\n",
      "epoch 59 | loss: 1.01675 | train_balanced_accuracy: 0.60704 | valid_balanced_accuracy: 0.58434 |  1:18:51s\n",
      "\n",
      "Early stopping occured at epoch 59 with best_epoch = 49 and best_valid_balanced_accuracy = 0.61042\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'step_size': 10}, optimizer_params={'lr': 0.01}, n_steps=8, n_shared=5, n_independent=4, n_a=32, momentum=0.02, lambda_sparse=0.001, clip_value=10, total=79.3min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'step_size': 10}, optimizer_params={'lr': 0.01}, n_steps=8, n_shared=5, n_independent=4, n_a=32, momentum=0.02, lambda_sparse=0.001, clip_value=10 \n",
      "epoch 0  | loss: 2.01301 | train_balanced_accuracy: 0.24718 | valid_balanced_accuracy: 0.24754 |  0:01:17s\n",
      "epoch 1  | loss: 1.59724 | train_balanced_accuracy: 0.29209 | valid_balanced_accuracy: 0.2774  |  0:02:35s\n",
      "epoch 2  | loss: 1.52329 | train_balanced_accuracy: 0.30991 | valid_balanced_accuracy: 0.3011  |  0:03:53s\n",
      "epoch 3  | loss: 1.46685 | train_balanced_accuracy: 0.36107 | valid_balanced_accuracy: 0.35226 |  0:05:10s\n",
      "epoch 4  | loss: 1.44911 | train_balanced_accuracy: 0.35229 | valid_balanced_accuracy: 0.35378 |  0:06:28s\n",
      "epoch 5  | loss: 1.40415 | train_balanced_accuracy: 0.40933 | valid_balanced_accuracy: 0.39914 |  0:07:45s\n",
      "epoch 6  | loss: 1.35366 | train_balanced_accuracy: 0.40026 | valid_balanced_accuracy: 0.38862 |  0:09:04s\n",
      "epoch 7  | loss: 1.33538 | train_balanced_accuracy: 0.422   | valid_balanced_accuracy: 0.42166 |  0:10:24s\n",
      "epoch 8  | loss: 1.33787 | train_balanced_accuracy: 0.4612  | valid_balanced_accuracy: 0.45277 |  0:11:44s\n",
      "epoch 9  | loss: 1.30815 | train_balanced_accuracy: 0.4654  | valid_balanced_accuracy: 0.45658 |  0:13:05s\n",
      "epoch 10 | loss: 1.30579 | train_balanced_accuracy: 0.46389 | valid_balanced_accuracy: 0.45044 |  0:14:25s\n",
      "epoch 11 | loss: 1.28486 | train_balanced_accuracy: 0.46949 | valid_balanced_accuracy: 0.45913 |  0:15:45s\n",
      "epoch 12 | loss: 1.25978 | train_balanced_accuracy: 0.47879 | valid_balanced_accuracy: 0.46954 |  0:17:05s\n",
      "epoch 13 | loss: 1.25742 | train_balanced_accuracy: 0.46946 | valid_balanced_accuracy: 0.45299 |  0:18:25s\n",
      "epoch 14 | loss: 1.24353 | train_balanced_accuracy: 0.50251 | valid_balanced_accuracy: 0.49572 |  0:19:45s\n",
      "epoch 15 | loss: 1.24049 | train_balanced_accuracy: 0.51796 | valid_balanced_accuracy: 0.50722 |  0:21:05s\n",
      "epoch 16 | loss: 1.21946 | train_balanced_accuracy: 0.51691 | valid_balanced_accuracy: 0.49208 |  0:22:25s\n",
      "epoch 17 | loss: 1.21098 | train_balanced_accuracy: 0.50986 | valid_balanced_accuracy: 0.50924 |  0:23:45s\n",
      "epoch 18 | loss: 1.20207 | train_balanced_accuracy: 0.52434 | valid_balanced_accuracy: 0.50736 |  0:25:06s\n",
      "epoch 19 | loss: 1.19489 | train_balanced_accuracy: 0.47864 | valid_balanced_accuracy: 0.46931 |  0:26:26s\n",
      "epoch 20 | loss: 1.19723 | train_balanced_accuracy: 0.525   | valid_balanced_accuracy: 0.51624 |  0:27:46s\n",
      "epoch 21 | loss: 1.18669 | train_balanced_accuracy: 0.54209 | valid_balanced_accuracy: 0.52177 |  0:29:06s\n",
      "epoch 22 | loss: 1.179   | train_balanced_accuracy: 0.55485 | valid_balanced_accuracy: 0.53742 |  0:30:27s\n",
      "epoch 23 | loss: 1.17004 | train_balanced_accuracy: 0.54529 | valid_balanced_accuracy: 0.53602 |  0:31:48s\n",
      "epoch 24 | loss: 1.18337 | train_balanced_accuracy: 0.55403 | valid_balanced_accuracy: 0.53003 |  0:33:08s\n",
      "epoch 25 | loss: 1.17033 | train_balanced_accuracy: 0.541   | valid_balanced_accuracy: 0.51949 |  0:34:29s\n",
      "epoch 26 | loss: 1.16072 | train_balanced_accuracy: 0.55942 | valid_balanced_accuracy: 0.56192 |  0:35:49s\n",
      "epoch 27 | loss: 1.16448 | train_balanced_accuracy: 0.53692 | valid_balanced_accuracy: 0.51874 |  0:37:09s\n",
      "epoch 28 | loss: 1.1599  | train_balanced_accuracy: 0.56791 | valid_balanced_accuracy: 0.56031 |  0:38:29s\n",
      "epoch 29 | loss: 1.14961 | train_balanced_accuracy: 0.56475 | valid_balanced_accuracy: 0.56428 |  0:39:49s\n",
      "epoch 30 | loss: 1.14758 | train_balanced_accuracy: 0.56845 | valid_balanced_accuracy: 0.55929 |  0:41:07s\n",
      "epoch 31 | loss: 1.14131 | train_balanced_accuracy: 0.57239 | valid_balanced_accuracy: 0.5678  |  0:42:25s\n",
      "epoch 32 | loss: 1.13646 | train_balanced_accuracy: 0.5713  | valid_balanced_accuracy: 0.5428  |  0:43:45s\n",
      "epoch 33 | loss: 1.1334  | train_balanced_accuracy: 0.56554 | valid_balanced_accuracy: 0.54326 |  0:45:05s\n",
      "epoch 34 | loss: 1.12472 | train_balanced_accuracy: 0.55462 | valid_balanced_accuracy: 0.53846 |  0:46:24s\n",
      "epoch 35 | loss: 1.12634 | train_balanced_accuracy: 0.58848 | valid_balanced_accuracy: 0.58677 |  0:47:44s\n",
      "epoch 36 | loss: 1.1178  | train_balanced_accuracy: 0.58526 | valid_balanced_accuracy: 0.58343 |  0:49:02s\n",
      "epoch 37 | loss: 1.11933 | train_balanced_accuracy: 0.58891 | valid_balanced_accuracy: 0.58004 |  0:50:21s\n",
      "epoch 38 | loss: 1.11144 | train_balanced_accuracy: 0.57828 | valid_balanced_accuracy: 0.55424 |  0:51:39s\n",
      "epoch 39 | loss: 1.12739 | train_balanced_accuracy: 0.57376 | valid_balanced_accuracy: 0.57227 |  0:52:56s\n",
      "epoch 40 | loss: 1.13048 | train_balanced_accuracy: 0.59714 | valid_balanced_accuracy: 0.58365 |  0:54:14s\n",
      "epoch 41 | loss: 1.11542 | train_balanced_accuracy: 0.56189 | valid_balanced_accuracy: 0.54075 |  0:55:31s\n",
      "epoch 42 | loss: 1.11169 | train_balanced_accuracy: 0.58855 | valid_balanced_accuracy: 0.55531 |  0:56:48s\n",
      "epoch 43 | loss: 1.10112 | train_balanced_accuracy: 0.58048 | valid_balanced_accuracy: 0.5584  |  0:58:05s\n",
      "epoch 44 | loss: 1.0961  | train_balanced_accuracy: 0.58079 | valid_balanced_accuracy: 0.57823 |  0:59:22s\n",
      "epoch 45 | loss: 1.0999  | train_balanced_accuracy: 0.60375 | valid_balanced_accuracy: 0.59716 |  1:00:41s\n",
      "epoch 46 | loss: 1.0918  | train_balanced_accuracy: 0.61358 | valid_balanced_accuracy: 0.57406 |  1:02:01s\n",
      "epoch 47 | loss: 1.08589 | train_balanced_accuracy: 0.61711 | valid_balanced_accuracy: 0.5914  |  1:03:20s\n",
      "epoch 48 | loss: 1.09201 | train_balanced_accuracy: 0.61992 | valid_balanced_accuracy: 0.56776 |  1:04:39s\n",
      "epoch 49 | loss: 1.08522 | train_balanced_accuracy: 0.59793 | valid_balanced_accuracy: 0.56793 |  1:05:59s\n",
      "epoch 50 | loss: 1.08845 | train_balanced_accuracy: 0.60667 | valid_balanced_accuracy: 0.56803 |  1:07:18s\n",
      "epoch 51 | loss: 1.07504 | train_balanced_accuracy: 0.60262 | valid_balanced_accuracy: 0.58905 |  1:08:38s\n",
      "epoch 52 | loss: 1.07591 | train_balanced_accuracy: 0.61382 | valid_balanced_accuracy: 0.60197 |  1:09:57s\n",
      "epoch 53 | loss: 1.07198 | train_balanced_accuracy: 0.62004 | valid_balanced_accuracy: 0.59656 |  1:11:15s\n",
      "epoch 54 | loss: 1.07048 | train_balanced_accuracy: 0.61268 | valid_balanced_accuracy: 0.57829 |  1:12:32s\n",
      "epoch 55 | loss: 1.09297 | train_balanced_accuracy: 0.60656 | valid_balanced_accuracy: 0.58725 |  1:13:49s\n",
      "epoch 56 | loss: 1.10194 | train_balanced_accuracy: 0.56088 | valid_balanced_accuracy: 0.54822 |  1:15:06s\n",
      "epoch 57 | loss: 1.11546 | train_balanced_accuracy: 0.58014 | valid_balanced_accuracy: 0.57778 |  1:16:23s\n",
      "epoch 58 | loss: 1.09658 | train_balanced_accuracy: 0.59242 | valid_balanced_accuracy: 0.58264 |  1:17:40s\n",
      "epoch 59 | loss: 1.09498 | train_balanced_accuracy: 0.55772 | valid_balanced_accuracy: 0.55173 |  1:18:57s\n",
      "epoch 60 | loss: 1.11084 | train_balanced_accuracy: 0.56125 | valid_balanced_accuracy: 0.54274 |  1:20:14s\n",
      "epoch 61 | loss: 1.12168 | train_balanced_accuracy: 0.59619 | valid_balanced_accuracy: 0.58414 |  1:21:31s\n",
      "epoch 62 | loss: 1.09943 | train_balanced_accuracy: 0.59143 | valid_balanced_accuracy: 0.57873 |  1:22:48s\n",
      "\n",
      "Early stopping occured at epoch 62 with best_epoch = 52 and best_valid_balanced_accuracy = 0.60197\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'step_size': 10}, optimizer_params={'lr': 0.01}, n_steps=8, n_shared=5, n_independent=4, n_a=32, momentum=0.02, lambda_sparse=0.001, clip_value=10, total=83.2min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'step_size': 10}, optimizer_params={'lr': 0.01}, n_steps=8, n_shared=5, n_independent=4, n_a=32, momentum=0.02, lambda_sparse=0.001, clip_value=10 \n",
      "epoch 0  | loss: 2.01389 | train_balanced_accuracy: 0.22985 | valid_balanced_accuracy: 0.22961 |  0:01:17s\n",
      "epoch 1  | loss: 1.63089 | train_balanced_accuracy: 0.24816 | valid_balanced_accuracy: 0.24856 |  0:02:36s\n",
      "epoch 2  | loss: 1.57875 | train_balanced_accuracy: 0.27551 | valid_balanced_accuracy: 0.27271 |  0:03:54s\n",
      "epoch 3  | loss: 1.50981 | train_balanced_accuracy: 0.30132 | valid_balanced_accuracy: 0.29515 |  0:05:14s\n",
      "epoch 4  | loss: 1.47082 | train_balanced_accuracy: 0.35614 | valid_balanced_accuracy: 0.35008 |  0:06:36s\n",
      "epoch 5  | loss: 1.43685 | train_balanced_accuracy: 0.35384 | valid_balanced_accuracy: 0.34147 |  0:07:54s\n",
      "epoch 6  | loss: 1.46968 | train_balanced_accuracy: 0.33932 | valid_balanced_accuracy: 0.33861 |  0:09:12s\n",
      "epoch 7  | loss: 1.45434 | train_balanced_accuracy: 0.37236 | valid_balanced_accuracy: 0.36539 |  0:10:29s\n",
      "epoch 8  | loss: 1.43236 | train_balanced_accuracy: 0.3693  | valid_balanced_accuracy: 0.36458 |  0:11:47s\n",
      "epoch 9  | loss: 1.4528  | train_balanced_accuracy: 0.39335 | valid_balanced_accuracy: 0.39231 |  0:13:05s\n",
      "epoch 10 | loss: 1.42125 | train_balanced_accuracy: 0.40299 | valid_balanced_accuracy: 0.39613 |  0:14:23s\n",
      "epoch 11 | loss: 1.40585 | train_balanced_accuracy: 0.4095  | valid_balanced_accuracy: 0.4     |  0:15:41s\n",
      "epoch 12 | loss: 1.38368 | train_balanced_accuracy: 0.40906 | valid_balanced_accuracy: 0.40807 |  0:16:59s\n",
      "epoch 13 | loss: 1.36921 | train_balanced_accuracy: 0.42528 | valid_balanced_accuracy: 0.42019 |  0:18:17s\n",
      "epoch 14 | loss: 1.36226 | train_balanced_accuracy: 0.42902 | valid_balanced_accuracy: 0.42386 |  0:19:36s\n",
      "epoch 15 | loss: 1.35621 | train_balanced_accuracy: 0.41527 | valid_balanced_accuracy: 0.40666 |  0:20:56s\n",
      "epoch 16 | loss: 1.36963 | train_balanced_accuracy: 0.4513  | valid_balanced_accuracy: 0.43728 |  0:22:16s\n",
      "epoch 17 | loss: 1.34505 | train_balanced_accuracy: 0.42375 | valid_balanced_accuracy: 0.41539 |  0:23:36s\n",
      "epoch 18 | loss: 1.3336  | train_balanced_accuracy: 0.44844 | valid_balanced_accuracy: 0.43058 |  0:24:55s\n",
      "epoch 19 | loss: 1.31798 | train_balanced_accuracy: 0.45391 | valid_balanced_accuracy: 0.44471 |  0:26:14s\n",
      "epoch 20 | loss: 1.30324 | train_balanced_accuracy: 0.4739  | valid_balanced_accuracy: 0.46401 |  0:27:31s\n",
      "epoch 21 | loss: 1.29886 | train_balanced_accuracy: 0.46597 | valid_balanced_accuracy: 0.45871 |  0:28:49s\n",
      "epoch 22 | loss: 1.29888 | train_balanced_accuracy: 0.45901 | valid_balanced_accuracy: 0.44888 |  0:30:08s\n",
      "epoch 23 | loss: 1.28349 | train_balanced_accuracy: 0.42211 | valid_balanced_accuracy: 0.42318 |  0:31:28s\n",
      "epoch 24 | loss: 1.2799  | train_balanced_accuracy: 0.48027 | valid_balanced_accuracy: 0.47691 |  0:32:47s\n",
      "epoch 25 | loss: 1.29208 | train_balanced_accuracy: 0.46495 | valid_balanced_accuracy: 0.45465 |  0:34:05s\n",
      "epoch 26 | loss: 1.29293 | train_balanced_accuracy: 0.4756  | valid_balanced_accuracy: 0.46738 |  0:35:24s\n",
      "epoch 27 | loss: 1.29167 | train_balanced_accuracy: 0.44753 | valid_balanced_accuracy: 0.44048 |  0:36:42s\n",
      "epoch 28 | loss: 1.29109 | train_balanced_accuracy: 0.49384 | valid_balanced_accuracy: 0.4972  |  0:38:01s\n",
      "epoch 29 | loss: 1.27407 | train_balanced_accuracy: 0.47507 | valid_balanced_accuracy: 0.46678 |  0:39:19s\n",
      "epoch 30 | loss: 1.25806 | train_balanced_accuracy: 0.47378 | valid_balanced_accuracy: 0.46026 |  0:40:36s\n",
      "epoch 31 | loss: 1.26699 | train_balanced_accuracy: 0.51897 | valid_balanced_accuracy: 0.48838 |  0:41:54s\n",
      "epoch 32 | loss: 1.25629 | train_balanced_accuracy: 0.51366 | valid_balanced_accuracy: 0.50814 |  0:43:12s\n",
      "epoch 33 | loss: 1.25007 | train_balanced_accuracy: 0.50569 | valid_balanced_accuracy: 0.4974  |  0:44:29s\n",
      "epoch 34 | loss: 1.24187 | train_balanced_accuracy: 0.49194 | valid_balanced_accuracy: 0.4855  |  0:45:47s\n",
      "epoch 35 | loss: 1.25633 | train_balanced_accuracy: 0.52019 | valid_balanced_accuracy: 0.51179 |  0:47:05s\n",
      "epoch 36 | loss: 1.24634 | train_balanced_accuracy: 0.50651 | valid_balanced_accuracy: 0.49995 |  0:48:22s\n",
      "epoch 37 | loss: 1.26501 | train_balanced_accuracy: 0.48769 | valid_balanced_accuracy: 0.47579 |  0:49:40s\n",
      "epoch 38 | loss: 1.26475 | train_balanced_accuracy: 0.51972 | valid_balanced_accuracy: 0.50654 |  0:50:58s\n",
      "epoch 39 | loss: 1.25041 | train_balanced_accuracy: 0.53778 | valid_balanced_accuracy: 0.52011 |  0:52:18s\n",
      "epoch 40 | loss: 1.2469  | train_balanced_accuracy: 0.48016 | valid_balanced_accuracy: 0.46656 |  0:53:38s\n",
      "epoch 41 | loss: 1.24157 | train_balanced_accuracy: 0.50033 | valid_balanced_accuracy: 0.49008 |  0:54:58s\n",
      "epoch 42 | loss: 1.23647 | train_balanced_accuracy: 0.51394 | valid_balanced_accuracy: 0.49898 |  0:56:18s\n",
      "epoch 43 | loss: 1.21946 | train_balanced_accuracy: 0.53243 | valid_balanced_accuracy: 0.52175 |  0:57:38s\n",
      "epoch 44 | loss: 1.2188  | train_balanced_accuracy: 0.53218 | valid_balanced_accuracy: 0.51252 |  0:58:59s\n",
      "epoch 45 | loss: 1.21143 | train_balanced_accuracy: 0.55367 | valid_balanced_accuracy: 0.52762 |  1:00:20s\n",
      "epoch 46 | loss: 1.22145 | train_balanced_accuracy: 0.53487 | valid_balanced_accuracy: 0.51637 |  1:01:40s\n",
      "epoch 47 | loss: 1.20655 | train_balanced_accuracy: 0.56771 | valid_balanced_accuracy: 0.54451 |  1:02:58s\n",
      "epoch 48 | loss: 1.19616 | train_balanced_accuracy: 0.57581 | valid_balanced_accuracy: 0.53948 |  1:04:16s\n",
      "epoch 49 | loss: 1.18423 | train_balanced_accuracy: 0.52951 | valid_balanced_accuracy: 0.51802 |  1:05:35s\n",
      "epoch 50 | loss: 1.19231 | train_balanced_accuracy: 0.57329 | valid_balanced_accuracy: 0.53084 |  1:06:53s\n",
      "epoch 51 | loss: 1.18368 | train_balanced_accuracy: 0.56878 | valid_balanced_accuracy: 0.54201 |  1:08:11s\n",
      "epoch 52 | loss: 1.17931 | train_balanced_accuracy: 0.58011 | valid_balanced_accuracy: 0.53548 |  1:09:29s\n",
      "epoch 53 | loss: 1.17766 | train_balanced_accuracy: 0.59234 | valid_balanced_accuracy: 0.55561 |  1:10:47s\n",
      "epoch 54 | loss: 1.17936 | train_balanced_accuracy: 0.58112 | valid_balanced_accuracy: 0.54426 |  1:12:06s\n",
      "epoch 55 | loss: 1.15457 | train_balanced_accuracy: 0.60312 | valid_balanced_accuracy: 0.56615 |  1:13:24s\n",
      "epoch 56 | loss: 1.15641 | train_balanced_accuracy: 0.57288 | valid_balanced_accuracy: 0.53469 |  1:14:42s\n",
      "epoch 57 | loss: 1.14652 | train_balanced_accuracy: 0.58159 | valid_balanced_accuracy: 0.54762 |  1:16:00s\n",
      "epoch 58 | loss: 1.14348 | train_balanced_accuracy: 0.58164 | valid_balanced_accuracy: 0.53861 |  1:17:18s\n",
      "epoch 59 | loss: 1.13679 | train_balanced_accuracy: 0.60264 | valid_balanced_accuracy: 0.563   |  1:18:36s\n",
      "epoch 60 | loss: 1.12633 | train_balanced_accuracy: 0.58321 | valid_balanced_accuracy: 0.53822 |  1:19:54s\n",
      "epoch 61 | loss: 1.12802 | train_balanced_accuracy: 0.60432 | valid_balanced_accuracy: 0.56394 |  1:21:11s\n",
      "epoch 62 | loss: 1.14107 | train_balanced_accuracy: 0.59273 | valid_balanced_accuracy: 0.5576  |  1:22:29s\n",
      "epoch 63 | loss: 1.12344 | train_balanced_accuracy: 0.5784  | valid_balanced_accuracy: 0.54261 |  1:23:47s\n",
      "epoch 64 | loss: 1.15044 | train_balanced_accuracy: 0.58988 | valid_balanced_accuracy: 0.55172 |  1:25:05s\n",
      "epoch 65 | loss: 1.14457 | train_balanced_accuracy: 0.60638 | valid_balanced_accuracy: 0.56428 |  1:26:23s\n",
      "\n",
      "Early stopping occured at epoch 65 with best_epoch = 55 and best_valid_balanced_accuracy = 0.56615\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'step_size': 10}, optimizer_params={'lr': 0.01}, n_steps=8, n_shared=5, n_independent=4, n_a=32, momentum=0.02, lambda_sparse=0.001, clip_value=10, total=86.8min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'step_size': 10}, optimizer_params={'lr': 0.01}, n_steps=8, n_shared=5, n_independent=4, n_a=32, momentum=0.02, lambda_sparse=0.001, clip_value=10 \n",
      "epoch 0  | loss: 2.04859 | train_balanced_accuracy: 0.26592 | valid_balanced_accuracy: 0.26237 |  0:01:19s\n",
      "epoch 1  | loss: 1.59165 | train_balanced_accuracy: 0.24989 | valid_balanced_accuracy: 0.25312 |  0:02:39s\n",
      "epoch 2  | loss: 1.48781 | train_balanced_accuracy: 0.31688 | valid_balanced_accuracy: 0.30748 |  0:03:59s\n",
      "epoch 3  | loss: 1.43163 | train_balanced_accuracy: 0.36577 | valid_balanced_accuracy: 0.3626  |  0:05:20s\n",
      "epoch 4  | loss: 1.40155 | train_balanced_accuracy: 0.3685  | valid_balanced_accuracy: 0.36628 |  0:06:40s\n",
      "epoch 5  | loss: 1.36778 | train_balanced_accuracy: 0.41313 | valid_balanced_accuracy: 0.40251 |  0:07:59s\n",
      "epoch 6  | loss: 1.31614 | train_balanced_accuracy: 0.40046 | valid_balanced_accuracy: 0.39925 |  0:09:18s\n",
      "epoch 7  | loss: 1.29284 | train_balanced_accuracy: 0.48188 | valid_balanced_accuracy: 0.47689 |  0:10:39s\n",
      "epoch 8  | loss: 1.28178 | train_balanced_accuracy: 0.47594 | valid_balanced_accuracy: 0.45918 |  0:11:59s\n",
      "epoch 9  | loss: 1.24994 | train_balanced_accuracy: 0.51973 | valid_balanced_accuracy: 0.51152 |  0:13:20s\n",
      "epoch 10 | loss: 1.21202 | train_balanced_accuracy: 0.51295 | valid_balanced_accuracy: 0.50356 |  0:14:40s\n",
      "epoch 11 | loss: 1.1883  | train_balanced_accuracy: 0.52804 | valid_balanced_accuracy: 0.50359 |  0:16:00s\n",
      "epoch 12 | loss: 1.17256 | train_balanced_accuracy: 0.5331  | valid_balanced_accuracy: 0.51989 |  0:17:19s\n",
      "epoch 13 | loss: 1.16439 | train_balanced_accuracy: 0.54985 | valid_balanced_accuracy: 0.53833 |  0:18:40s\n",
      "epoch 14 | loss: 1.16363 | train_balanced_accuracy: 0.5263  | valid_balanced_accuracy: 0.51375 |  0:19:58s\n",
      "epoch 15 | loss: 1.13834 | train_balanced_accuracy: 0.56687 | valid_balanced_accuracy: 0.55729 |  0:21:16s\n",
      "epoch 16 | loss: 1.1441  | train_balanced_accuracy: 0.55602 | valid_balanced_accuracy: 0.53176 |  0:22:33s\n",
      "epoch 17 | loss: 1.13217 | train_balanced_accuracy: 0.52021 | valid_balanced_accuracy: 0.50909 |  0:23:50s\n",
      "epoch 18 | loss: 1.11582 | train_balanced_accuracy: 0.57446 | valid_balanced_accuracy: 0.54714 |  0:25:07s\n",
      "epoch 19 | loss: 1.10651 | train_balanced_accuracy: 0.58008 | valid_balanced_accuracy: 0.56429 |  0:26:25s\n",
      "epoch 20 | loss: 1.10749 | train_balanced_accuracy: 0.60198 | valid_balanced_accuracy: 0.5735  |  0:27:42s\n",
      "epoch 21 | loss: 1.09227 | train_balanced_accuracy: 0.56193 | valid_balanced_accuracy: 0.55114 |  0:29:00s\n",
      "epoch 22 | loss: 1.07884 | train_balanced_accuracy: 0.60456 | valid_balanced_accuracy: 0.5749  |  0:30:17s\n",
      "epoch 23 | loss: 1.06783 | train_balanced_accuracy: 0.57165 | valid_balanced_accuracy: 0.54904 |  0:31:34s\n",
      "epoch 24 | loss: 1.0695  | train_balanced_accuracy: 0.58189 | valid_balanced_accuracy: 0.54465 |  0:32:51s\n",
      "epoch 25 | loss: 1.08001 | train_balanced_accuracy: 0.62799 | valid_balanced_accuracy: 0.5932  |  0:34:09s\n",
      "epoch 26 | loss: 1.06748 | train_balanced_accuracy: 0.59389 | valid_balanced_accuracy: 0.57343 |  0:35:27s\n",
      "epoch 27 | loss: 1.06531 | train_balanced_accuracy: 0.58545 | valid_balanced_accuracy: 0.54388 |  0:36:44s\n",
      "epoch 28 | loss: 1.06702 | train_balanced_accuracy: 0.61673 | valid_balanced_accuracy: 0.58114 |  0:38:02s\n",
      "epoch 29 | loss: 1.06559 | train_balanced_accuracy: 0.60615 | valid_balanced_accuracy: 0.56492 |  0:39:20s\n",
      "epoch 30 | loss: 1.04426 | train_balanced_accuracy: 0.61858 | valid_balanced_accuracy: 0.5834  |  0:40:38s\n",
      "epoch 31 | loss: 1.03566 | train_balanced_accuracy: 0.60876 | valid_balanced_accuracy: 0.56234 |  0:41:56s\n",
      "epoch 32 | loss: 1.03866 | train_balanced_accuracy: 0.61773 | valid_balanced_accuracy: 0.58782 |  0:43:14s\n",
      "epoch 33 | loss: 1.03879 | train_balanced_accuracy: 0.61849 | valid_balanced_accuracy: 0.58178 |  0:44:31s\n",
      "epoch 34 | loss: 1.0226  | train_balanced_accuracy: 0.62925 | valid_balanced_accuracy: 0.59644 |  0:45:49s\n",
      "epoch 35 | loss: 1.02209 | train_balanced_accuracy: 0.64143 | valid_balanced_accuracy: 0.60067 |  0:47:09s\n",
      "epoch 36 | loss: 1.00863 | train_balanced_accuracy: 0.63179 | valid_balanced_accuracy: 0.59562 |  0:48:29s\n",
      "epoch 37 | loss: 1.00681 | train_balanced_accuracy: 0.66297 | valid_balanced_accuracy: 0.60124 |  0:49:50s\n",
      "epoch 38 | loss: 1.00159 | train_balanced_accuracy: 0.65106 | valid_balanced_accuracy: 0.60928 |  0:51:11s\n",
      "epoch 39 | loss: 0.99655 | train_balanced_accuracy: 0.64955 | valid_balanced_accuracy: 0.61261 |  0:52:32s\n",
      "epoch 40 | loss: 0.99231 | train_balanced_accuracy: 0.62774 | valid_balanced_accuracy: 0.60596 |  0:53:52s\n",
      "epoch 41 | loss: 0.99195 | train_balanced_accuracy: 0.61534 | valid_balanced_accuracy: 0.59136 |  0:55:12s\n",
      "epoch 42 | loss: 0.9847  | train_balanced_accuracy: 0.62976 | valid_balanced_accuracy: 0.60434 |  0:56:33s\n",
      "epoch 43 | loss: 0.9822  | train_balanced_accuracy: 0.62943 | valid_balanced_accuracy: 0.59608 |  0:57:54s\n",
      "epoch 44 | loss: 0.98148 | train_balanced_accuracy: 0.61711 | valid_balanced_accuracy: 0.59336 |  0:59:14s\n",
      "epoch 45 | loss: 0.97918 | train_balanced_accuracy: 0.67888 | valid_balanced_accuracy: 0.63736 |  1:00:35s\n",
      "epoch 46 | loss: 0.97779 | train_balanced_accuracy: 0.66726 | valid_balanced_accuracy: 0.61507 |  1:01:53s\n",
      "epoch 47 | loss: 0.9667  | train_balanced_accuracy: 0.65642 | valid_balanced_accuracy: 0.61874 |  1:03:11s\n",
      "epoch 48 | loss: 0.97221 | train_balanced_accuracy: 0.65922 | valid_balanced_accuracy: 0.61727 |  1:04:30s\n",
      "epoch 49 | loss: 0.9672  | train_balanced_accuracy: 0.66494 | valid_balanced_accuracy: 0.62325 |  1:05:48s\n",
      "epoch 50 | loss: 0.96289 | train_balanced_accuracy: 0.66366 | valid_balanced_accuracy: 0.62422 |  1:07:06s\n",
      "epoch 51 | loss: 0.9567  | train_balanced_accuracy: 0.66467 | valid_balanced_accuracy: 0.62341 |  1:08:24s\n",
      "epoch 52 | loss: 0.96567 | train_balanced_accuracy: 0.65941 | valid_balanced_accuracy: 0.62166 |  1:09:43s\n",
      "epoch 53 | loss: 0.96431 | train_balanced_accuracy: 0.66092 | valid_balanced_accuracy: 0.61731 |  1:11:01s\n",
      "epoch 54 | loss: 0.9523  | train_balanced_accuracy: 0.66436 | valid_balanced_accuracy: 0.6142  |  1:12:20s\n",
      "epoch 55 | loss: 0.95277 | train_balanced_accuracy: 0.68286 | valid_balanced_accuracy: 0.61328 |  1:13:39s\n",
      "\n",
      "Early stopping occured at epoch 55 with best_epoch = 45 and best_valid_balanced_accuracy = 0.63736\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'step_size': 10}, optimizer_params={'lr': 0.01}, n_steps=8, n_shared=5, n_independent=4, n_a=32, momentum=0.02, lambda_sparse=0.001, clip_value=10, total=74.1min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'gamma': 1.8}, optimizer_params={'lr': 1e-05}, n_steps=8, n_shared=4, n_independent=1, n_a=16, momentum=0.05, lambda_sparse=0.01, clip_value=0.01 \n",
      "epoch 0  | loss: 6.60183 | train_balanced_accuracy: 0.03686 | valid_balanced_accuracy: 0.03849 |  0:00:49s\n",
      "epoch 1  | loss: 6.47131 | train_balanced_accuracy: 0.03757 | valid_balanced_accuracy: 0.03646 |  0:01:39s\n",
      "epoch 2  | loss: 6.33675 | train_balanced_accuracy: 0.03619 | valid_balanced_accuracy: 0.03637 |  0:02:29s\n",
      "epoch 3  | loss: 6.2068  | train_balanced_accuracy: 0.03839 | valid_balanced_accuracy: 0.03805 |  0:03:19s\n",
      "epoch 4  | loss: 6.07095 | train_balanced_accuracy: 0.03974 | valid_balanced_accuracy: 0.04066 |  0:04:09s\n",
      "epoch 5  | loss: 5.94783 | train_balanced_accuracy: 0.03763 | valid_balanced_accuracy: 0.03969 |  0:04:58s\n",
      "epoch 6  | loss: 5.81994 | train_balanced_accuracy: 0.03891 | valid_balanced_accuracy: 0.03981 |  0:05:48s\n",
      "epoch 7  | loss: 5.71762 | train_balanced_accuracy: 0.04115 | valid_balanced_accuracy: 0.04078 |  0:06:37s\n",
      "epoch 8  | loss: 5.60907 | train_balanced_accuracy: 0.04042 | valid_balanced_accuracy: 0.03874 |  0:07:27s\n",
      "epoch 9  | loss: 5.49915 | train_balanced_accuracy: 0.04141 | valid_balanced_accuracy: 0.04127 |  0:08:17s\n",
      "epoch 10 | loss: 5.38453 | train_balanced_accuracy: 0.04119 | valid_balanced_accuracy: 0.04235 |  0:09:07s\n",
      "epoch 11 | loss: 5.28955 | train_balanced_accuracy: 0.04318 | valid_balanced_accuracy: 0.04283 |  0:09:56s\n",
      "epoch 12 | loss: 5.18744 | train_balanced_accuracy: 0.04554 | valid_balanced_accuracy: 0.04673 |  0:10:46s\n",
      "epoch 13 | loss: 5.07395 | train_balanced_accuracy: 0.04653 | valid_balanced_accuracy: 0.04689 |  0:11:36s\n",
      "epoch 14 | loss: 4.97139 | train_balanced_accuracy: 0.04394 | valid_balanced_accuracy: 0.04379 |  0:12:26s\n",
      "epoch 15 | loss: 4.87373 | train_balanced_accuracy: 0.04581 | valid_balanced_accuracy: 0.0469  |  0:13:16s\n",
      "epoch 16 | loss: 4.79706 | train_balanced_accuracy: 0.04538 | valid_balanced_accuracy: 0.04675 |  0:14:06s\n",
      "epoch 17 | loss: 4.70898 | train_balanced_accuracy: 0.04606 | valid_balanced_accuracy: 0.04988 |  0:14:55s\n",
      "epoch 18 | loss: 4.62993 | train_balanced_accuracy: 0.04617 | valid_balanced_accuracy: 0.04808 |  0:15:45s\n",
      "epoch 19 | loss: 4.54491 | train_balanced_accuracy: 0.04969 | valid_balanced_accuracy: 0.05043 |  0:16:34s\n",
      "epoch 20 | loss: 4.46913 | train_balanced_accuracy: 0.05066 | valid_balanced_accuracy: 0.05114 |  0:17:24s\n",
      "epoch 21 | loss: 4.39342 | train_balanced_accuracy: 0.05182 | valid_balanced_accuracy: 0.05354 |  0:18:14s\n",
      "epoch 22 | loss: 4.31522 | train_balanced_accuracy: 0.04861 | valid_balanced_accuracy: 0.05049 |  0:19:04s\n",
      "epoch 23 | loss: 4.24333 | train_balanced_accuracy: 0.05179 | valid_balanced_accuracy: 0.05319 |  0:19:54s\n",
      "epoch 24 | loss: 4.17683 | train_balanced_accuracy: 0.05098 | valid_balanced_accuracy: 0.05156 |  0:20:44s\n",
      "epoch 25 | loss: 4.11524 | train_balanced_accuracy: 0.05247 | valid_balanced_accuracy: 0.05514 |  0:21:34s\n",
      "epoch 26 | loss: 4.05002 | train_balanced_accuracy: 0.05297 | valid_balanced_accuracy: 0.05429 |  0:22:24s\n",
      "epoch 27 | loss: 3.9828  | train_balanced_accuracy: 0.05255 | valid_balanced_accuracy: 0.05342 |  0:23:15s\n",
      "epoch 28 | loss: 3.92215 | train_balanced_accuracy: 0.05327 | valid_balanced_accuracy: 0.05475 |  0:24:05s\n",
      "epoch 29 | loss: 3.8705  | train_balanced_accuracy: 0.05355 | valid_balanced_accuracy: 0.05554 |  0:24:55s\n",
      "epoch 30 | loss: 3.81033 | train_balanced_accuracy: 0.05347 | valid_balanced_accuracy: 0.05528 |  0:25:46s\n",
      "epoch 31 | loss: 3.75428 | train_balanced_accuracy: 0.05397 | valid_balanced_accuracy: 0.05652 |  0:26:37s\n",
      "epoch 32 | loss: 3.7095  | train_balanced_accuracy: 0.05563 | valid_balanced_accuracy: 0.058   |  0:27:28s\n",
      "epoch 33 | loss: 3.65671 | train_balanced_accuracy: 0.05519 | valid_balanced_accuracy: 0.05696 |  0:28:18s\n",
      "epoch 34 | loss: 3.60927 | train_balanced_accuracy: 0.05497 | valid_balanced_accuracy: 0.05802 |  0:29:08s\n",
      "epoch 35 | loss: 3.56509 | train_balanced_accuracy: 0.05538 | valid_balanced_accuracy: 0.0565  |  0:29:58s\n",
      "epoch 36 | loss: 3.5233  | train_balanced_accuracy: 0.0551  | valid_balanced_accuracy: 0.05848 |  0:30:48s\n",
      "epoch 37 | loss: 3.48021 | train_balanced_accuracy: 0.05623 | valid_balanced_accuracy: 0.05755 |  0:31:40s\n",
      "epoch 38 | loss: 3.44971 | train_balanced_accuracy: 0.05472 | valid_balanced_accuracy: 0.05567 |  0:32:32s\n",
      "epoch 39 | loss: 3.40434 | train_balanced_accuracy: 0.05683 | valid_balanced_accuracy: 0.06011 |  0:33:23s\n",
      "epoch 40 | loss: 3.37173 | train_balanced_accuracy: 0.05787 | valid_balanced_accuracy: 0.05803 |  0:34:14s\n",
      "epoch 41 | loss: 3.34063 | train_balanced_accuracy: 0.05612 | valid_balanced_accuracy: 0.05781 |  0:35:04s\n",
      "epoch 42 | loss: 3.31058 | train_balanced_accuracy: 0.05991 | valid_balanced_accuracy: 0.06031 |  0:35:53s\n",
      "epoch 43 | loss: 3.27019 | train_balanced_accuracy: 0.05999 | valid_balanced_accuracy: 0.0614  |  0:36:43s\n",
      "epoch 44 | loss: 3.24181 | train_balanced_accuracy: 0.05716 | valid_balanced_accuracy: 0.05789 |  0:37:33s\n",
      "epoch 45 | loss: 3.21725 | train_balanced_accuracy: 0.0608  | valid_balanced_accuracy: 0.06217 |  0:38:23s\n",
      "epoch 46 | loss: 3.19286 | train_balanced_accuracy: 0.0599  | valid_balanced_accuracy: 0.06107 |  0:39:12s\n",
      "epoch 47 | loss: 3.16456 | train_balanced_accuracy: 0.06153 | valid_balanced_accuracy: 0.0624  |  0:40:02s\n",
      "epoch 48 | loss: 3.12797 | train_balanced_accuracy: 0.06079 | valid_balanced_accuracy: 0.06306 |  0:40:52s\n",
      "epoch 49 | loss: 3.10123 | train_balanced_accuracy: 0.06278 | valid_balanced_accuracy: 0.06515 |  0:41:42s\n",
      "epoch 50 | loss: 3.08544 | train_balanced_accuracy: 0.06041 | valid_balanced_accuracy: 0.06226 |  0:42:31s\n",
      "epoch 51 | loss: 3.06215 | train_balanced_accuracy: 0.06221 | valid_balanced_accuracy: 0.06325 |  0:43:21s\n",
      "epoch 52 | loss: 3.03618 | train_balanced_accuracy: 0.06145 | valid_balanced_accuracy: 0.06318 |  0:44:11s\n",
      "epoch 53 | loss: 3.01399 | train_balanced_accuracy: 0.06267 | valid_balanced_accuracy: 0.06226 |  0:45:00s\n",
      "epoch 54 | loss: 2.99133 | train_balanced_accuracy: 0.06172 | valid_balanced_accuracy: 0.06266 |  0:45:50s\n",
      "epoch 55 | loss: 2.97284 | train_balanced_accuracy: 0.06127 | valid_balanced_accuracy: 0.06278 |  0:46:40s\n",
      "epoch 56 | loss: 2.94761 | train_balanced_accuracy: 0.06229 | valid_balanced_accuracy: 0.06291 |  0:47:30s\n",
      "epoch 57 | loss: 2.93397 | train_balanced_accuracy: 0.0629  | valid_balanced_accuracy: 0.06502 |  0:48:19s\n",
      "epoch 58 | loss: 2.91421 | train_balanced_accuracy: 0.06178 | valid_balanced_accuracy: 0.06326 |  0:49:09s\n",
      "epoch 59 | loss: 2.89763 | train_balanced_accuracy: 0.0638  | valid_balanced_accuracy: 0.06436 |  0:49:58s\n",
      "\n",
      "Early stopping occured at epoch 59 with best_epoch = 49 and best_valid_balanced_accuracy = 0.06515\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'gamma': 1.8}, optimizer_params={'lr': 1e-05}, n_steps=8, n_shared=4, n_independent=1, n_a=16, momentum=0.05, lambda_sparse=0.01, clip_value=0.01, total=50.2min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'gamma': 1.8}, optimizer_params={'lr': 1e-05}, n_steps=8, n_shared=4, n_independent=1, n_a=16, momentum=0.05, lambda_sparse=0.01, clip_value=0.01 \n",
      "epoch 0  | loss: 6.60642 | train_balanced_accuracy: 0.03567 | valid_balanced_accuracy: 0.03727 |  0:00:49s\n",
      "epoch 1  | loss: 6.46975 | train_balanced_accuracy: 0.03611 | valid_balanced_accuracy: 0.03667 |  0:01:38s\n",
      "epoch 2  | loss: 6.33607 | train_balanced_accuracy: 0.03797 | valid_balanced_accuracy: 0.03838 |  0:02:28s\n",
      "epoch 3  | loss: 6.20199 | train_balanced_accuracy: 0.03638 | valid_balanced_accuracy: 0.04022 |  0:03:17s\n",
      "epoch 4  | loss: 6.06481 | train_balanced_accuracy: 0.03863 | valid_balanced_accuracy: 0.04006 |  0:04:06s\n",
      "epoch 5  | loss: 5.94169 | train_balanced_accuracy: 0.03769 | valid_balanced_accuracy: 0.03719 |  0:04:56s\n",
      "epoch 6  | loss: 5.83226 | train_balanced_accuracy: 0.03862 | valid_balanced_accuracy: 0.0395  |  0:05:45s\n",
      "epoch 7  | loss: 5.705   | train_balanced_accuracy: 0.03908 | valid_balanced_accuracy: 0.04047 |  0:06:35s\n",
      "epoch 8  | loss: 5.58911 | train_balanced_accuracy: 0.04259 | valid_balanced_accuracy: 0.04297 |  0:07:25s\n",
      "epoch 9  | loss: 5.47233 | train_balanced_accuracy: 0.04091 | valid_balanced_accuracy: 0.04165 |  0:08:15s\n",
      "epoch 10 | loss: 5.37032 | train_balanced_accuracy: 0.04185 | valid_balanced_accuracy: 0.04259 |  0:09:04s\n",
      "epoch 11 | loss: 5.25602 | train_balanced_accuracy: 0.04431 | valid_balanced_accuracy: 0.04401 |  0:09:53s\n",
      "epoch 12 | loss: 5.15104 | train_balanced_accuracy: 0.04619 | valid_balanced_accuracy: 0.0448  |  0:10:43s\n",
      "epoch 13 | loss: 5.03851 | train_balanced_accuracy: 0.04499 | valid_balanced_accuracy: 0.04691 |  0:11:33s\n",
      "epoch 14 | loss: 4.93826 | train_balanced_accuracy: 0.04507 | valid_balanced_accuracy: 0.04597 |  0:12:22s\n",
      "epoch 15 | loss: 4.8464  | train_balanced_accuracy: 0.04698 | valid_balanced_accuracy: 0.04549 |  0:13:11s\n",
      "epoch 16 | loss: 4.75714 | train_balanced_accuracy: 0.04617 | valid_balanced_accuracy: 0.04718 |  0:14:01s\n",
      "epoch 17 | loss: 4.67427 | train_balanced_accuracy: 0.04906 | valid_balanced_accuracy: 0.04924 |  0:14:50s\n",
      "epoch 18 | loss: 4.59185 | train_balanced_accuracy: 0.04604 | valid_balanced_accuracy: 0.04499 |  0:15:39s\n",
      "epoch 19 | loss: 4.50084 | train_balanced_accuracy: 0.05037 | valid_balanced_accuracy: 0.04929 |  0:16:29s\n",
      "epoch 20 | loss: 4.43006 | train_balanced_accuracy: 0.04767 | valid_balanced_accuracy: 0.05007 |  0:17:18s\n",
      "epoch 21 | loss: 4.35235 | train_balanced_accuracy: 0.05152 | valid_balanced_accuracy: 0.05088 |  0:18:08s\n",
      "epoch 22 | loss: 4.27561 | train_balanced_accuracy: 0.04963 | valid_balanced_accuracy: 0.05173 |  0:18:57s\n",
      "epoch 23 | loss: 4.215   | train_balanced_accuracy: 0.0512  | valid_balanced_accuracy: 0.05079 |  0:19:47s\n",
      "epoch 24 | loss: 4.146   | train_balanced_accuracy: 0.05273 | valid_balanced_accuracy: 0.05376 |  0:20:37s\n",
      "epoch 25 | loss: 4.07631 | train_balanced_accuracy: 0.0544  | valid_balanced_accuracy: 0.05625 |  0:21:28s\n",
      "epoch 26 | loss: 4.0078  | train_balanced_accuracy: 0.05162 | valid_balanced_accuracy: 0.05358 |  0:22:20s\n",
      "epoch 27 | loss: 3.95327 | train_balanced_accuracy: 0.05128 | valid_balanced_accuracy: 0.0521  |  0:23:12s\n",
      "epoch 28 | loss: 3.89987 | train_balanced_accuracy: 0.05305 | valid_balanced_accuracy: 0.05422 |  0:24:03s\n",
      "epoch 29 | loss: 3.84017 | train_balanced_accuracy: 0.0552  | valid_balanced_accuracy: 0.05629 |  0:24:54s\n",
      "epoch 30 | loss: 3.7897  | train_balanced_accuracy: 0.0542  | valid_balanced_accuracy: 0.0525  |  0:25:45s\n",
      "epoch 31 | loss: 3.73801 | train_balanced_accuracy: 0.05301 | valid_balanced_accuracy: 0.05456 |  0:26:36s\n",
      "epoch 32 | loss: 3.68932 | train_balanced_accuracy: 0.05464 | valid_balanced_accuracy: 0.05736 |  0:27:28s\n",
      "epoch 33 | loss: 3.64451 | train_balanced_accuracy: 0.05484 | valid_balanced_accuracy: 0.05687 |  0:28:19s\n",
      "epoch 34 | loss: 3.60049 | train_balanced_accuracy: 0.05405 | valid_balanced_accuracy: 0.05488 |  0:29:11s\n",
      "epoch 35 | loss: 3.56083 | train_balanced_accuracy: 0.05574 | valid_balanced_accuracy: 0.05912 |  0:30:02s\n",
      "epoch 36 | loss: 3.51269 | train_balanced_accuracy: 0.05644 | valid_balanced_accuracy: 0.05803 |  0:30:53s\n",
      "epoch 37 | loss: 3.48022 | train_balanced_accuracy: 0.05584 | valid_balanced_accuracy: 0.0593  |  0:31:45s\n",
      "epoch 38 | loss: 3.44108 | train_balanced_accuracy: 0.05504 | valid_balanced_accuracy: 0.05567 |  0:32:36s\n",
      "epoch 39 | loss: 3.39941 | train_balanced_accuracy: 0.05561 | valid_balanced_accuracy: 0.05831 |  0:33:28s\n",
      "epoch 40 | loss: 3.36256 | train_balanced_accuracy: 0.05743 | valid_balanced_accuracy: 0.05921 |  0:34:19s\n",
      "epoch 41 | loss: 3.33323 | train_balanced_accuracy: 0.05658 | valid_balanced_accuracy: 0.05791 |  0:35:10s\n",
      "epoch 42 | loss: 3.29987 | train_balanced_accuracy: 0.06027 | valid_balanced_accuracy: 0.06179 |  0:36:02s\n",
      "epoch 43 | loss: 3.26434 | train_balanced_accuracy: 0.0598  | valid_balanced_accuracy: 0.06005 |  0:36:53s\n",
      "epoch 44 | loss: 3.23063 | train_balanced_accuracy: 0.05958 | valid_balanced_accuracy: 0.06235 |  0:37:45s\n",
      "epoch 45 | loss: 3.20303 | train_balanced_accuracy: 0.0597  | valid_balanced_accuracy: 0.0623  |  0:38:36s\n",
      "epoch 46 | loss: 3.17422 | train_balanced_accuracy: 0.05873 | valid_balanced_accuracy: 0.06048 |  0:39:28s\n",
      "epoch 47 | loss: 3.14989 | train_balanced_accuracy: 0.0606  | valid_balanced_accuracy: 0.06114 |  0:40:19s\n",
      "epoch 48 | loss: 3.12016 | train_balanced_accuracy: 0.06001 | valid_balanced_accuracy: 0.06054 |  0:41:11s\n",
      "epoch 49 | loss: 3.0959  | train_balanced_accuracy: 0.06183 | valid_balanced_accuracy: 0.06444 |  0:42:02s\n",
      "epoch 50 | loss: 3.0779  | train_balanced_accuracy: 0.06035 | valid_balanced_accuracy: 0.06291 |  0:42:54s\n",
      "epoch 51 | loss: 3.05442 | train_balanced_accuracy: 0.06014 | valid_balanced_accuracy: 0.06374 |  0:43:46s\n",
      "epoch 52 | loss: 3.03099 | train_balanced_accuracy: 0.06299 | valid_balanced_accuracy: 0.06505 |  0:44:37s\n",
      "epoch 53 | loss: 3.00202 | train_balanced_accuracy: 0.06191 | valid_balanced_accuracy: 0.06293 |  0:45:29s\n",
      "epoch 54 | loss: 2.99283 | train_balanced_accuracy: 0.06178 | valid_balanced_accuracy: 0.06459 |  0:46:20s\n",
      "epoch 55 | loss: 2.9641  | train_balanced_accuracy: 0.06175 | valid_balanced_accuracy: 0.06308 |  0:47:12s\n",
      "epoch 56 | loss: 2.94972 | train_balanced_accuracy: 0.06332 | valid_balanced_accuracy: 0.0633  |  0:48:03s\n",
      "epoch 57 | loss: 2.92584 | train_balanced_accuracy: 0.06071 | valid_balanced_accuracy: 0.06191 |  0:48:55s\n",
      "epoch 58 | loss: 2.90652 | train_balanced_accuracy: 0.0617  | valid_balanced_accuracy: 0.06354 |  0:49:46s\n",
      "epoch 59 | loss: 2.88967 | train_balanced_accuracy: 0.06376 | valid_balanced_accuracy: 0.06491 |  0:50:37s\n",
      "epoch 60 | loss: 2.87209 | train_balanced_accuracy: 0.06439 | valid_balanced_accuracy: 0.06465 |  0:51:29s\n",
      "epoch 61 | loss: 2.85521 | train_balanced_accuracy: 0.06247 | valid_balanced_accuracy: 0.06408 |  0:52:20s\n",
      "epoch 62 | loss: 2.84475 | train_balanced_accuracy: 0.06145 | valid_balanced_accuracy: 0.06321 |  0:53:10s\n",
      "\n",
      "Early stopping occured at epoch 62 with best_epoch = 52 and best_valid_balanced_accuracy = 0.06505\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'gamma': 1.8}, optimizer_params={'lr': 1e-05}, n_steps=8, n_shared=4, n_independent=1, n_a=16, momentum=0.05, lambda_sparse=0.01, clip_value=0.01, total=53.4min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'gamma': 1.8}, optimizer_params={'lr': 1e-05}, n_steps=8, n_shared=4, n_independent=1, n_a=16, momentum=0.05, lambda_sparse=0.01, clip_value=0.01 \n",
      "epoch 0  | loss: 6.59365 | train_balanced_accuracy: 0.03626 | valid_balanced_accuracy: 0.03965 |  0:00:50s\n",
      "epoch 1  | loss: 6.44381 | train_balanced_accuracy: 0.03546 | valid_balanced_accuracy: 0.03549 |  0:01:40s\n",
      "epoch 2  | loss: 6.30945 | train_balanced_accuracy: 0.03633 | valid_balanced_accuracy: 0.03809 |  0:02:30s\n",
      "epoch 3  | loss: 6.181   | train_balanced_accuracy: 0.03655 | valid_balanced_accuracy: 0.04218 |  0:03:21s\n",
      "epoch 4  | loss: 6.04342 | train_balanced_accuracy: 0.03863 | valid_balanced_accuracy: 0.0429  |  0:04:11s\n",
      "epoch 5  | loss: 5.93244 | train_balanced_accuracy: 0.03805 | valid_balanced_accuracy: 0.04192 |  0:05:00s\n",
      "epoch 6  | loss: 5.79654 | train_balanced_accuracy: 0.03788 | valid_balanced_accuracy: 0.03978 |  0:05:50s\n",
      "epoch 7  | loss: 5.68332 | train_balanced_accuracy: 0.04114 | valid_balanced_accuracy: 0.04354 |  0:06:41s\n",
      "epoch 8  | loss: 5.57427 | train_balanced_accuracy: 0.0412  | valid_balanced_accuracy: 0.04019 |  0:07:32s\n",
      "epoch 9  | loss: 5.46195 | train_balanced_accuracy: 0.04218 | valid_balanced_accuracy: 0.04372 |  0:08:22s\n",
      "epoch 10 | loss: 5.35046 | train_balanced_accuracy: 0.04224 | valid_balanced_accuracy: 0.04395 |  0:09:13s\n",
      "epoch 11 | loss: 5.24339 | train_balanced_accuracy: 0.04353 | valid_balanced_accuracy: 0.04481 |  0:10:04s\n",
      "epoch 12 | loss: 5.14575 | train_balanced_accuracy: 0.046   | valid_balanced_accuracy: 0.04651 |  0:10:54s\n",
      "epoch 13 | loss: 5.0506  | train_balanced_accuracy: 0.04677 | valid_balanced_accuracy: 0.04618 |  0:11:45s\n",
      "epoch 14 | loss: 4.95317 | train_balanced_accuracy: 0.04574 | valid_balanced_accuracy: 0.04587 |  0:12:35s\n",
      "epoch 15 | loss: 4.86878 | train_balanced_accuracy: 0.04678 | valid_balanced_accuracy: 0.04621 |  0:13:26s\n",
      "epoch 16 | loss: 4.75647 | train_balanced_accuracy: 0.04811 | valid_balanced_accuracy: 0.04751 |  0:14:16s\n",
      "epoch 17 | loss: 4.67289 | train_balanced_accuracy: 0.04714 | valid_balanced_accuracy: 0.0474  |  0:15:07s\n",
      "epoch 18 | loss: 4.58979 | train_balanced_accuracy: 0.04992 | valid_balanced_accuracy: 0.0498  |  0:15:58s\n",
      "epoch 19 | loss: 4.50646 | train_balanced_accuracy: 0.05106 | valid_balanced_accuracy: 0.05031 |  0:16:48s\n",
      "epoch 20 | loss: 4.43145 | train_balanced_accuracy: 0.0484  | valid_balanced_accuracy: 0.0489  |  0:17:39s\n",
      "epoch 21 | loss: 4.35914 | train_balanced_accuracy: 0.05064 | valid_balanced_accuracy: 0.05018 |  0:18:30s\n",
      "epoch 22 | loss: 4.28454 | train_balanced_accuracy: 0.0507  | valid_balanced_accuracy: 0.05022 |  0:19:20s\n",
      "epoch 23 | loss: 4.21025 | train_balanced_accuracy: 0.05193 | valid_balanced_accuracy: 0.05039 |  0:20:09s\n",
      "epoch 24 | loss: 4.13928 | train_balanced_accuracy: 0.05326 | valid_balanced_accuracy: 0.05221 |  0:20:59s\n",
      "epoch 25 | loss: 4.08004 | train_balanced_accuracy: 0.05225 | valid_balanced_accuracy: 0.05281 |  0:21:49s\n",
      "epoch 26 | loss: 4.01154 | train_balanced_accuracy: 0.05203 | valid_balanced_accuracy: 0.05274 |  0:22:39s\n",
      "epoch 27 | loss: 3.95316 | train_balanced_accuracy: 0.05402 | valid_balanced_accuracy: 0.05609 |  0:23:28s\n",
      "epoch 28 | loss: 3.89429 | train_balanced_accuracy: 0.05371 | valid_balanced_accuracy: 0.05429 |  0:24:18s\n",
      "epoch 29 | loss: 3.84625 | train_balanced_accuracy: 0.05487 | valid_balanced_accuracy: 0.05443 |  0:25:07s\n",
      "epoch 30 | loss: 3.79592 | train_balanced_accuracy: 0.05381 | valid_balanced_accuracy: 0.05497 |  0:25:57s\n",
      "epoch 31 | loss: 3.74045 | train_balanced_accuracy: 0.05365 | valid_balanced_accuracy: 0.05493 |  0:26:46s\n",
      "epoch 32 | loss: 3.69705 | train_balanced_accuracy: 0.05396 | valid_balanced_accuracy: 0.0532  |  0:27:36s\n",
      "epoch 33 | loss: 3.64104 | train_balanced_accuracy: 0.05325 | valid_balanced_accuracy: 0.0543  |  0:28:26s\n",
      "epoch 34 | loss: 3.59319 | train_balanced_accuracy: 0.05774 | valid_balanced_accuracy: 0.05706 |  0:29:16s\n",
      "epoch 35 | loss: 3.54363 | train_balanced_accuracy: 0.05619 | valid_balanced_accuracy: 0.05601 |  0:30:06s\n",
      "epoch 36 | loss: 3.5026  | train_balanced_accuracy: 0.05739 | valid_balanced_accuracy: 0.06002 |  0:30:57s\n",
      "epoch 37 | loss: 3.46204 | train_balanced_accuracy: 0.05522 | valid_balanced_accuracy: 0.05608 |  0:31:48s\n",
      "epoch 38 | loss: 3.42376 | train_balanced_accuracy: 0.05773 | valid_balanced_accuracy: 0.05759 |  0:32:39s\n",
      "epoch 39 | loss: 3.39107 | train_balanced_accuracy: 0.05809 | valid_balanced_accuracy: 0.05965 |  0:33:31s\n",
      "epoch 40 | loss: 3.3508  | train_balanced_accuracy: 0.05818 | valid_balanced_accuracy: 0.05902 |  0:34:22s\n",
      "epoch 41 | loss: 3.32019 | train_balanced_accuracy: 0.05853 | valid_balanced_accuracy: 0.05936 |  0:35:14s\n",
      "epoch 42 | loss: 3.291   | train_balanced_accuracy: 0.05995 | valid_balanced_accuracy: 0.06012 |  0:36:05s\n",
      "epoch 43 | loss: 3.25578 | train_balanced_accuracy: 0.05939 | valid_balanced_accuracy: 0.05933 |  0:36:57s\n",
      "epoch 44 | loss: 3.22869 | train_balanced_accuracy: 0.0592  | valid_balanced_accuracy: 0.0604  |  0:37:48s\n",
      "epoch 45 | loss: 3.19952 | train_balanced_accuracy: 0.05969 | valid_balanced_accuracy: 0.06039 |  0:38:40s\n",
      "epoch 46 | loss: 3.17084 | train_balanced_accuracy: 0.05855 | valid_balanced_accuracy: 0.05884 |  0:39:30s\n",
      "epoch 47 | loss: 3.14721 | train_balanced_accuracy: 0.058   | valid_balanced_accuracy: 0.06035 |  0:40:20s\n",
      "epoch 48 | loss: 3.1211  | train_balanced_accuracy: 0.06145 | valid_balanced_accuracy: 0.06228 |  0:41:10s\n",
      "epoch 49 | loss: 3.09178 | train_balanced_accuracy: 0.06102 | valid_balanced_accuracy: 0.06201 |  0:42:00s\n",
      "epoch 50 | loss: 3.07176 | train_balanced_accuracy: 0.06351 | valid_balanced_accuracy: 0.06391 |  0:42:49s\n",
      "epoch 51 | loss: 3.04954 | train_balanced_accuracy: 0.06242 | valid_balanced_accuracy: 0.0631  |  0:43:39s\n",
      "epoch 52 | loss: 3.02835 | train_balanced_accuracy: 0.06289 | valid_balanced_accuracy: 0.06327 |  0:44:29s\n",
      "epoch 53 | loss: 2.9984  | train_balanced_accuracy: 0.06447 | valid_balanced_accuracy: 0.06402 |  0:45:19s\n",
      "epoch 54 | loss: 2.98876 | train_balanced_accuracy: 0.06147 | valid_balanced_accuracy: 0.06248 |  0:46:09s\n",
      "epoch 55 | loss: 2.95956 | train_balanced_accuracy: 0.06165 | valid_balanced_accuracy: 0.06218 |  0:46:58s\n",
      "epoch 56 | loss: 2.93766 | train_balanced_accuracy: 0.061   | valid_balanced_accuracy: 0.06151 |  0:47:48s\n",
      "epoch 57 | loss: 2.91967 | train_balanced_accuracy: 0.06329 | valid_balanced_accuracy: 0.06408 |  0:48:37s\n",
      "epoch 58 | loss: 2.90797 | train_balanced_accuracy: 0.06219 | valid_balanced_accuracy: 0.06247 |  0:49:27s\n",
      "epoch 59 | loss: 2.8844  | train_balanced_accuracy: 0.06451 | valid_balanced_accuracy: 0.06573 |  0:50:17s\n",
      "epoch 60 | loss: 2.86356 | train_balanced_accuracy: 0.06415 | valid_balanced_accuracy: 0.06446 |  0:51:07s\n",
      "epoch 61 | loss: 2.84168 | train_balanced_accuracy: 0.06361 | valid_balanced_accuracy: 0.06434 |  0:51:57s\n",
      "epoch 62 | loss: 2.83215 | train_balanced_accuracy: 0.06274 | valid_balanced_accuracy: 0.06306 |  0:52:47s\n",
      "epoch 63 | loss: 2.81278 | train_balanced_accuracy: 0.06425 | valid_balanced_accuracy: 0.06434 |  0:53:36s\n",
      "epoch 64 | loss: 2.78778 | train_balanced_accuracy: 0.06635 | valid_balanced_accuracy: 0.06657 |  0:54:26s\n",
      "epoch 65 | loss: 2.7803  | train_balanced_accuracy: 0.06387 | valid_balanced_accuracy: 0.06328 |  0:55:16s\n",
      "epoch 66 | loss: 2.76465 | train_balanced_accuracy: 0.06664 | valid_balanced_accuracy: 0.06704 |  0:56:05s\n",
      "epoch 67 | loss: 2.75008 | train_balanced_accuracy: 0.06744 | valid_balanced_accuracy: 0.06715 |  0:56:55s\n",
      "epoch 68 | loss: 2.73134 | train_balanced_accuracy: 0.06795 | valid_balanced_accuracy: 0.06712 |  0:57:45s\n",
      "epoch 69 | loss: 2.71934 | train_balanced_accuracy: 0.06579 | valid_balanced_accuracy: 0.06622 |  0:58:36s\n",
      "epoch 70 | loss: 2.71489 | train_balanced_accuracy: 0.06783 | valid_balanced_accuracy: 0.06704 |  0:59:27s\n",
      "epoch 71 | loss: 2.69333 | train_balanced_accuracy: 0.06649 | valid_balanced_accuracy: 0.06756 |  1:00:18s\n",
      "epoch 72 | loss: 2.68106 | train_balanced_accuracy: 0.06972 | valid_balanced_accuracy: 0.07019 |  1:01:09s\n",
      "epoch 73 | loss: 2.66641 | train_balanced_accuracy: 0.06953 | valid_balanced_accuracy: 0.06916 |  1:02:01s\n",
      "epoch 74 | loss: 2.65202 | train_balanced_accuracy: 0.07114 | valid_balanced_accuracy: 0.0702  |  1:02:52s\n",
      "epoch 75 | loss: 2.64077 | train_balanced_accuracy: 0.06804 | valid_balanced_accuracy: 0.06703 |  1:03:44s\n",
      "epoch 76 | loss: 2.63007 | train_balanced_accuracy: 0.07077 | valid_balanced_accuracy: 0.07135 |  1:04:36s\n",
      "epoch 77 | loss: 2.61644 | train_balanced_accuracy: 0.07122 | valid_balanced_accuracy: 0.07152 |  1:05:27s\n",
      "epoch 78 | loss: 2.60791 | train_balanced_accuracy: 0.07106 | valid_balanced_accuracy: 0.07029 |  1:06:19s\n",
      "epoch 79 | loss: 2.59846 | train_balanced_accuracy: 0.06912 | valid_balanced_accuracy: 0.06953 |  1:07:10s\n",
      "epoch 80 | loss: 2.58598 | train_balanced_accuracy: 0.06822 | valid_balanced_accuracy: 0.06865 |  1:08:02s\n",
      "epoch 81 | loss: 2.57596 | train_balanced_accuracy: 0.07269 | valid_balanced_accuracy: 0.07188 |  1:08:53s\n",
      "epoch 82 | loss: 2.56319 | train_balanced_accuracy: 0.07196 | valid_balanced_accuracy: 0.07117 |  1:09:45s\n",
      "epoch 83 | loss: 2.54894 | train_balanced_accuracy: 0.07278 | valid_balanced_accuracy: 0.07208 |  1:10:36s\n",
      "epoch 84 | loss: 2.541   | train_balanced_accuracy: 0.07314 | valid_balanced_accuracy: 0.07273 |  1:11:27s\n",
      "epoch 85 | loss: 2.52766 | train_balanced_accuracy: 0.07445 | valid_balanced_accuracy: 0.07417 |  1:12:18s\n",
      "epoch 86 | loss: 2.52309 | train_balanced_accuracy: 0.07251 | valid_balanced_accuracy: 0.07237 |  1:13:10s\n",
      "epoch 87 | loss: 2.51334 | train_balanced_accuracy: 0.07331 | valid_balanced_accuracy: 0.07401 |  1:14:00s\n",
      "epoch 88 | loss: 2.50298 | train_balanced_accuracy: 0.07513 | valid_balanced_accuracy: 0.07539 |  1:14:51s\n",
      "epoch 89 | loss: 2.49451 | train_balanced_accuracy: 0.07652 | valid_balanced_accuracy: 0.07744 |  1:15:42s\n",
      "epoch 90 | loss: 2.488   | train_balanced_accuracy: 0.07621 | valid_balanced_accuracy: 0.07576 |  1:16:32s\n",
      "epoch 91 | loss: 2.47284 | train_balanced_accuracy: 0.07656 | valid_balanced_accuracy: 0.0762  |  1:17:21s\n",
      "epoch 92 | loss: 2.4689  | train_balanced_accuracy: 0.07772 | valid_balanced_accuracy: 0.07712 |  1:18:11s\n",
      "epoch 93 | loss: 2.46441 | train_balanced_accuracy: 0.08055 | valid_balanced_accuracy: 0.07976 |  1:19:01s\n",
      "epoch 94 | loss: 2.44999 | train_balanced_accuracy: 0.07936 | valid_balanced_accuracy: 0.07858 |  1:19:50s\n",
      "epoch 95 | loss: 2.44532 | train_balanced_accuracy: 0.07867 | valid_balanced_accuracy: 0.07877 |  1:20:40s\n",
      "epoch 96 | loss: 2.43568 | train_balanced_accuracy: 0.0806  | valid_balanced_accuracy: 0.08049 |  1:21:29s\n",
      "epoch 97 | loss: 2.43029 | train_balanced_accuracy: 0.0809  | valid_balanced_accuracy: 0.08222 |  1:22:18s\n",
      "epoch 98 | loss: 2.42095 | train_balanced_accuracy: 0.08334 | valid_balanced_accuracy: 0.08288 |  1:23:07s\n",
      "epoch 99 | loss: 2.41058 | train_balanced_accuracy: 0.0835  | valid_balanced_accuracy: 0.08323 |  1:23:56s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_valid_balanced_accuracy = 0.08323\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'gamma': 1.8}, optimizer_params={'lr': 1e-05}, n_steps=8, n_shared=4, n_independent=1, n_a=16, momentum=0.05, lambda_sparse=0.01, clip_value=0.01, total=84.2min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'gamma': 1.8}, optimizer_params={'lr': 1e-05}, n_steps=8, n_shared=4, n_independent=1, n_a=16, momentum=0.05, lambda_sparse=0.01, clip_value=0.01 \n",
      "epoch 0  | loss: 6.60359 | train_balanced_accuracy: 0.03606 | valid_balanced_accuracy: 0.03653 |  0:00:50s\n",
      "epoch 1  | loss: 6.46649 | train_balanced_accuracy: 0.03818 | valid_balanced_accuracy: 0.04145 |  0:01:41s\n",
      "epoch 2  | loss: 6.34239 | train_balanced_accuracy: 0.03495 | valid_balanced_accuracy: 0.03766 |  0:02:32s\n",
      "epoch 3  | loss: 6.21865 | train_balanced_accuracy: 0.03618 | valid_balanced_accuracy: 0.0364  |  0:03:23s\n",
      "epoch 4  | loss: 6.08925 | train_balanced_accuracy: 0.03784 | valid_balanced_accuracy: 0.04089 |  0:04:15s\n",
      "epoch 5  | loss: 5.97325 | train_balanced_accuracy: 0.03731 | valid_balanced_accuracy: 0.03805 |  0:05:06s\n",
      "epoch 6  | loss: 5.85333 | train_balanced_accuracy: 0.03861 | valid_balanced_accuracy: 0.03897 |  0:05:57s\n",
      "epoch 7  | loss: 5.74302 | train_balanced_accuracy: 0.04114 | valid_balanced_accuracy: 0.04169 |  0:06:48s\n",
      "epoch 8  | loss: 5.62446 | train_balanced_accuracy: 0.04045 | valid_balanced_accuracy: 0.04145 |  0:07:39s\n",
      "epoch 9  | loss: 5.5188  | train_balanced_accuracy: 0.04254 | valid_balanced_accuracy: 0.04296 |  0:08:31s\n",
      "epoch 10 | loss: 5.41674 | train_balanced_accuracy: 0.04168 | valid_balanced_accuracy: 0.04147 |  0:09:22s\n",
      "epoch 11 | loss: 5.3123  | train_balanced_accuracy: 0.04354 | valid_balanced_accuracy: 0.04465 |  0:10:13s\n",
      "epoch 12 | loss: 5.18849 | train_balanced_accuracy: 0.04452 | valid_balanced_accuracy: 0.0453  |  0:11:04s\n",
      "epoch 13 | loss: 5.09745 | train_balanced_accuracy: 0.04428 | valid_balanced_accuracy: 0.0447  |  0:11:55s\n",
      "epoch 14 | loss: 5.01067 | train_balanced_accuracy: 0.04575 | valid_balanced_accuracy: 0.04696 |  0:12:45s\n",
      "epoch 15 | loss: 4.89917 | train_balanced_accuracy: 0.04787 | valid_balanced_accuracy: 0.04713 |  0:13:35s\n",
      "epoch 16 | loss: 4.81257 | train_balanced_accuracy: 0.04539 | valid_balanced_accuracy: 0.0457  |  0:14:25s\n",
      "epoch 17 | loss: 4.7352  | train_balanced_accuracy: 0.04906 | valid_balanced_accuracy: 0.04738 |  0:15:15s\n",
      "epoch 18 | loss: 4.64808 | train_balanced_accuracy: 0.04707 | valid_balanced_accuracy: 0.04799 |  0:16:05s\n",
      "epoch 19 | loss: 4.56443 | train_balanced_accuracy: 0.04929 | valid_balanced_accuracy: 0.04901 |  0:16:55s\n",
      "epoch 20 | loss: 4.48828 | train_balanced_accuracy: 0.0468  | valid_balanced_accuracy: 0.04599 |  0:17:44s\n",
      "epoch 21 | loss: 4.40984 | train_balanced_accuracy: 0.04931 | valid_balanced_accuracy: 0.04814 |  0:18:34s\n",
      "epoch 22 | loss: 4.3278  | train_balanced_accuracy: 0.05244 | valid_balanced_accuracy: 0.05229 |  0:19:23s\n",
      "epoch 23 | loss: 4.26511 | train_balanced_accuracy: 0.04992 | valid_balanced_accuracy: 0.05067 |  0:20:12s\n",
      "epoch 24 | loss: 4.18029 | train_balanced_accuracy: 0.05261 | valid_balanced_accuracy: 0.05279 |  0:21:02s\n",
      "epoch 25 | loss: 4.12476 | train_balanced_accuracy: 0.05116 | valid_balanced_accuracy: 0.05153 |  0:21:51s\n",
      "epoch 26 | loss: 4.05914 | train_balanced_accuracy: 0.05215 | valid_balanced_accuracy: 0.0554  |  0:22:41s\n",
      "epoch 27 | loss: 3.99341 | train_balanced_accuracy: 0.05455 | valid_balanced_accuracy: 0.05353 |  0:23:30s\n",
      "epoch 28 | loss: 3.9329  | train_balanced_accuracy: 0.0538  | valid_balanced_accuracy: 0.05442 |  0:24:19s\n",
      "epoch 29 | loss: 3.88427 | train_balanced_accuracy: 0.05281 | valid_balanced_accuracy: 0.05482 |  0:25:09s\n",
      "epoch 30 | loss: 3.83389 | train_balanced_accuracy: 0.0547  | valid_balanced_accuracy: 0.05597 |  0:25:58s\n",
      "epoch 31 | loss: 3.77279 | train_balanced_accuracy: 0.05258 | valid_balanced_accuracy: 0.05579 |  0:26:48s\n",
      "epoch 32 | loss: 3.72303 | train_balanced_accuracy: 0.05568 | valid_balanced_accuracy: 0.0585  |  0:27:37s\n",
      "epoch 33 | loss: 3.67244 | train_balanced_accuracy: 0.05339 | valid_balanced_accuracy: 0.05585 |  0:28:27s\n",
      "epoch 34 | loss: 3.6181  | train_balanced_accuracy: 0.05619 | valid_balanced_accuracy: 0.05787 |  0:29:16s\n",
      "epoch 35 | loss: 3.57354 | train_balanced_accuracy: 0.05643 | valid_balanced_accuracy: 0.05878 |  0:30:07s\n",
      "epoch 36 | loss: 3.52727 | train_balanced_accuracy: 0.05682 | valid_balanced_accuracy: 0.05935 |  0:30:57s\n",
      "epoch 37 | loss: 3.489   | train_balanced_accuracy: 0.05787 | valid_balanced_accuracy: 0.06013 |  0:31:46s\n",
      "epoch 38 | loss: 3.45159 | train_balanced_accuracy: 0.05748 | valid_balanced_accuracy: 0.06136 |  0:32:36s\n",
      "epoch 39 | loss: 3.42389 | train_balanced_accuracy: 0.05688 | valid_balanced_accuracy: 0.05986 |  0:33:26s\n",
      "epoch 40 | loss: 3.37832 | train_balanced_accuracy: 0.05706 | valid_balanced_accuracy: 0.05873 |  0:34:16s\n",
      "epoch 41 | loss: 3.34607 | train_balanced_accuracy: 0.05836 | valid_balanced_accuracy: 0.05978 |  0:35:06s\n",
      "epoch 42 | loss: 3.31102 | train_balanced_accuracy: 0.05916 | valid_balanced_accuracy: 0.06068 |  0:35:57s\n",
      "epoch 43 | loss: 3.27979 | train_balanced_accuracy: 0.05896 | valid_balanced_accuracy: 0.0626  |  0:36:48s\n",
      "epoch 44 | loss: 3.25254 | train_balanced_accuracy: 0.05864 | valid_balanced_accuracy: 0.06036 |  0:37:38s\n",
      "epoch 45 | loss: 3.22593 | train_balanced_accuracy: 0.06042 | valid_balanced_accuracy: 0.06052 |  0:38:29s\n",
      "epoch 46 | loss: 3.19522 | train_balanced_accuracy: 0.05897 | valid_balanced_accuracy: 0.06065 |  0:39:19s\n",
      "epoch 47 | loss: 3.1678  | train_balanced_accuracy: 0.06026 | valid_balanced_accuracy: 0.06404 |  0:40:10s\n",
      "epoch 48 | loss: 3.14033 | train_balanced_accuracy: 0.06119 | valid_balanced_accuracy: 0.06284 |  0:41:01s\n",
      "epoch 49 | loss: 3.11544 | train_balanced_accuracy: 0.06123 | valid_balanced_accuracy: 0.06185 |  0:41:51s\n",
      "epoch 50 | loss: 3.092   | train_balanced_accuracy: 0.05989 | valid_balanced_accuracy: 0.06018 |  0:42:42s\n",
      "epoch 51 | loss: 3.06547 | train_balanced_accuracy: 0.06161 | valid_balanced_accuracy: 0.06212 |  0:43:32s\n",
      "epoch 52 | loss: 3.05088 | train_balanced_accuracy: 0.06056 | valid_balanced_accuracy: 0.06141 |  0:44:23s\n",
      "epoch 53 | loss: 3.02661 | train_balanced_accuracy: 0.06182 | valid_balanced_accuracy: 0.06523 |  0:45:13s\n",
      "epoch 54 | loss: 3.00455 | train_balanced_accuracy: 0.06319 | valid_balanced_accuracy: 0.06472 |  0:46:04s\n",
      "epoch 55 | loss: 2.98434 | train_balanced_accuracy: 0.06342 | valid_balanced_accuracy: 0.06508 |  0:46:54s\n",
      "epoch 56 | loss: 2.96218 | train_balanced_accuracy: 0.06415 | valid_balanced_accuracy: 0.06591 |  0:47:44s\n",
      "epoch 57 | loss: 2.94224 | train_balanced_accuracy: 0.06427 | valid_balanced_accuracy: 0.065   |  0:48:35s\n",
      "epoch 58 | loss: 2.92266 | train_balanced_accuracy: 0.06339 | valid_balanced_accuracy: 0.06484 |  0:49:25s\n",
      "epoch 59 | loss: 2.89944 | train_balanced_accuracy: 0.06487 | valid_balanced_accuracy: 0.06534 |  0:50:15s\n",
      "epoch 60 | loss: 2.87996 | train_balanced_accuracy: 0.06481 | valid_balanced_accuracy: 0.06655 |  0:51:04s\n",
      "epoch 61 | loss: 2.86247 | train_balanced_accuracy: 0.06364 | valid_balanced_accuracy: 0.06524 |  0:51:54s\n",
      "epoch 62 | loss: 2.84937 | train_balanced_accuracy: 0.06495 | valid_balanced_accuracy: 0.0649  |  0:52:43s\n",
      "epoch 63 | loss: 2.8288  | train_balanced_accuracy: 0.06575 | valid_balanced_accuracy: 0.06619 |  0:53:34s\n",
      "epoch 64 | loss: 2.81037 | train_balanced_accuracy: 0.06508 | valid_balanced_accuracy: 0.06554 |  0:54:25s\n",
      "epoch 65 | loss: 2.79825 | train_balanced_accuracy: 0.06597 | valid_balanced_accuracy: 0.06585 |  0:55:17s\n",
      "epoch 66 | loss: 2.77893 | train_balanced_accuracy: 0.06627 | valid_balanced_accuracy: 0.0672  |  0:56:08s\n",
      "epoch 67 | loss: 2.77011 | train_balanced_accuracy: 0.06482 | valid_balanced_accuracy: 0.0639  |  0:57:00s\n",
      "epoch 68 | loss: 2.75101 | train_balanced_accuracy: 0.06781 | valid_balanced_accuracy: 0.06752 |  0:57:51s\n",
      "epoch 69 | loss: 2.73517 | train_balanced_accuracy: 0.06603 | valid_balanced_accuracy: 0.06608 |  0:58:43s\n",
      "epoch 70 | loss: 2.71735 | train_balanced_accuracy: 0.06753 | valid_balanced_accuracy: 0.06728 |  0:59:34s\n",
      "epoch 71 | loss: 2.70583 | train_balanced_accuracy: 0.06677 | valid_balanced_accuracy: 0.06819 |  1:00:26s\n",
      "epoch 72 | loss: 2.69    | train_balanced_accuracy: 0.06725 | valid_balanced_accuracy: 0.06676 |  1:01:15s\n",
      "epoch 73 | loss: 2.67893 | train_balanced_accuracy: 0.0705  | valid_balanced_accuracy: 0.06964 |  1:02:05s\n",
      "epoch 74 | loss: 2.66314 | train_balanced_accuracy: 0.06812 | valid_balanced_accuracy: 0.06797 |  1:02:55s\n",
      "epoch 75 | loss: 2.65338 | train_balanced_accuracy: 0.06878 | valid_balanced_accuracy: 0.06804 |  1:03:45s\n",
      "epoch 76 | loss: 2.63941 | train_balanced_accuracy: 0.06867 | valid_balanced_accuracy: 0.07032 |  1:04:34s\n",
      "epoch 77 | loss: 2.62776 | train_balanced_accuracy: 0.07161 | valid_balanced_accuracy: 0.07058 |  1:05:24s\n",
      "epoch 78 | loss: 2.61368 | train_balanced_accuracy: 0.07019 | valid_balanced_accuracy: 0.07095 |  1:06:13s\n",
      "epoch 79 | loss: 2.60205 | train_balanced_accuracy: 0.06904 | valid_balanced_accuracy: 0.06892 |  1:07:02s\n",
      "epoch 80 | loss: 2.59228 | train_balanced_accuracy: 0.07015 | valid_balanced_accuracy: 0.07144 |  1:07:52s\n",
      "epoch 81 | loss: 2.58551 | train_balanced_accuracy: 0.07126 | valid_balanced_accuracy: 0.07087 |  1:08:42s\n",
      "epoch 82 | loss: 2.57283 | train_balanced_accuracy: 0.07116 | valid_balanced_accuracy: 0.07031 |  1:09:31s\n",
      "epoch 83 | loss: 2.56469 | train_balanced_accuracy: 0.07279 | valid_balanced_accuracy: 0.07289 |  1:10:21s\n",
      "epoch 84 | loss: 2.55454 | train_balanced_accuracy: 0.07249 | valid_balanced_accuracy: 0.07226 |  1:11:10s\n",
      "epoch 85 | loss: 2.53909 | train_balanced_accuracy: 0.07359 | valid_balanced_accuracy: 0.07334 |  1:12:00s\n",
      "epoch 86 | loss: 2.53047 | train_balanced_accuracy: 0.07216 | valid_balanced_accuracy: 0.07183 |  1:12:50s\n",
      "epoch 87 | loss: 2.52396 | train_balanced_accuracy: 0.07648 | valid_balanced_accuracy: 0.0766  |  1:13:40s\n",
      "epoch 88 | loss: 2.5167  | train_balanced_accuracy: 0.07562 | valid_balanced_accuracy: 0.07735 |  1:14:30s\n",
      "epoch 89 | loss: 2.50566 | train_balanced_accuracy: 0.07654 | valid_balanced_accuracy: 0.07661 |  1:15:20s\n",
      "epoch 90 | loss: 2.49829 | train_balanced_accuracy: 0.0754  | valid_balanced_accuracy: 0.07487 |  1:16:10s\n",
      "epoch 91 | loss: 2.48626 | train_balanced_accuracy: 0.07461 | valid_balanced_accuracy: 0.075   |  1:17:00s\n",
      "epoch 92 | loss: 2.48168 | train_balanced_accuracy: 0.07684 | valid_balanced_accuracy: 0.07648 |  1:17:50s\n",
      "epoch 93 | loss: 2.47365 | train_balanced_accuracy: 0.07923 | valid_balanced_accuracy: 0.0782  |  1:18:40s\n",
      "epoch 94 | loss: 2.46618 | train_balanced_accuracy: 0.07923 | valid_balanced_accuracy: 0.08016 |  1:19:30s\n",
      "epoch 95 | loss: 2.45628 | train_balanced_accuracy: 0.0792  | valid_balanced_accuracy: 0.07915 |  1:20:19s\n",
      "epoch 96 | loss: 2.44664 | train_balanced_accuracy: 0.07941 | valid_balanced_accuracy: 0.07911 |  1:21:09s\n",
      "epoch 97 | loss: 2.44392 | train_balanced_accuracy: 0.07968 | valid_balanced_accuracy: 0.08049 |  1:21:59s\n",
      "epoch 98 | loss: 2.43339 | train_balanced_accuracy: 0.08074 | valid_balanced_accuracy: 0.08104 |  1:22:49s\n",
      "epoch 99 | loss: 2.42765 | train_balanced_accuracy: 0.08084 | valid_balanced_accuracy: 0.08172 |  1:23:39s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_valid_balanced_accuracy = 0.08172\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'gamma': 1.8}, optimizer_params={'lr': 1e-05}, n_steps=8, n_shared=4, n_independent=1, n_a=16, momentum=0.05, lambda_sparse=0.01, clip_value=0.01, total=83.9min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'gamma': 1.8}, optimizer_params={'lr': 1e-05}, n_steps=8, n_shared=4, n_independent=1, n_a=16, momentum=0.05, lambda_sparse=0.01, clip_value=0.01 \n",
      "epoch 0  | loss: 6.59925 | train_balanced_accuracy: 0.03514 | valid_balanced_accuracy: 0.03494 |  0:00:50s\n",
      "epoch 1  | loss: 6.47529 | train_balanced_accuracy: 0.03807 | valid_balanced_accuracy: 0.03915 |  0:01:40s\n",
      "epoch 2  | loss: 6.34036 | train_balanced_accuracy: 0.03631 | valid_balanced_accuracy: 0.03549 |  0:02:30s\n",
      "epoch 3  | loss: 6.20944 | train_balanced_accuracy: 0.03687 | valid_balanced_accuracy: 0.03866 |  0:03:21s\n",
      "epoch 4  | loss: 6.07716 | train_balanced_accuracy: 0.03859 | valid_balanced_accuracy: 0.03816 |  0:04:11s\n",
      "epoch 5  | loss: 5.96276 | train_balanced_accuracy: 0.03844 | valid_balanced_accuracy: 0.0387  |  0:05:02s\n",
      "epoch 6  | loss: 5.8477  | train_balanced_accuracy: 0.03993 | valid_balanced_accuracy: 0.0406  |  0:05:53s\n",
      "epoch 7  | loss: 5.72228 | train_balanced_accuracy: 0.04133 | valid_balanced_accuracy: 0.0417  |  0:06:43s\n",
      "epoch 8  | loss: 5.61543 | train_balanced_accuracy: 0.04284 | valid_balanced_accuracy: 0.04286 |  0:07:34s\n",
      "epoch 9  | loss: 5.49863 | train_balanced_accuracy: 0.04167 | valid_balanced_accuracy: 0.0419  |  0:08:24s\n",
      "epoch 10 | loss: 5.3847  | train_balanced_accuracy: 0.04173 | valid_balanced_accuracy: 0.0406  |  0:09:13s\n",
      "epoch 11 | loss: 5.27787 | train_balanced_accuracy: 0.04158 | valid_balanced_accuracy: 0.04147 |  0:10:03s\n",
      "epoch 12 | loss: 5.17209 | train_balanced_accuracy: 0.03966 | valid_balanced_accuracy: 0.04006 |  0:10:53s\n",
      "epoch 13 | loss: 5.07391 | train_balanced_accuracy: 0.04327 | valid_balanced_accuracy: 0.04285 |  0:11:45s\n",
      "epoch 14 | loss: 4.97858 | train_balanced_accuracy: 0.04574 | valid_balanced_accuracy: 0.04541 |  0:12:37s\n",
      "epoch 15 | loss: 4.87547 | train_balanced_accuracy: 0.04469 | valid_balanced_accuracy: 0.04495 |  0:13:28s\n",
      "epoch 16 | loss: 4.79467 | train_balanced_accuracy: 0.0472  | valid_balanced_accuracy: 0.04825 |  0:14:20s\n",
      "epoch 17 | loss: 4.71778 | train_balanced_accuracy: 0.0445  | valid_balanced_accuracy: 0.04261 |  0:15:11s\n",
      "epoch 18 | loss: 4.62968 | train_balanced_accuracy: 0.04678 | valid_balanced_accuracy: 0.04517 |  0:16:02s\n",
      "epoch 19 | loss: 4.54793 | train_balanced_accuracy: 0.04991 | valid_balanced_accuracy: 0.04951 |  0:16:53s\n",
      "epoch 20 | loss: 4.45829 | train_balanced_accuracy: 0.04766 | valid_balanced_accuracy: 0.04658 |  0:17:42s\n",
      "epoch 21 | loss: 4.38314 | train_balanced_accuracy: 0.04915 | valid_balanced_accuracy: 0.05085 |  0:18:33s\n",
      "epoch 22 | loss: 4.30542 | train_balanced_accuracy: 0.05118 | valid_balanced_accuracy: 0.0523  |  0:19:22s\n",
      "epoch 23 | loss: 4.2367  | train_balanced_accuracy: 0.05117 | valid_balanced_accuracy: 0.04985 |  0:20:12s\n",
      "epoch 24 | loss: 4.17099 | train_balanced_accuracy: 0.05247 | valid_balanced_accuracy: 0.05208 |  0:21:02s\n",
      "epoch 25 | loss: 4.11263 | train_balanced_accuracy: 0.05236 | valid_balanced_accuracy: 0.05197 |  0:21:52s\n",
      "epoch 26 | loss: 4.04537 | train_balanced_accuracy: 0.0515  | valid_balanced_accuracy: 0.05326 |  0:22:43s\n",
      "epoch 27 | loss: 3.98546 | train_balanced_accuracy: 0.05208 | valid_balanced_accuracy: 0.05296 |  0:23:34s\n",
      "epoch 28 | loss: 3.92908 | train_balanced_accuracy: 0.05279 | valid_balanced_accuracy: 0.05185 |  0:24:26s\n",
      "epoch 29 | loss: 3.87253 | train_balanced_accuracy: 0.05334 | valid_balanced_accuracy: 0.05398 |  0:25:17s\n",
      "epoch 30 | loss: 3.82605 | train_balanced_accuracy: 0.05356 | valid_balanced_accuracy: 0.05491 |  0:26:08s\n",
      "epoch 31 | loss: 3.77697 | train_balanced_accuracy: 0.05295 | valid_balanced_accuracy: 0.05317 |  0:26:59s\n",
      "epoch 32 | loss: 3.73596 | train_balanced_accuracy: 0.05474 | valid_balanced_accuracy: 0.05583 |  0:27:49s\n",
      "epoch 33 | loss: 3.68775 | train_balanced_accuracy: 0.05357 | valid_balanced_accuracy: 0.05276 |  0:28:39s\n",
      "epoch 34 | loss: 3.63678 | train_balanced_accuracy: 0.05434 | valid_balanced_accuracy: 0.05612 |  0:29:28s\n",
      "epoch 35 | loss: 3.59542 | train_balanced_accuracy: 0.05391 | valid_balanced_accuracy: 0.05388 |  0:30:18s\n",
      "epoch 36 | loss: 3.55037 | train_balanced_accuracy: 0.05451 | valid_balanced_accuracy: 0.05549 |  0:31:08s\n",
      "epoch 37 | loss: 3.51504 | train_balanced_accuracy: 0.05547 | valid_balanced_accuracy: 0.05512 |  0:31:58s\n",
      "epoch 38 | loss: 3.47733 | train_balanced_accuracy: 0.05683 | valid_balanced_accuracy: 0.05665 |  0:32:47s\n",
      "epoch 39 | loss: 3.44295 | train_balanced_accuracy: 0.05561 | valid_balanced_accuracy: 0.05699 |  0:33:37s\n",
      "epoch 40 | loss: 3.40191 | train_balanced_accuracy: 0.05613 | valid_balanced_accuracy: 0.05717 |  0:34:27s\n",
      "epoch 41 | loss: 3.36204 | train_balanced_accuracy: 0.05818 | valid_balanced_accuracy: 0.05965 |  0:35:17s\n",
      "epoch 42 | loss: 3.33421 | train_balanced_accuracy: 0.05863 | valid_balanced_accuracy: 0.05975 |  0:36:07s\n",
      "epoch 43 | loss: 3.30272 | train_balanced_accuracy: 0.05844 | valid_balanced_accuracy: 0.0587  |  0:36:56s\n",
      "epoch 44 | loss: 3.27266 | train_balanced_accuracy: 0.05803 | valid_balanced_accuracy: 0.05936 |  0:37:46s\n",
      "epoch 45 | loss: 3.24257 | train_balanced_accuracy: 0.05704 | valid_balanced_accuracy: 0.05783 |  0:38:36s\n",
      "epoch 46 | loss: 3.21283 | train_balanced_accuracy: 0.0577  | valid_balanced_accuracy: 0.05879 |  0:39:25s\n",
      "epoch 47 | loss: 3.18306 | train_balanced_accuracy: 0.05919 | valid_balanced_accuracy: 0.06168 |  0:40:15s\n",
      "epoch 48 | loss: 3.15882 | train_balanced_accuracy: 0.05907 | valid_balanced_accuracy: 0.05967 |  0:41:04s\n",
      "epoch 49 | loss: 3.13588 | train_balanced_accuracy: 0.06019 | valid_balanced_accuracy: 0.06073 |  0:41:53s\n",
      "epoch 50 | loss: 3.11484 | train_balanced_accuracy: 0.05895 | valid_balanced_accuracy: 0.05997 |  0:42:43s\n",
      "epoch 51 | loss: 3.07764 | train_balanced_accuracy: 0.05899 | valid_balanced_accuracy: 0.05958 |  0:43:32s\n",
      "epoch 52 | loss: 3.06814 | train_balanced_accuracy: 0.05815 | valid_balanced_accuracy: 0.05882 |  0:44:22s\n",
      "epoch 53 | loss: 3.04356 | train_balanced_accuracy: 0.0594  | valid_balanced_accuracy: 0.06126 |  0:45:11s\n",
      "epoch 54 | loss: 3.02111 | train_balanced_accuracy: 0.06206 | valid_balanced_accuracy: 0.06234 |  0:46:01s\n",
      "epoch 55 | loss: 2.997   | train_balanced_accuracy: 0.06315 | valid_balanced_accuracy: 0.0644  |  0:46:50s\n",
      "epoch 56 | loss: 2.98004 | train_balanced_accuracy: 0.06138 | valid_balanced_accuracy: 0.0613  |  0:47:40s\n",
      "epoch 57 | loss: 2.95829 | train_balanced_accuracy: 0.0611  | valid_balanced_accuracy: 0.06239 |  0:48:32s\n",
      "epoch 58 | loss: 2.94036 | train_balanced_accuracy: 0.06078 | valid_balanced_accuracy: 0.06237 |  0:49:22s\n",
      "epoch 59 | loss: 2.9168  | train_balanced_accuracy: 0.06237 | valid_balanced_accuracy: 0.06264 |  0:50:12s\n",
      "epoch 60 | loss: 2.89778 | train_balanced_accuracy: 0.06305 | valid_balanced_accuracy: 0.06508 |  0:51:01s\n",
      "epoch 61 | loss: 2.88155 | train_balanced_accuracy: 0.06236 | valid_balanced_accuracy: 0.06226 |  0:51:51s\n",
      "epoch 62 | loss: 2.86892 | train_balanced_accuracy: 0.06151 | valid_balanced_accuracy: 0.0618  |  0:52:40s\n",
      "epoch 63 | loss: 2.84906 | train_balanced_accuracy: 0.06312 | valid_balanced_accuracy: 0.0633  |  0:53:30s\n",
      "epoch 64 | loss: 2.83344 | train_balanced_accuracy: 0.06476 | valid_balanced_accuracy: 0.06546 |  0:54:19s\n",
      "epoch 65 | loss: 2.81657 | train_balanced_accuracy: 0.06616 | valid_balanced_accuracy: 0.06742 |  0:55:09s\n",
      "epoch 66 | loss: 2.80222 | train_balanced_accuracy: 0.06557 | valid_balanced_accuracy: 0.06623 |  0:55:59s\n",
      "epoch 67 | loss: 2.78763 | train_balanced_accuracy: 0.06494 | valid_balanced_accuracy: 0.06522 |  0:56:48s\n",
      "epoch 68 | loss: 2.76885 | train_balanced_accuracy: 0.06376 | valid_balanced_accuracy: 0.06537 |  0:57:38s\n",
      "epoch 69 | loss: 2.75231 | train_balanced_accuracy: 0.06446 | valid_balanced_accuracy: 0.06453 |  0:58:27s\n",
      "epoch 70 | loss: 2.74035 | train_balanced_accuracy: 0.06599 | valid_balanced_accuracy: 0.06653 |  0:59:18s\n",
      "epoch 71 | loss: 2.72863 | train_balanced_accuracy: 0.06638 | valid_balanced_accuracy: 0.06634 |  1:00:08s\n",
      "epoch 72 | loss: 2.71855 | train_balanced_accuracy: 0.06588 | valid_balanced_accuracy: 0.0664  |  1:00:57s\n",
      "epoch 73 | loss: 2.70554 | train_balanced_accuracy: 0.06689 | valid_balanced_accuracy: 0.06712 |  1:01:48s\n",
      "epoch 74 | loss: 2.69093 | train_balanced_accuracy: 0.06705 | valid_balanced_accuracy: 0.06712 |  1:02:38s\n",
      "epoch 75 | loss: 2.67717 | train_balanced_accuracy: 0.0691  | valid_balanced_accuracy: 0.06916 |  1:03:29s\n",
      "epoch 76 | loss: 2.66362 | train_balanced_accuracy: 0.06781 | valid_balanced_accuracy: 0.06818 |  1:04:19s\n",
      "epoch 77 | loss: 2.65228 | train_balanced_accuracy: 0.06912 | valid_balanced_accuracy: 0.06871 |  1:05:09s\n",
      "epoch 78 | loss: 2.6418  | train_balanced_accuracy: 0.06883 | valid_balanced_accuracy: 0.06901 |  1:05:59s\n",
      "epoch 79 | loss: 2.62909 | train_balanced_accuracy: 0.06695 | valid_balanced_accuracy: 0.06774 |  1:06:49s\n",
      "epoch 80 | loss: 2.6154  | train_balanced_accuracy: 0.06799 | valid_balanced_accuracy: 0.06938 |  1:07:39s\n",
      "epoch 81 | loss: 2.61107 | train_balanced_accuracy: 0.06949 | valid_balanced_accuracy: 0.0704  |  1:08:30s\n",
      "epoch 82 | loss: 2.59579 | train_balanced_accuracy: 0.07095 | valid_balanced_accuracy: 0.07065 |  1:09:20s\n",
      "epoch 83 | loss: 2.58367 | train_balanced_accuracy: 0.06841 | valid_balanced_accuracy: 0.06803 |  1:10:09s\n",
      "epoch 84 | loss: 2.5772  | train_balanced_accuracy: 0.07112 | valid_balanced_accuracy: 0.07143 |  1:10:59s\n",
      "epoch 85 | loss: 2.5692  | train_balanced_accuracy: 0.07176 | valid_balanced_accuracy: 0.07233 |  1:11:50s\n",
      "epoch 86 | loss: 2.55706 | train_balanced_accuracy: 0.07285 | valid_balanced_accuracy: 0.07221 |  1:12:41s\n",
      "epoch 87 | loss: 2.5479  | train_balanced_accuracy: 0.07183 | valid_balanced_accuracy: 0.07176 |  1:13:31s\n",
      "epoch 88 | loss: 2.54057 | train_balanced_accuracy: 0.07158 | valid_balanced_accuracy: 0.07136 |  1:14:22s\n",
      "epoch 89 | loss: 2.52594 | train_balanced_accuracy: 0.07299 | valid_balanced_accuracy: 0.07359 |  1:15:13s\n",
      "epoch 90 | loss: 2.52115 | train_balanced_accuracy: 0.07232 | valid_balanced_accuracy: 0.07202 |  1:16:04s\n",
      "epoch 91 | loss: 2.51629 | train_balanced_accuracy: 0.07272 | valid_balanced_accuracy: 0.07337 |  1:16:56s\n",
      "epoch 92 | loss: 2.5019  | train_balanced_accuracy: 0.07482 | valid_balanced_accuracy: 0.07554 |  1:17:46s\n",
      "epoch 93 | loss: 2.49113 | train_balanced_accuracy: 0.07447 | valid_balanced_accuracy: 0.07494 |  1:18:36s\n",
      "epoch 94 | loss: 2.48639 | train_balanced_accuracy: 0.0748  | valid_balanced_accuracy: 0.07516 |  1:19:26s\n",
      "epoch 95 | loss: 2.47528 | train_balanced_accuracy: 0.07757 | valid_balanced_accuracy: 0.07788 |  1:20:16s\n",
      "epoch 96 | loss: 2.47099 | train_balanced_accuracy: 0.07807 | valid_balanced_accuracy: 0.07795 |  1:21:05s\n",
      "epoch 97 | loss: 2.46193 | train_balanced_accuracy: 0.07775 | valid_balanced_accuracy: 0.07687 |  1:21:55s\n",
      "epoch 98 | loss: 2.4597  | train_balanced_accuracy: 0.07908 | valid_balanced_accuracy: 0.07828 |  1:22:45s\n",
      "epoch 99 | loss: 2.45034 | train_balanced_accuracy: 0.07959 | valid_balanced_accuracy: 0.07977 |  1:23:36s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_valid_balanced_accuracy = 0.07977\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'gamma': 1.8}, optimizer_params={'lr': 1e-05}, n_steps=8, n_shared=4, n_independent=1, n_a=16, momentum=0.05, lambda_sparse=0.01, clip_value=0.01, total=83.9min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'step_size': 50}, optimizer_params={'lr': 0.001}, n_steps=13, n_shared=1, n_independent=5, n_a=16, momentum=0.005, lambda_sparse=0.0001, clip_value=1.0 \n",
      "epoch 0  | loss: 6.08645 | train_balanced_accuracy: 0.11806 | valid_balanced_accuracy: 0.12026 |  0:01:29s\n",
      "epoch 1  | loss: 2.30299 | train_balanced_accuracy: 0.15181 | valid_balanced_accuracy: 0.153   |  0:03:00s\n",
      "epoch 2  | loss: 1.98913 | train_balanced_accuracy: 0.18717 | valid_balanced_accuracy: 0.18596 |  0:04:30s\n",
      "epoch 3  | loss: 1.83902 | train_balanced_accuracy: 0.19987 | valid_balanced_accuracy: 0.20313 |  0:06:02s\n",
      "epoch 4  | loss: 1.75193 | train_balanced_accuracy: 0.20188 | valid_balanced_accuracy: 0.20582 |  0:07:32s\n",
      "epoch 5  | loss: 1.7106  | train_balanced_accuracy: 0.22705 | valid_balanced_accuracy: 0.23012 |  0:09:03s\n",
      "epoch 6  | loss: 1.6854  | train_balanced_accuracy: 0.20644 | valid_balanced_accuracy: 0.2037  |  0:10:34s\n",
      "epoch 7  | loss: 1.65832 | train_balanced_accuracy: 0.23227 | valid_balanced_accuracy: 0.23065 |  0:12:05s\n",
      "epoch 8  | loss: 1.63625 | train_balanced_accuracy: 0.2215  | valid_balanced_accuracy: 0.22094 |  0:13:36s\n",
      "epoch 9  | loss: 1.63584 | train_balanced_accuracy: 0.24868 | valid_balanced_accuracy: 0.24771 |  0:15:06s\n",
      "epoch 10 | loss: 1.61525 | train_balanced_accuracy: 0.24948 | valid_balanced_accuracy: 0.25244 |  0:16:37s\n",
      "epoch 11 | loss: 1.60468 | train_balanced_accuracy: 0.24437 | valid_balanced_accuracy: 0.24649 |  0:18:09s\n",
      "epoch 12 | loss: 1.59298 | train_balanced_accuracy: 0.25548 | valid_balanced_accuracy: 0.25251 |  0:19:40s\n",
      "epoch 13 | loss: 1.58704 | train_balanced_accuracy: 0.26404 | valid_balanced_accuracy: 0.26395 |  0:21:10s\n",
      "epoch 14 | loss: 1.5884  | train_balanced_accuracy: 0.26154 | valid_balanced_accuracy: 0.26258 |  0:22:41s\n",
      "epoch 15 | loss: 1.58498 | train_balanced_accuracy: 0.25522 | valid_balanced_accuracy: 0.25758 |  0:24:09s\n",
      "epoch 16 | loss: 1.57986 | train_balanced_accuracy: 0.26565 | valid_balanced_accuracy: 0.2581  |  0:25:36s\n",
      "epoch 17 | loss: 1.57387 | train_balanced_accuracy: 0.2606  | valid_balanced_accuracy: 0.26368 |  0:27:03s\n",
      "epoch 18 | loss: 1.55757 | train_balanced_accuracy: 0.24329 | valid_balanced_accuracy: 0.24561 |  0:28:31s\n",
      "epoch 19 | loss: 1.55996 | train_balanced_accuracy: 0.27073 | valid_balanced_accuracy: 0.26844 |  0:29:58s\n",
      "epoch 20 | loss: 1.55794 | train_balanced_accuracy: 0.26409 | valid_balanced_accuracy: 0.25886 |  0:31:26s\n",
      "epoch 21 | loss: 1.5524  | train_balanced_accuracy: 0.27761 | valid_balanced_accuracy: 0.27713 |  0:32:56s\n",
      "epoch 22 | loss: 1.54645 | train_balanced_accuracy: 0.26034 | valid_balanced_accuracy: 0.26475 |  0:34:26s\n",
      "epoch 23 | loss: 1.54197 | train_balanced_accuracy: 0.26946 | valid_balanced_accuracy: 0.26425 |  0:35:57s\n",
      "epoch 24 | loss: 1.53959 | train_balanced_accuracy: 0.27339 | valid_balanced_accuracy: 0.27336 |  0:37:28s\n",
      "epoch 25 | loss: 1.54132 | train_balanced_accuracy: 0.27967 | valid_balanced_accuracy: 0.28141 |  0:38:59s\n",
      "epoch 26 | loss: 1.53436 | train_balanced_accuracy: 0.28432 | valid_balanced_accuracy: 0.28467 |  0:40:29s\n",
      "epoch 27 | loss: 1.53279 | train_balanced_accuracy: 0.28057 | valid_balanced_accuracy: 0.27803 |  0:41:58s\n",
      "epoch 28 | loss: 1.52448 | train_balanced_accuracy: 0.3051  | valid_balanced_accuracy: 0.30288 |  0:43:27s\n",
      "epoch 29 | loss: 1.51741 | train_balanced_accuracy: 0.28408 | valid_balanced_accuracy: 0.29128 |  0:44:55s\n",
      "epoch 30 | loss: 1.51504 | train_balanced_accuracy: 0.28347 | valid_balanced_accuracy: 0.28621 |  0:46:24s\n",
      "epoch 31 | loss: 1.51294 | train_balanced_accuracy: 0.29715 | valid_balanced_accuracy: 0.2977  |  0:47:55s\n",
      "epoch 32 | loss: 1.51725 | train_balanced_accuracy: 0.30667 | valid_balanced_accuracy: 0.31212 |  0:49:24s\n",
      "epoch 33 | loss: 1.51415 | train_balanced_accuracy: 0.28868 | valid_balanced_accuracy: 0.29062 |  0:50:55s\n",
      "epoch 34 | loss: 1.51206 | train_balanced_accuracy: 0.31618 | valid_balanced_accuracy: 0.31444 |  0:52:27s\n",
      "epoch 35 | loss: 1.50699 | train_balanced_accuracy: 0.29862 | valid_balanced_accuracy: 0.30411 |  0:53:58s\n",
      "epoch 36 | loss: 1.50699 | train_balanced_accuracy: 0.30378 | valid_balanced_accuracy: 0.30855 |  0:55:30s\n",
      "epoch 37 | loss: 1.50707 | train_balanced_accuracy: 0.30654 | valid_balanced_accuracy: 0.31043 |  0:57:01s\n",
      "epoch 38 | loss: 1.50531 | train_balanced_accuracy: 0.31488 | valid_balanced_accuracy: 0.30981 |  0:58:33s\n",
      "epoch 39 | loss: 1.50124 | train_balanced_accuracy: 0.31457 | valid_balanced_accuracy: 0.31767 |  1:00:02s\n",
      "epoch 40 | loss: 1.49828 | train_balanced_accuracy: 0.31369 | valid_balanced_accuracy: 0.30441 |  1:01:32s\n",
      "epoch 41 | loss: 1.49606 | train_balanced_accuracy: 0.30913 | valid_balanced_accuracy: 0.30275 |  1:03:01s\n",
      "epoch 42 | loss: 1.48691 | train_balanced_accuracy: 0.32893 | valid_balanced_accuracy: 0.32894 |  1:04:31s\n",
      "epoch 43 | loss: 1.4822  | train_balanced_accuracy: 0.3522  | valid_balanced_accuracy: 0.35016 |  1:06:00s\n",
      "epoch 44 | loss: 1.47919 | train_balanced_accuracy: 0.34391 | valid_balanced_accuracy: 0.33344 |  1:07:28s\n",
      "epoch 45 | loss: 1.48145 | train_balanced_accuracy: 0.33364 | valid_balanced_accuracy: 0.3232  |  1:08:56s\n",
      "epoch 46 | loss: 1.47454 | train_balanced_accuracy: 0.33431 | valid_balanced_accuracy: 0.33303 |  1:10:25s\n",
      "epoch 47 | loss: 1.46929 | train_balanced_accuracy: 0.34195 | valid_balanced_accuracy: 0.33387 |  1:11:54s\n",
      "epoch 48 | loss: 1.46588 | train_balanced_accuracy: 0.34631 | valid_balanced_accuracy: 0.34638 |  1:13:23s\n",
      "epoch 49 | loss: 1.46165 | train_balanced_accuracy: 0.35232 | valid_balanced_accuracy: 0.36111 |  1:14:51s\n",
      "epoch 50 | loss: 1.46048 | train_balanced_accuracy: 0.3334  | valid_balanced_accuracy: 0.33167 |  1:16:19s\n",
      "epoch 51 | loss: 1.46127 | train_balanced_accuracy: 0.34096 | valid_balanced_accuracy: 0.34643 |  1:17:47s\n",
      "epoch 52 | loss: 1.46069 | train_balanced_accuracy: 0.35708 | valid_balanced_accuracy: 0.35892 |  1:19:15s\n",
      "epoch 53 | loss: 1.45559 | train_balanced_accuracy: 0.34513 | valid_balanced_accuracy: 0.35288 |  1:20:42s\n",
      "epoch 54 | loss: 1.45185 | train_balanced_accuracy: 0.34107 | valid_balanced_accuracy: 0.3516  |  1:22:10s\n",
      "epoch 55 | loss: 1.446   | train_balanced_accuracy: 0.34169 | valid_balanced_accuracy: 0.35075 |  1:23:40s\n",
      "epoch 56 | loss: 1.45051 | train_balanced_accuracy: 0.35181 | valid_balanced_accuracy: 0.35073 |  1:25:12s\n",
      "epoch 57 | loss: 1.44617 | train_balanced_accuracy: 0.34877 | valid_balanced_accuracy: 0.35329 |  1:26:42s\n",
      "epoch 58 | loss: 1.44536 | train_balanced_accuracy: 0.35698 | valid_balanced_accuracy: 0.3531  |  1:28:10s\n",
      "epoch 59 | loss: 1.43806 | train_balanced_accuracy: 0.36384 | valid_balanced_accuracy: 0.36544 |  1:29:39s\n",
      "epoch 60 | loss: 1.43597 | train_balanced_accuracy: 0.35983 | valid_balanced_accuracy: 0.3561  |  1:31:07s\n",
      "epoch 61 | loss: 1.43755 | train_balanced_accuracy: 0.34366 | valid_balanced_accuracy: 0.34436 |  1:32:36s\n",
      "epoch 62 | loss: 1.42916 | train_balanced_accuracy: 0.35554 | valid_balanced_accuracy: 0.35434 |  1:34:04s\n",
      "epoch 63 | loss: 1.4258  | train_balanced_accuracy: 0.36214 | valid_balanced_accuracy: 0.36545 |  1:35:32s\n",
      "epoch 64 | loss: 1.42748 | train_balanced_accuracy: 0.37055 | valid_balanced_accuracy: 0.36752 |  1:37:00s\n",
      "epoch 65 | loss: 1.4146  | train_balanced_accuracy: 0.39172 | valid_balanced_accuracy: 0.39442 |  1:38:28s\n",
      "epoch 66 | loss: 1.40803 | train_balanced_accuracy: 0.39713 | valid_balanced_accuracy: 0.39563 |  1:39:56s\n",
      "epoch 67 | loss: 1.41198 | train_balanced_accuracy: 0.36248 | valid_balanced_accuracy: 0.361   |  1:41:26s\n",
      "epoch 68 | loss: 1.41371 | train_balanced_accuracy: 0.36828 | valid_balanced_accuracy: 0.37321 |  1:42:56s\n",
      "epoch 69 | loss: 1.41394 | train_balanced_accuracy: 0.36042 | valid_balanced_accuracy: 0.3638  |  1:44:28s\n",
      "epoch 70 | loss: 1.4112  | train_balanced_accuracy: 0.38554 | valid_balanced_accuracy: 0.38767 |  1:45:59s\n",
      "epoch 71 | loss: 1.41107 | train_balanced_accuracy: 0.39039 | valid_balanced_accuracy: 0.38924 |  1:47:28s\n",
      "epoch 72 | loss: 1.40259 | train_balanced_accuracy: 0.38474 | valid_balanced_accuracy: 0.38149 |  1:48:57s\n",
      "epoch 73 | loss: 1.39641 | train_balanced_accuracy: 0.37845 | valid_balanced_accuracy: 0.37223 |  1:50:27s\n",
      "epoch 74 | loss: 1.39672 | train_balanced_accuracy: 0.37349 | valid_balanced_accuracy: 0.37638 |  1:51:56s\n",
      "epoch 75 | loss: 1.3976  | train_balanced_accuracy: 0.39222 | valid_balanced_accuracy: 0.39001 |  1:53:25s\n",
      "epoch 76 | loss: 1.39369 | train_balanced_accuracy: 0.38621 | valid_balanced_accuracy: 0.38985 |  1:54:54s\n",
      "\n",
      "Early stopping occured at epoch 76 with best_epoch = 66 and best_valid_balanced_accuracy = 0.39563\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'step_size': 50}, optimizer_params={'lr': 0.001}, n_steps=13, n_shared=1, n_independent=5, n_a=16, momentum=0.005, lambda_sparse=0.0001, clip_value=1.0, total=115.4min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'step_size': 50}, optimizer_params={'lr': 0.001}, n_steps=13, n_shared=1, n_independent=5, n_a=16, momentum=0.005, lambda_sparse=0.0001, clip_value=1.0 \n",
      "epoch 0  | loss: 6.16441 | train_balanced_accuracy: 0.10057 | valid_balanced_accuracy: 0.09942 |  0:01:29s\n",
      "epoch 1  | loss: 2.3351  | train_balanced_accuracy: 0.1502  | valid_balanced_accuracy: 0.15516 |  0:02:58s\n",
      "epoch 2  | loss: 1.95817 | train_balanced_accuracy: 0.16828 | valid_balanced_accuracy: 0.17091 |  0:04:27s\n",
      "epoch 3  | loss: 1.81947 | train_balanced_accuracy: 0.20288 | valid_balanced_accuracy: 0.20448 |  0:05:57s\n",
      "epoch 4  | loss: 1.76805 | train_balanced_accuracy: 0.19612 | valid_balanced_accuracy: 0.20087 |  0:07:27s\n",
      "epoch 5  | loss: 1.74798 | train_balanced_accuracy: 0.1986  | valid_balanced_accuracy: 0.2003  |  0:08:57s\n",
      "epoch 6  | loss: 1.70356 | train_balanced_accuracy: 0.22452 | valid_balanced_accuracy: 0.22464 |  0:10:27s\n",
      "epoch 7  | loss: 1.68056 | train_balanced_accuracy: 0.24255 | valid_balanced_accuracy: 0.24381 |  0:11:56s\n",
      "epoch 8  | loss: 1.65255 | train_balanced_accuracy: 0.24731 | valid_balanced_accuracy: 0.24187 |  0:13:26s\n",
      "epoch 9  | loss: 1.63504 | train_balanced_accuracy: 0.22566 | valid_balanced_accuracy: 0.22484 |  0:14:55s\n",
      "epoch 10 | loss: 1.6209  | train_balanced_accuracy: 0.23503 | valid_balanced_accuracy: 0.23687 |  0:16:24s\n",
      "epoch 11 | loss: 1.60644 | train_balanced_accuracy: 0.23924 | valid_balanced_accuracy: 0.23913 |  0:17:52s\n",
      "epoch 12 | loss: 1.5989  | train_balanced_accuracy: 0.238   | valid_balanced_accuracy: 0.23607 |  0:19:21s\n",
      "epoch 13 | loss: 1.59945 | train_balanced_accuracy: 0.24695 | valid_balanced_accuracy: 0.2456  |  0:20:50s\n",
      "epoch 14 | loss: 1.59989 | train_balanced_accuracy: 0.26331 | valid_balanced_accuracy: 0.26586 |  0:22:18s\n",
      "epoch 15 | loss: 1.59241 | train_balanced_accuracy: 0.25133 | valid_balanced_accuracy: 0.25073 |  0:23:49s\n",
      "epoch 16 | loss: 1.58417 | train_balanced_accuracy: 0.27995 | valid_balanced_accuracy: 0.28513 |  0:25:20s\n",
      "epoch 17 | loss: 1.57917 | train_balanced_accuracy: 0.27523 | valid_balanced_accuracy: 0.27896 |  0:26:51s\n",
      "epoch 18 | loss: 1.5767  | train_balanced_accuracy: 0.27955 | valid_balanced_accuracy: 0.27315 |  0:28:20s\n",
      "epoch 19 | loss: 1.57078 | train_balanced_accuracy: 0.27817 | valid_balanced_accuracy: 0.27848 |  0:29:48s\n",
      "epoch 20 | loss: 1.5739  | train_balanced_accuracy: 0.25783 | valid_balanced_accuracy: 0.25771 |  0:31:16s\n",
      "epoch 21 | loss: 1.56672 | train_balanced_accuracy: 0.2754  | valid_balanced_accuracy: 0.28153 |  0:32:44s\n",
      "epoch 22 | loss: 1.55892 | train_balanced_accuracy: 0.27432 | valid_balanced_accuracy: 0.26604 |  0:34:12s\n",
      "epoch 23 | loss: 1.558   | train_balanced_accuracy: 0.25783 | valid_balanced_accuracy: 0.25901 |  0:35:40s\n",
      "epoch 24 | loss: 1.55253 | train_balanced_accuracy: 0.26215 | valid_balanced_accuracy: 0.26397 |  0:37:08s\n",
      "epoch 25 | loss: 1.55271 | train_balanced_accuracy: 0.29836 | valid_balanced_accuracy: 0.29189 |  0:38:36s\n",
      "epoch 26 | loss: 1.5498  | train_balanced_accuracy: 0.29357 | valid_balanced_accuracy: 0.29478 |  0:40:04s\n",
      "epoch 27 | loss: 1.53918 | train_balanced_accuracy: 0.28116 | valid_balanced_accuracy: 0.281   |  0:41:34s\n",
      "epoch 28 | loss: 1.5276  | train_balanced_accuracy: 0.30619 | valid_balanced_accuracy: 0.30188 |  0:43:02s\n",
      "epoch 29 | loss: 1.51973 | train_balanced_accuracy: 0.29826 | valid_balanced_accuracy: 0.294   |  0:44:31s\n",
      "epoch 30 | loss: 1.50914 | train_balanced_accuracy: 0.30467 | valid_balanced_accuracy: 0.30022 |  0:46:00s\n",
      "epoch 31 | loss: 1.50636 | train_balanced_accuracy: 0.3048  | valid_balanced_accuracy: 0.3068  |  0:47:28s\n",
      "epoch 32 | loss: 1.50411 | train_balanced_accuracy: 0.31518 | valid_balanced_accuracy: 0.31621 |  0:48:57s\n",
      "epoch 33 | loss: 1.49741 | train_balanced_accuracy: 0.30785 | valid_balanced_accuracy: 0.30455 |  0:50:27s\n",
      "epoch 34 | loss: 1.49081 | train_balanced_accuracy: 0.34235 | valid_balanced_accuracy: 0.34399 |  0:51:56s\n",
      "epoch 35 | loss: 1.48576 | train_balanced_accuracy: 0.34281 | valid_balanced_accuracy: 0.33887 |  0:53:25s\n",
      "epoch 36 | loss: 1.4762  | train_balanced_accuracy: 0.34758 | valid_balanced_accuracy: 0.34603 |  0:54:53s\n",
      "epoch 37 | loss: 1.46994 | train_balanced_accuracy: 0.34539 | valid_balanced_accuracy: 0.3404  |  0:56:22s\n",
      "epoch 38 | loss: 1.46927 | train_balanced_accuracy: 0.33639 | valid_balanced_accuracy: 0.33107 |  0:57:51s\n",
      "epoch 39 | loss: 1.47134 | train_balanced_accuracy: 0.34372 | valid_balanced_accuracy: 0.33975 |  0:59:19s\n",
      "epoch 40 | loss: 1.46291 | train_balanced_accuracy: 0.34584 | valid_balanced_accuracy: 0.33995 |  1:00:47s\n",
      "epoch 41 | loss: 1.46748 | train_balanced_accuracy: 0.36462 | valid_balanced_accuracy: 0.3597  |  1:02:16s\n",
      "epoch 42 | loss: 1.46172 | train_balanced_accuracy: 0.35629 | valid_balanced_accuracy: 0.35392 |  1:03:43s\n",
      "epoch 43 | loss: 1.45798 | train_balanced_accuracy: 0.35205 | valid_balanced_accuracy: 0.33861 |  1:05:11s\n",
      "epoch 44 | loss: 1.45477 | train_balanced_accuracy: 0.34744 | valid_balanced_accuracy: 0.33337 |  1:06:41s\n",
      "epoch 45 | loss: 1.45428 | train_balanced_accuracy: 0.36915 | valid_balanced_accuracy: 0.36984 |  1:08:12s\n",
      "epoch 46 | loss: 1.44807 | train_balanced_accuracy: 0.35809 | valid_balanced_accuracy: 0.35511 |  1:09:41s\n",
      "epoch 47 | loss: 1.43604 | train_balanced_accuracy: 0.38939 | valid_balanced_accuracy: 0.3924  |  1:11:10s\n",
      "epoch 48 | loss: 1.42885 | train_balanced_accuracy: 0.37268 | valid_balanced_accuracy: 0.36814 |  1:12:38s\n",
      "epoch 49 | loss: 1.42404 | train_balanced_accuracy: 0.35589 | valid_balanced_accuracy: 0.34481 |  1:14:06s\n",
      "epoch 50 | loss: 1.43384 | train_balanced_accuracy: 0.38024 | valid_balanced_accuracy: 0.38471 |  1:15:34s\n",
      "epoch 51 | loss: 1.42499 | train_balanced_accuracy: 0.37554 | valid_balanced_accuracy: 0.36322 |  1:17:03s\n",
      "epoch 52 | loss: 1.42036 | train_balanced_accuracy: 0.38508 | valid_balanced_accuracy: 0.37927 |  1:18:31s\n",
      "epoch 53 | loss: 1.41454 | train_balanced_accuracy: 0.38183 | valid_balanced_accuracy: 0.37776 |  1:20:00s\n",
      "epoch 54 | loss: 1.41469 | train_balanced_accuracy: 0.39225 | valid_balanced_accuracy: 0.39254 |  1:21:28s\n",
      "epoch 55 | loss: 1.40764 | train_balanced_accuracy: 0.38806 | valid_balanced_accuracy: 0.38751 |  1:22:57s\n",
      "epoch 56 | loss: 1.40632 | train_balanced_accuracy: 0.38762 | valid_balanced_accuracy: 0.37837 |  1:24:25s\n",
      "epoch 57 | loss: 1.40247 | train_balanced_accuracy: 0.38304 | valid_balanced_accuracy: 0.37633 |  1:25:54s\n",
      "epoch 58 | loss: 1.40318 | train_balanced_accuracy: 0.39685 | valid_balanced_accuracy: 0.38901 |  1:27:21s\n",
      "epoch 59 | loss: 1.40153 | train_balanced_accuracy: 0.39355 | valid_balanced_accuracy: 0.39542 |  1:28:50s\n",
      "epoch 60 | loss: 1.39975 | train_balanced_accuracy: 0.39461 | valid_balanced_accuracy: 0.39341 |  1:30:18s\n",
      "epoch 61 | loss: 1.39727 | train_balanced_accuracy: 0.39531 | valid_balanced_accuracy: 0.40003 |  1:31:46s\n",
      "epoch 62 | loss: 1.38915 | train_balanced_accuracy: 0.39645 | valid_balanced_accuracy: 0.39552 |  1:33:14s\n",
      "epoch 63 | loss: 1.38222 | train_balanced_accuracy: 0.41908 | valid_balanced_accuracy: 0.41098 |  1:34:41s\n",
      "epoch 64 | loss: 1.37409 | train_balanced_accuracy: 0.41516 | valid_balanced_accuracy: 0.41258 |  1:36:09s\n",
      "epoch 65 | loss: 1.37334 | train_balanced_accuracy: 0.41778 | valid_balanced_accuracy: 0.41072 |  1:37:37s\n",
      "epoch 66 | loss: 1.37081 | train_balanced_accuracy: 0.41821 | valid_balanced_accuracy: 0.41104 |  1:39:05s\n",
      "epoch 67 | loss: 1.36757 | train_balanced_accuracy: 0.40318 | valid_balanced_accuracy: 0.39275 |  1:40:34s\n",
      "epoch 68 | loss: 1.37807 | train_balanced_accuracy: 0.4044  | valid_balanced_accuracy: 0.39866 |  1:42:02s\n",
      "epoch 69 | loss: 1.38651 | train_balanced_accuracy: 0.39766 | valid_balanced_accuracy: 0.38665 |  1:43:30s\n",
      "epoch 70 | loss: 1.38269 | train_balanced_accuracy: 0.41051 | valid_balanced_accuracy: 0.39664 |  1:44:58s\n",
      "epoch 71 | loss: 1.38269 | train_balanced_accuracy: 0.42212 | valid_balanced_accuracy: 0.41943 |  1:46:29s\n",
      "epoch 72 | loss: 1.37985 | train_balanced_accuracy: 0.41296 | valid_balanced_accuracy: 0.4068  |  1:48:00s\n",
      "epoch 73 | loss: 1.37397 | train_balanced_accuracy: 0.41408 | valid_balanced_accuracy: 0.40958 |  1:49:31s\n",
      "epoch 74 | loss: 1.37421 | train_balanced_accuracy: 0.41182 | valid_balanced_accuracy: 0.40529 |  1:51:02s\n",
      "epoch 75 | loss: 1.37159 | train_balanced_accuracy: 0.41602 | valid_balanced_accuracy: 0.41576 |  1:52:33s\n",
      "epoch 76 | loss: 1.36447 | train_balanced_accuracy: 0.41192 | valid_balanced_accuracy: 0.4038  |  1:54:04s\n",
      "epoch 77 | loss: 1.36499 | train_balanced_accuracy: 0.40964 | valid_balanced_accuracy: 0.40288 |  1:55:35s\n",
      "epoch 78 | loss: 1.36546 | train_balanced_accuracy: 0.43069 | valid_balanced_accuracy: 0.42463 |  1:57:07s\n",
      "epoch 79 | loss: 1.36456 | train_balanced_accuracy: 0.4219  | valid_balanced_accuracy: 0.41348 |  1:58:38s\n",
      "epoch 80 | loss: 1.36706 | train_balanced_accuracy: 0.42949 | valid_balanced_accuracy: 0.42935 |  2:00:09s\n",
      "epoch 81 | loss: 1.3663  | train_balanced_accuracy: 0.42043 | valid_balanced_accuracy: 0.40942 |  2:01:40s\n",
      "epoch 82 | loss: 1.37004 | train_balanced_accuracy: 0.41361 | valid_balanced_accuracy: 0.40292 |  2:03:10s\n",
      "epoch 83 | loss: 1.3724  | train_balanced_accuracy: 0.42269 | valid_balanced_accuracy: 0.41433 |  2:04:39s\n",
      "epoch 84 | loss: 1.3621  | train_balanced_accuracy: 0.43152 | valid_balanced_accuracy: 0.42257 |  2:06:06s\n",
      "epoch 85 | loss: 1.36188 | train_balanced_accuracy: 0.4217  | valid_balanced_accuracy: 0.42339 |  2:07:34s\n",
      "epoch 86 | loss: 1.35961 | train_balanced_accuracy: 0.44277 | valid_balanced_accuracy: 0.43516 |  2:09:01s\n",
      "epoch 87 | loss: 1.35601 | train_balanced_accuracy: 0.41827 | valid_balanced_accuracy: 0.41259 |  2:10:29s\n",
      "epoch 88 | loss: 1.35123 | train_balanced_accuracy: 0.43737 | valid_balanced_accuracy: 0.42608 |  2:11:57s\n",
      "epoch 89 | loss: 1.34669 | train_balanced_accuracy: 0.43561 | valid_balanced_accuracy: 0.42641 |  2:13:25s\n",
      "epoch 90 | loss: 1.34594 | train_balanced_accuracy: 0.42376 | valid_balanced_accuracy: 0.41276 |  2:14:53s\n",
      "epoch 91 | loss: 1.34396 | train_balanced_accuracy: 0.44451 | valid_balanced_accuracy: 0.43599 |  2:16:21s\n",
      "epoch 92 | loss: 1.34596 | train_balanced_accuracy: 0.43589 | valid_balanced_accuracy: 0.42638 |  2:17:51s\n",
      "epoch 93 | loss: 1.33896 | train_balanced_accuracy: 0.44205 | valid_balanced_accuracy: 0.43256 |  2:19:19s\n",
      "epoch 94 | loss: 1.33474 | train_balanced_accuracy: 0.43429 | valid_balanced_accuracy: 0.42409 |  2:20:48s\n",
      "epoch 95 | loss: 1.33262 | train_balanced_accuracy: 0.45051 | valid_balanced_accuracy: 0.43789 |  2:22:16s\n",
      "epoch 96 | loss: 1.32653 | train_balanced_accuracy: 0.44517 | valid_balanced_accuracy: 0.43639 |  2:23:46s\n",
      "epoch 97 | loss: 1.3287  | train_balanced_accuracy: 0.45168 | valid_balanced_accuracy: 0.44043 |  2:25:17s\n",
      "epoch 98 | loss: 1.3238  | train_balanced_accuracy: 0.45117 | valid_balanced_accuracy: 0.43732 |  2:26:49s\n",
      "epoch 99 | loss: 1.31733 | train_balanced_accuracy: 0.44429 | valid_balanced_accuracy: 0.43837 |  2:28:20s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 97 and best_valid_balanced_accuracy = 0.44043\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'step_size': 50}, optimizer_params={'lr': 0.001}, n_steps=13, n_shared=1, n_independent=5, n_a=16, momentum=0.005, lambda_sparse=0.0001, clip_value=1.0, total=148.8min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'step_size': 50}, optimizer_params={'lr': 0.001}, n_steps=13, n_shared=1, n_independent=5, n_a=16, momentum=0.005, lambda_sparse=0.0001, clip_value=1.0 \n",
      "epoch 0  | loss: 6.09578 | train_balanced_accuracy: 0.12381 | valid_balanced_accuracy: 0.1234  |  0:01:28s\n",
      "epoch 1  | loss: 2.33449 | train_balanced_accuracy: 0.15929 | valid_balanced_accuracy: 0.15678 |  0:02:57s\n",
      "epoch 2  | loss: 1.98607 | train_balanced_accuracy: 0.18681 | valid_balanced_accuracy: 0.18712 |  0:04:25s\n",
      "epoch 3  | loss: 1.83862 | train_balanced_accuracy: 0.20862 | valid_balanced_accuracy: 0.21051 |  0:05:54s\n",
      "epoch 4  | loss: 1.74993 | train_balanced_accuracy: 0.19876 | valid_balanced_accuracy: 0.20415 |  0:07:23s\n",
      "epoch 5  | loss: 1.73016 | train_balanced_accuracy: 0.20554 | valid_balanced_accuracy: 0.20204 |  0:08:53s\n",
      "epoch 6  | loss: 1.70707 | train_balanced_accuracy: 0.22587 | valid_balanced_accuracy: 0.22659 |  0:10:22s\n",
      "epoch 7  | loss: 1.67778 | train_balanced_accuracy: 0.22531 | valid_balanced_accuracy: 0.22774 |  0:11:50s\n",
      "epoch 8  | loss: 1.64267 | train_balanced_accuracy: 0.23944 | valid_balanced_accuracy: 0.23351 |  0:13:19s\n",
      "epoch 9  | loss: 1.62655 | train_balanced_accuracy: 0.23681 | valid_balanced_accuracy: 0.23837 |  0:14:48s\n",
      "epoch 10 | loss: 1.6135  | train_balanced_accuracy: 0.24959 | valid_balanced_accuracy: 0.24773 |  0:16:16s\n",
      "epoch 11 | loss: 1.61413 | train_balanced_accuracy: 0.23155 | valid_balanced_accuracy: 0.23473 |  0:17:45s\n",
      "epoch 12 | loss: 1.60187 | train_balanced_accuracy: 0.24348 | valid_balanced_accuracy: 0.24613 |  0:19:13s\n",
      "epoch 13 | loss: 1.59263 | train_balanced_accuracy: 0.27618 | valid_balanced_accuracy: 0.28016 |  0:20:42s\n",
      "epoch 14 | loss: 1.58064 | train_balanced_accuracy: 0.29127 | valid_balanced_accuracy: 0.29231 |  0:22:11s\n",
      "epoch 15 | loss: 1.56137 | train_balanced_accuracy: 0.27982 | valid_balanced_accuracy: 0.27859 |  0:23:39s\n",
      "epoch 16 | loss: 1.55357 | train_balanced_accuracy: 0.2822  | valid_balanced_accuracy: 0.27721 |  0:25:08s\n",
      "epoch 17 | loss: 1.55321 | train_balanced_accuracy: 0.28337 | valid_balanced_accuracy: 0.29205 |  0:26:37s\n",
      "epoch 18 | loss: 1.54492 | train_balanced_accuracy: 0.3031  | valid_balanced_accuracy: 0.30209 |  0:28:08s\n",
      "epoch 19 | loss: 1.53167 | train_balanced_accuracy: 0.29321 | valid_balanced_accuracy: 0.29657 |  0:29:40s\n",
      "epoch 20 | loss: 1.53522 | train_balanced_accuracy: 0.29118 | valid_balanced_accuracy: 0.28853 |  0:31:11s\n",
      "epoch 21 | loss: 1.52578 | train_balanced_accuracy: 0.29979 | valid_balanced_accuracy: 0.30088 |  0:32:43s\n",
      "epoch 22 | loss: 1.52609 | train_balanced_accuracy: 0.29614 | valid_balanced_accuracy: 0.29438 |  0:34:15s\n",
      "epoch 23 | loss: 1.51837 | train_balanced_accuracy: 0.30662 | valid_balanced_accuracy: 0.30618 |  0:35:46s\n",
      "epoch 24 | loss: 1.51029 | train_balanced_accuracy: 0.2898  | valid_balanced_accuracy: 0.2949  |  0:37:16s\n",
      "epoch 25 | loss: 1.5071  | train_balanced_accuracy: 0.31824 | valid_balanced_accuracy: 0.32431 |  0:38:44s\n",
      "epoch 26 | loss: 1.5013  | train_balanced_accuracy: 0.32114 | valid_balanced_accuracy: 0.31777 |  0:40:12s\n",
      "epoch 27 | loss: 1.49502 | train_balanced_accuracy: 0.31157 | valid_balanced_accuracy: 0.31142 |  0:41:41s\n",
      "epoch 28 | loss: 1.494   | train_balanced_accuracy: 0.3346  | valid_balanced_accuracy: 0.32756 |  0:43:10s\n",
      "epoch 29 | loss: 1.48921 | train_balanced_accuracy: 0.32127 | valid_balanced_accuracy: 0.31478 |  0:44:40s\n",
      "epoch 30 | loss: 1.48475 | train_balanced_accuracy: 0.32752 | valid_balanced_accuracy: 0.31686 |  0:46:09s\n",
      "epoch 31 | loss: 1.48382 | train_balanced_accuracy: 0.33855 | valid_balanced_accuracy: 0.33964 |  0:47:40s\n",
      "epoch 32 | loss: 1.48026 | train_balanced_accuracy: 0.34192 | valid_balanced_accuracy: 0.33457 |  0:49:11s\n",
      "epoch 33 | loss: 1.48699 | train_balanced_accuracy: 0.32868 | valid_balanced_accuracy: 0.33058 |  0:50:42s\n",
      "epoch 34 | loss: 1.4785  | train_balanced_accuracy: 0.352   | valid_balanced_accuracy: 0.34425 |  0:52:14s\n",
      "epoch 35 | loss: 1.47012 | train_balanced_accuracy: 0.34744 | valid_balanced_accuracy: 0.34468 |  0:53:45s\n",
      "epoch 36 | loss: 1.46594 | train_balanced_accuracy: 0.34747 | valid_balanced_accuracy: 0.34369 |  0:55:15s\n",
      "epoch 37 | loss: 1.46113 | train_balanced_accuracy: 0.3327  | valid_balanced_accuracy: 0.33466 |  0:56:45s\n",
      "epoch 38 | loss: 1.45931 | train_balanced_accuracy: 0.3493  | valid_balanced_accuracy: 0.35068 |  0:58:15s\n",
      "epoch 39 | loss: 1.45959 | train_balanced_accuracy: 0.34576 | valid_balanced_accuracy: 0.34771 |  0:59:44s\n",
      "epoch 40 | loss: 1.45481 | train_balanced_accuracy: 0.36225 | valid_balanced_accuracy: 0.36265 |  1:01:13s\n",
      "epoch 41 | loss: 1.45    | train_balanced_accuracy: 0.35439 | valid_balanced_accuracy: 0.35539 |  1:02:43s\n",
      "epoch 42 | loss: 1.44552 | train_balanced_accuracy: 0.35645 | valid_balanced_accuracy: 0.3504  |  1:04:13s\n",
      "epoch 43 | loss: 1.45024 | train_balanced_accuracy: 0.32719 | valid_balanced_accuracy: 0.32505 |  1:05:43s\n",
      "epoch 44 | loss: 1.45493 | train_balanced_accuracy: 0.34724 | valid_balanced_accuracy: 0.3403  |  1:07:11s\n",
      "epoch 45 | loss: 1.45298 | train_balanced_accuracy: 0.36448 | valid_balanced_accuracy: 0.35848 |  1:08:39s\n",
      "epoch 46 | loss: 1.44651 | train_balanced_accuracy: 0.37039 | valid_balanced_accuracy: 0.36478 |  1:10:07s\n",
      "epoch 47 | loss: 1.43654 | train_balanced_accuracy: 0.37504 | valid_balanced_accuracy: 0.37647 |  1:11:36s\n",
      "epoch 48 | loss: 1.44134 | train_balanced_accuracy: 0.36722 | valid_balanced_accuracy: 0.36619 |  1:13:05s\n",
      "epoch 49 | loss: 1.43153 | train_balanced_accuracy: 0.37839 | valid_balanced_accuracy: 0.37068 |  1:14:35s\n",
      "epoch 50 | loss: 1.43087 | train_balanced_accuracy: 0.36657 | valid_balanced_accuracy: 0.35364 |  1:16:04s\n",
      "epoch 51 | loss: 1.43061 | train_balanced_accuracy: 0.35848 | valid_balanced_accuracy: 0.35789 |  1:17:32s\n",
      "epoch 52 | loss: 1.42385 | train_balanced_accuracy: 0.37257 | valid_balanced_accuracy: 0.37075 |  1:19:01s\n",
      "epoch 53 | loss: 1.4213  | train_balanced_accuracy: 0.36644 | valid_balanced_accuracy: 0.36816 |  1:20:33s\n",
      "epoch 54 | loss: 1.42801 | train_balanced_accuracy: 0.35812 | valid_balanced_accuracy: 0.36301 |  1:22:04s\n",
      "epoch 55 | loss: 1.42599 | train_balanced_accuracy: 0.35925 | valid_balanced_accuracy: 0.35741 |  1:23:36s\n",
      "epoch 56 | loss: 1.42218 | train_balanced_accuracy: 0.35135 | valid_balanced_accuracy: 0.34085 |  1:25:09s\n",
      "epoch 57 | loss: 1.41947 | train_balanced_accuracy: 0.3586  | valid_balanced_accuracy: 0.3529  |  1:26:41s\n",
      "\n",
      "Early stopping occured at epoch 57 with best_epoch = 47 and best_valid_balanced_accuracy = 0.37647\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'step_size': 50}, optimizer_params={'lr': 0.001}, n_steps=13, n_shared=1, n_independent=5, n_a=16, momentum=0.005, lambda_sparse=0.0001, clip_value=1.0, total=87.1min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'step_size': 50}, optimizer_params={'lr': 0.001}, n_steps=13, n_shared=1, n_independent=5, n_a=16, momentum=0.005, lambda_sparse=0.0001, clip_value=1.0 \n",
      "epoch 0  | loss: 6.06271 | train_balanced_accuracy: 0.11875 | valid_balanced_accuracy: 0.11711 |  0:01:30s\n",
      "epoch 1  | loss: 2.32534 | train_balanced_accuracy: 0.14675 | valid_balanced_accuracy: 0.14635 |  0:02:58s\n",
      "epoch 2  | loss: 1.92014 | train_balanced_accuracy: 0.18311 | valid_balanced_accuracy: 0.18192 |  0:04:26s\n",
      "epoch 3  | loss: 1.80372 | train_balanced_accuracy: 0.19414 | valid_balanced_accuracy: 0.19318 |  0:05:55s\n",
      "epoch 4  | loss: 1.76317 | train_balanced_accuracy: 0.21034 | valid_balanced_accuracy: 0.21071 |  0:07:23s\n",
      "epoch 5  | loss: 1.72867 | train_balanced_accuracy: 0.21473 | valid_balanced_accuracy: 0.21438 |  0:08:51s\n",
      "epoch 6  | loss: 1.69969 | train_balanced_accuracy: 0.22234 | valid_balanced_accuracy: 0.22683 |  0:10:20s\n",
      "epoch 7  | loss: 1.68525 | train_balanced_accuracy: 0.2162  | valid_balanced_accuracy: 0.21335 |  0:11:48s\n",
      "epoch 8  | loss: 1.68085 | train_balanced_accuracy: 0.19935 | valid_balanced_accuracy: 0.2015  |  0:13:16s\n",
      "epoch 9  | loss: 1.66465 | train_balanced_accuracy: 0.2149  | valid_balanced_accuracy: 0.21829 |  0:14:44s\n",
      "epoch 10 | loss: 1.63431 | train_balanced_accuracy: 0.2327  | valid_balanced_accuracy: 0.22522 |  0:16:13s\n",
      "epoch 11 | loss: 1.61763 | train_balanced_accuracy: 0.24801 | valid_balanced_accuracy: 0.24537 |  0:17:41s\n",
      "epoch 12 | loss: 1.60794 | train_balanced_accuracy: 0.24571 | valid_balanced_accuracy: 0.23935 |  0:19:10s\n",
      "epoch 13 | loss: 1.59686 | train_balanced_accuracy: 0.25079 | valid_balanced_accuracy: 0.2505  |  0:20:38s\n",
      "epoch 14 | loss: 1.58642 | train_balanced_accuracy: 0.25431 | valid_balanced_accuracy: 0.24387 |  0:22:07s\n",
      "epoch 15 | loss: 1.58174 | train_balanced_accuracy: 0.25833 | valid_balanced_accuracy: 0.2469  |  0:23:35s\n",
      "epoch 16 | loss: 1.56831 | train_balanced_accuracy: 0.28142 | valid_balanced_accuracy: 0.27721 |  0:25:04s\n",
      "epoch 17 | loss: 1.57044 | train_balanced_accuracy: 0.27213 | valid_balanced_accuracy: 0.26686 |  0:26:32s\n",
      "epoch 18 | loss: 1.55202 | train_balanced_accuracy: 0.28101 | valid_balanced_accuracy: 0.27182 |  0:28:01s\n",
      "epoch 19 | loss: 1.54156 | train_balanced_accuracy: 0.2922  | valid_balanced_accuracy: 0.2855  |  0:29:29s\n",
      "epoch 20 | loss: 1.52788 | train_balanced_accuracy: 0.30413 | valid_balanced_accuracy: 0.30228 |  0:30:57s\n",
      "epoch 21 | loss: 1.51654 | train_balanced_accuracy: 0.3124  | valid_balanced_accuracy: 0.30979 |  0:32:28s\n",
      "epoch 22 | loss: 1.50371 | train_balanced_accuracy: 0.30373 | valid_balanced_accuracy: 0.30524 |  0:33:57s\n",
      "epoch 23 | loss: 1.50416 | train_balanced_accuracy: 0.32959 | valid_balanced_accuracy: 0.32289 |  0:35:27s\n",
      "epoch 24 | loss: 1.49425 | train_balanced_accuracy: 0.32298 | valid_balanced_accuracy: 0.3194  |  0:36:58s\n",
      "epoch 25 | loss: 1.48867 | train_balanced_accuracy: 0.32249 | valid_balanced_accuracy: 0.31697 |  0:38:29s\n",
      "epoch 26 | loss: 1.48215 | train_balanced_accuracy: 0.32961 | valid_balanced_accuracy: 0.31667 |  0:39:59s\n",
      "epoch 27 | loss: 1.48186 | train_balanced_accuracy: 0.3196  | valid_balanced_accuracy: 0.31384 |  0:41:27s\n",
      "epoch 28 | loss: 1.47551 | train_balanced_accuracy: 0.32501 | valid_balanced_accuracy: 0.32359 |  0:42:55s\n",
      "epoch 29 | loss: 1.46183 | train_balanced_accuracy: 0.3662  | valid_balanced_accuracy: 0.36709 |  0:44:25s\n",
      "epoch 30 | loss: 1.45979 | train_balanced_accuracy: 0.34329 | valid_balanced_accuracy: 0.34382 |  0:45:53s\n",
      "epoch 31 | loss: 1.45075 | train_balanced_accuracy: 0.34615 | valid_balanced_accuracy: 0.33946 |  0:47:22s\n",
      "epoch 32 | loss: 1.44265 | train_balanced_accuracy: 0.33014 | valid_balanced_accuracy: 0.32095 |  0:48:50s\n",
      "epoch 33 | loss: 1.44106 | train_balanced_accuracy: 0.36699 | valid_balanced_accuracy: 0.35459 |  0:50:19s\n",
      "epoch 34 | loss: 1.44031 | train_balanced_accuracy: 0.37929 | valid_balanced_accuracy: 0.37396 |  0:51:47s\n",
      "epoch 35 | loss: 1.43193 | train_balanced_accuracy: 0.38274 | valid_balanced_accuracy: 0.37406 |  0:53:15s\n",
      "epoch 36 | loss: 1.4315  | train_balanced_accuracy: 0.37916 | valid_balanced_accuracy: 0.37805 |  0:54:43s\n",
      "epoch 37 | loss: 1.41614 | train_balanced_accuracy: 0.39529 | valid_balanced_accuracy: 0.38757 |  0:56:12s\n",
      "epoch 38 | loss: 1.41734 | train_balanced_accuracy: 0.36459 | valid_balanced_accuracy: 0.35345 |  0:57:42s\n",
      "epoch 39 | loss: 1.41402 | train_balanced_accuracy: 0.37811 | valid_balanced_accuracy: 0.37297 |  0:59:10s\n",
      "epoch 40 | loss: 1.41127 | train_balanced_accuracy: 0.38468 | valid_balanced_accuracy: 0.37726 |  1:00:39s\n",
      "epoch 41 | loss: 1.39887 | train_balanced_accuracy: 0.38791 | valid_balanced_accuracy: 0.38513 |  1:02:07s\n",
      "epoch 42 | loss: 1.38769 | train_balanced_accuracy: 0.40694 | valid_balanced_accuracy: 0.39879 |  1:03:36s\n",
      "epoch 43 | loss: 1.38367 | train_balanced_accuracy: 0.40405 | valid_balanced_accuracy: 0.40187 |  1:05:06s\n",
      "epoch 44 | loss: 1.38295 | train_balanced_accuracy: 0.38977 | valid_balanced_accuracy: 0.38595 |  1:06:34s\n",
      "epoch 45 | loss: 1.38041 | train_balanced_accuracy: 0.41613 | valid_balanced_accuracy: 0.40971 |  1:08:02s\n",
      "epoch 46 | loss: 1.36912 | train_balanced_accuracy: 0.40685 | valid_balanced_accuracy: 0.39905 |  1:09:32s\n",
      "epoch 47 | loss: 1.36777 | train_balanced_accuracy: 0.40005 | valid_balanced_accuracy: 0.39331 |  1:11:00s\n",
      "epoch 48 | loss: 1.36402 | train_balanced_accuracy: 0.41278 | valid_balanced_accuracy: 0.40197 |  1:12:30s\n",
      "epoch 49 | loss: 1.35582 | train_balanced_accuracy: 0.41812 | valid_balanced_accuracy: 0.41182 |  1:13:59s\n",
      "epoch 50 | loss: 1.35162 | train_balanced_accuracy: 0.41945 | valid_balanced_accuracy: 0.41071 |  1:15:27s\n",
      "epoch 51 | loss: 1.34541 | train_balanced_accuracy: 0.42333 | valid_balanced_accuracy: 0.41613 |  1:16:55s\n",
      "epoch 52 | loss: 1.34281 | train_balanced_accuracy: 0.4311  | valid_balanced_accuracy: 0.42551 |  1:18:24s\n",
      "epoch 53 | loss: 1.34005 | train_balanced_accuracy: 0.42814 | valid_balanced_accuracy: 0.41947 |  1:19:52s\n",
      "epoch 54 | loss: 1.33844 | train_balanced_accuracy: 0.42198 | valid_balanced_accuracy: 0.41063 |  1:21:20s\n",
      "epoch 55 | loss: 1.33343 | train_balanced_accuracy: 0.42886 | valid_balanced_accuracy: 0.41697 |  1:22:49s\n",
      "epoch 56 | loss: 1.33156 | train_balanced_accuracy: 0.42606 | valid_balanced_accuracy: 0.41616 |  1:24:18s\n",
      "epoch 57 | loss: 1.32488 | train_balanced_accuracy: 0.43499 | valid_balanced_accuracy: 0.41955 |  1:25:47s\n",
      "epoch 58 | loss: 1.32663 | train_balanced_accuracy: 0.43788 | valid_balanced_accuracy: 0.42297 |  1:27:15s\n",
      "epoch 59 | loss: 1.32588 | train_balanced_accuracy: 0.42479 | valid_balanced_accuracy: 0.4089  |  1:28:44s\n",
      "epoch 60 | loss: 1.32549 | train_balanced_accuracy: 0.42731 | valid_balanced_accuracy: 0.41758 |  1:30:12s\n",
      "epoch 61 | loss: 1.3234  | train_balanced_accuracy: 0.43919 | valid_balanced_accuracy: 0.43028 |  1:31:40s\n",
      "epoch 62 | loss: 1.319   | train_balanced_accuracy: 0.43826 | valid_balanced_accuracy: 0.42294 |  1:33:09s\n",
      "epoch 63 | loss: 1.31674 | train_balanced_accuracy: 0.44511 | valid_balanced_accuracy: 0.43258 |  1:34:38s\n",
      "epoch 64 | loss: 1.3199  | train_balanced_accuracy: 0.44528 | valid_balanced_accuracy: 0.43414 |  1:36:06s\n",
      "epoch 65 | loss: 1.31539 | train_balanced_accuracy: 0.44883 | valid_balanced_accuracy: 0.43765 |  1:37:34s\n",
      "epoch 66 | loss: 1.31165 | train_balanced_accuracy: 0.45111 | valid_balanced_accuracy: 0.44281 |  1:39:03s\n",
      "epoch 67 | loss: 1.30841 | train_balanced_accuracy: 0.45969 | valid_balanced_accuracy: 0.44798 |  1:40:32s\n",
      "epoch 68 | loss: 1.30483 | train_balanced_accuracy: 0.45277 | valid_balanced_accuracy: 0.43943 |  1:42:01s\n",
      "epoch 69 | loss: 1.30055 | train_balanced_accuracy: 0.4575  | valid_balanced_accuracy: 0.44865 |  1:43:29s\n",
      "epoch 70 | loss: 1.29559 | train_balanced_accuracy: 0.46625 | valid_balanced_accuracy: 0.4597  |  1:44:58s\n",
      "epoch 71 | loss: 1.29673 | train_balanced_accuracy: 0.46137 | valid_balanced_accuracy: 0.45176 |  1:46:27s\n",
      "epoch 72 | loss: 1.29229 | train_balanced_accuracy: 0.46042 | valid_balanced_accuracy: 0.45015 |  1:47:56s\n",
      "epoch 73 | loss: 1.29046 | train_balanced_accuracy: 0.47127 | valid_balanced_accuracy: 0.45657 |  1:49:24s\n",
      "epoch 74 | loss: 1.29156 | train_balanced_accuracy: 0.45452 | valid_balanced_accuracy: 0.44416 |  1:50:53s\n",
      "epoch 75 | loss: 1.28911 | train_balanced_accuracy: 0.47237 | valid_balanced_accuracy: 0.45842 |  1:52:23s\n",
      "epoch 76 | loss: 1.28438 | train_balanced_accuracy: 0.47088 | valid_balanced_accuracy: 0.46258 |  1:53:53s\n",
      "epoch 77 | loss: 1.28294 | train_balanced_accuracy: 0.46138 | valid_balanced_accuracy: 0.45574 |  1:55:23s\n",
      "epoch 78 | loss: 1.28182 | train_balanced_accuracy: 0.47171 | valid_balanced_accuracy: 0.46068 |  1:56:51s\n",
      "epoch 79 | loss: 1.28292 | train_balanced_accuracy: 0.46075 | valid_balanced_accuracy: 0.45092 |  1:58:19s\n",
      "epoch 80 | loss: 1.27995 | train_balanced_accuracy: 0.47313 | valid_balanced_accuracy: 0.47011 |  1:59:48s\n",
      "epoch 81 | loss: 1.28185 | train_balanced_accuracy: 0.46889 | valid_balanced_accuracy: 0.45565 |  2:01:16s\n",
      "epoch 82 | loss: 1.28312 | train_balanced_accuracy: 0.48481 | valid_balanced_accuracy: 0.46921 |  2:02:45s\n",
      "epoch 83 | loss: 1.27685 | train_balanced_accuracy: 0.47245 | valid_balanced_accuracy: 0.46582 |  2:04:14s\n",
      "epoch 84 | loss: 1.27667 | train_balanced_accuracy: 0.47277 | valid_balanced_accuracy: 0.46977 |  2:05:43s\n",
      "epoch 85 | loss: 1.27164 | train_balanced_accuracy: 0.47794 | valid_balanced_accuracy: 0.47721 |  2:07:11s\n",
      "epoch 86 | loss: 1.27353 | train_balanced_accuracy: 0.47463 | valid_balanced_accuracy: 0.4738  |  2:08:40s\n",
      "epoch 87 | loss: 1.27239 | train_balanced_accuracy: 0.47548 | valid_balanced_accuracy: 0.46433 |  2:10:11s\n",
      "epoch 88 | loss: 1.27241 | train_balanced_accuracy: 0.46701 | valid_balanced_accuracy: 0.45294 |  2:11:41s\n",
      "epoch 89 | loss: 1.27391 | train_balanced_accuracy: 0.49332 | valid_balanced_accuracy: 0.48574 |  2:13:11s\n",
      "epoch 90 | loss: 1.27164 | train_balanced_accuracy: 0.48713 | valid_balanced_accuracy: 0.47616 |  2:14:41s\n",
      "epoch 91 | loss: 1.27155 | train_balanced_accuracy: 0.4885  | valid_balanced_accuracy: 0.48004 |  2:16:11s\n",
      "epoch 92 | loss: 1.26803 | train_balanced_accuracy: 0.49228 | valid_balanced_accuracy: 0.48308 |  2:17:42s\n",
      "epoch 93 | loss: 1.26471 | train_balanced_accuracy: 0.49792 | valid_balanced_accuracy: 0.4857  |  2:19:12s\n",
      "epoch 94 | loss: 1.26401 | train_balanced_accuracy: 0.48551 | valid_balanced_accuracy: 0.46977 |  2:20:42s\n",
      "epoch 95 | loss: 1.2633  | train_balanced_accuracy: 0.48181 | valid_balanced_accuracy: 0.46616 |  2:22:12s\n",
      "epoch 96 | loss: 1.26539 | train_balanced_accuracy: 0.48968 | valid_balanced_accuracy: 0.47478 |  2:23:42s\n",
      "epoch 97 | loss: 1.26587 | train_balanced_accuracy: 0.49418 | valid_balanced_accuracy: 0.47866 |  2:25:13s\n",
      "epoch 98 | loss: 1.26153 | train_balanced_accuracy: 0.48799 | valid_balanced_accuracy: 0.46963 |  2:26:43s\n",
      "epoch 99 | loss: 1.25924 | train_balanced_accuracy: 0.49093 | valid_balanced_accuracy: 0.47919 |  2:28:13s\n",
      "\n",
      "Early stopping occured at epoch 99 with best_epoch = 89 and best_valid_balanced_accuracy = 0.48574\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'step_size': 50}, optimizer_params={'lr': 0.001}, n_steps=13, n_shared=1, n_independent=5, n_a=16, momentum=0.005, lambda_sparse=0.0001, clip_value=1.0, total=148.7min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'step_size': 50}, optimizer_params={'lr': 0.001}, n_steps=13, n_shared=1, n_independent=5, n_a=16, momentum=0.005, lambda_sparse=0.0001, clip_value=1.0 \n",
      "epoch 0  | loss: 5.94099 | train_balanced_accuracy: 0.12213 | valid_balanced_accuracy: 0.11986 |  0:01:29s\n",
      "epoch 1  | loss: 2.34972 | train_balanced_accuracy: 0.15448 | valid_balanced_accuracy: 0.15442 |  0:02:59s\n",
      "epoch 2  | loss: 1.96937 | train_balanced_accuracy: 0.17814 | valid_balanced_accuracy: 0.17774 |  0:04:30s\n",
      "epoch 3  | loss: 1.81079 | train_balanced_accuracy: 0.21307 | valid_balanced_accuracy: 0.21001 |  0:06:00s\n",
      "epoch 4  | loss: 1.74047 | train_balanced_accuracy: 0.21389 | valid_balanced_accuracy: 0.21628 |  0:07:30s\n",
      "epoch 5  | loss: 1.70811 | train_balanced_accuracy: 0.22439 | valid_balanced_accuracy: 0.22907 |  0:09:00s\n",
      "epoch 6  | loss: 1.67839 | train_balanced_accuracy: 0.23023 | valid_balanced_accuracy: 0.22758 |  0:10:30s\n",
      "epoch 7  | loss: 1.65268 | train_balanced_accuracy: 0.2353  | valid_balanced_accuracy: 0.23436 |  0:12:00s\n",
      "epoch 8  | loss: 1.63522 | train_balanced_accuracy: 0.21925 | valid_balanced_accuracy: 0.22125 |  0:13:30s\n",
      "epoch 9  | loss: 1.6269  | train_balanced_accuracy: 0.22245 | valid_balanced_accuracy: 0.22105 |  0:15:00s\n",
      "epoch 10 | loss: 1.6198  | train_balanced_accuracy: 0.23965 | valid_balanced_accuracy: 0.2418  |  0:16:30s\n",
      "epoch 11 | loss: 1.59844 | train_balanced_accuracy: 0.23914 | valid_balanced_accuracy: 0.23438 |  0:18:00s\n",
      "epoch 12 | loss: 1.57701 | train_balanced_accuracy: 0.24365 | valid_balanced_accuracy: 0.24216 |  0:19:30s\n",
      "epoch 13 | loss: 1.57186 | train_balanced_accuracy: 0.25653 | valid_balanced_accuracy: 0.25133 |  0:21:00s\n",
      "epoch 14 | loss: 1.56789 | train_balanced_accuracy: 0.26433 | valid_balanced_accuracy: 0.26413 |  0:22:30s\n",
      "epoch 15 | loss: 1.55451 | train_balanced_accuracy: 0.26351 | valid_balanced_accuracy: 0.26631 |  0:24:00s\n",
      "epoch 16 | loss: 1.54929 | train_balanced_accuracy: 0.29089 | valid_balanced_accuracy: 0.28668 |  0:25:30s\n",
      "epoch 17 | loss: 1.5432  | train_balanced_accuracy: 0.27602 | valid_balanced_accuracy: 0.27285 |  0:26:58s\n",
      "epoch 18 | loss: 1.53569 | train_balanced_accuracy: 0.27469 | valid_balanced_accuracy: 0.26928 |  0:28:27s\n",
      "epoch 19 | loss: 1.52833 | train_balanced_accuracy: 0.28308 | valid_balanced_accuracy: 0.2783  |  0:29:56s\n",
      "epoch 20 | loss: 1.51764 | train_balanced_accuracy: 0.30486 | valid_balanced_accuracy: 0.30301 |  0:31:24s\n",
      "epoch 21 | loss: 1.51653 | train_balanced_accuracy: 0.28698 | valid_balanced_accuracy: 0.29258 |  0:32:51s\n",
      "epoch 22 | loss: 1.52317 | train_balanced_accuracy: 0.28405 | valid_balanced_accuracy: 0.29523 |  0:34:19s\n",
      "epoch 23 | loss: 1.51451 | train_balanced_accuracy: 0.30765 | valid_balanced_accuracy: 0.3083  |  0:35:47s\n",
      "epoch 24 | loss: 1.51672 | train_balanced_accuracy: 0.31222 | valid_balanced_accuracy: 0.31018 |  0:37:15s\n",
      "epoch 25 | loss: 1.52073 | train_balanced_accuracy: 0.29669 | valid_balanced_accuracy: 0.29974 |  0:38:44s\n",
      "epoch 26 | loss: 1.51506 | train_balanced_accuracy: 0.29382 | valid_balanced_accuracy: 0.29689 |  0:40:12s\n",
      "epoch 27 | loss: 1.50521 | train_balanced_accuracy: 0.30255 | valid_balanced_accuracy: 0.29927 |  0:41:40s\n",
      "epoch 28 | loss: 1.506   | train_balanced_accuracy: 0.29758 | valid_balanced_accuracy: 0.29295 |  0:43:09s\n",
      "epoch 29 | loss: 1.51194 | train_balanced_accuracy: 0.30652 | valid_balanced_accuracy: 0.30533 |  0:44:37s\n",
      "epoch 30 | loss: 1.51042 | train_balanced_accuracy: 0.30697 | valid_balanced_accuracy: 0.30464 |  0:46:05s\n",
      "epoch 31 | loss: 1.50409 | train_balanced_accuracy: 0.29496 | valid_balanced_accuracy: 0.29061 |  0:47:33s\n",
      "epoch 32 | loss: 1.5025  | train_balanced_accuracy: 0.32121 | valid_balanced_accuracy: 0.32736 |  0:49:02s\n",
      "epoch 33 | loss: 1.49777 | train_balanced_accuracy: 0.29616 | valid_balanced_accuracy: 0.30397 |  0:50:30s\n",
      "epoch 34 | loss: 1.49746 | train_balanced_accuracy: 0.31547 | valid_balanced_accuracy: 0.31106 |  0:51:59s\n",
      "epoch 35 | loss: 1.49659 | train_balanced_accuracy: 0.31458 | valid_balanced_accuracy: 0.32222 |  0:53:27s\n",
      "epoch 36 | loss: 1.49585 | train_balanced_accuracy: 0.3031  | valid_balanced_accuracy: 0.30217 |  0:54:55s\n",
      "epoch 37 | loss: 1.48992 | train_balanced_accuracy: 0.3236  | valid_balanced_accuracy: 0.32184 |  0:56:24s\n",
      "epoch 38 | loss: 1.48948 | train_balanced_accuracy: 0.32084 | valid_balanced_accuracy: 0.31818 |  0:57:52s\n",
      "epoch 39 | loss: 1.48766 | train_balanced_accuracy: 0.30073 | valid_balanced_accuracy: 0.3061  |  0:59:20s\n",
      "epoch 40 | loss: 1.4855  | train_balanced_accuracy: 0.33469 | valid_balanced_accuracy: 0.33341 |  1:00:48s\n",
      "epoch 41 | loss: 1.47997 | train_balanced_accuracy: 0.33456 | valid_balanced_accuracy: 0.33454 |  1:02:16s\n",
      "epoch 42 | loss: 1.4752  | train_balanced_accuracy: 0.33865 | valid_balanced_accuracy: 0.34018 |  1:03:44s\n",
      "epoch 43 | loss: 1.46597 | train_balanced_accuracy: 0.33795 | valid_balanced_accuracy: 0.33649 |  1:05:12s\n",
      "epoch 44 | loss: 1.46423 | train_balanced_accuracy: 0.36086 | valid_balanced_accuracy: 0.35177 |  1:06:40s\n",
      "epoch 45 | loss: 1.46049 | train_balanced_accuracy: 0.36305 | valid_balanced_accuracy: 0.35822 |  1:08:08s\n",
      "epoch 46 | loss: 1.4536  | train_balanced_accuracy: 0.35308 | valid_balanced_accuracy: 0.33856 |  1:09:36s\n",
      "epoch 47 | loss: 1.45438 | train_balanced_accuracy: 0.33045 | valid_balanced_accuracy: 0.32316 |  1:11:04s\n",
      "epoch 48 | loss: 1.45446 | train_balanced_accuracy: 0.34962 | valid_balanced_accuracy: 0.34155 |  1:12:32s\n",
      "epoch 49 | loss: 1.44922 | train_balanced_accuracy: 0.36712 | valid_balanced_accuracy: 0.3597  |  1:14:00s\n",
      "epoch 50 | loss: 1.44842 | train_balanced_accuracy: 0.35847 | valid_balanced_accuracy: 0.35592 |  1:15:28s\n",
      "epoch 51 | loss: 1.44842 | train_balanced_accuracy: 0.34487 | valid_balanced_accuracy: 0.3325  |  1:16:56s\n",
      "epoch 52 | loss: 1.45389 | train_balanced_accuracy: 0.34675 | valid_balanced_accuracy: 0.34442 |  1:18:24s\n",
      "epoch 53 | loss: 1.45297 | train_balanced_accuracy: 0.33918 | valid_balanced_accuracy: 0.32611 |  1:19:52s\n",
      "epoch 54 | loss: 1.44379 | train_balanced_accuracy: 0.35466 | valid_balanced_accuracy: 0.33799 |  1:21:20s\n",
      "epoch 55 | loss: 1.4387  | train_balanced_accuracy: 0.34957 | valid_balanced_accuracy: 0.34349 |  1:22:48s\n",
      "epoch 56 | loss: 1.44117 | train_balanced_accuracy: 0.36601 | valid_balanced_accuracy: 0.36298 |  1:24:15s\n",
      "epoch 57 | loss: 1.43778 | train_balanced_accuracy: 0.36487 | valid_balanced_accuracy: 0.35499 |  1:25:43s\n",
      "epoch 58 | loss: 1.44801 | train_balanced_accuracy: 0.37004 | valid_balanced_accuracy: 0.36825 |  1:27:11s\n",
      "epoch 59 | loss: 1.43863 | train_balanced_accuracy: 0.36418 | valid_balanced_accuracy: 0.36483 |  1:28:40s\n",
      "epoch 60 | loss: 1.43681 | train_balanced_accuracy: 0.37355 | valid_balanced_accuracy: 0.36586 |  1:30:08s\n",
      "epoch 61 | loss: 1.43622 | train_balanced_accuracy: 0.36088 | valid_balanced_accuracy: 0.35896 |  1:31:37s\n",
      "epoch 62 | loss: 1.43568 | train_balanced_accuracy: 0.37517 | valid_balanced_accuracy: 0.37168 |  1:33:05s\n",
      "epoch 63 | loss: 1.43113 | train_balanced_accuracy: 0.3639  | valid_balanced_accuracy: 0.3649  |  1:34:33s\n",
      "epoch 64 | loss: 1.43094 | train_balanced_accuracy: 0.37559 | valid_balanced_accuracy: 0.3645  |  1:36:02s\n",
      "epoch 65 | loss: 1.42411 | train_balanced_accuracy: 0.3639  | valid_balanced_accuracy: 0.35653 |  1:37:30s\n",
      "epoch 66 | loss: 1.42393 | train_balanced_accuracy: 0.38097 | valid_balanced_accuracy: 0.38692 |  1:38:59s\n",
      "epoch 67 | loss: 1.42447 | train_balanced_accuracy: 0.37953 | valid_balanced_accuracy: 0.37481 |  1:40:27s\n",
      "epoch 68 | loss: 1.42358 | train_balanced_accuracy: 0.38903 | valid_balanced_accuracy: 0.38471 |  1:41:56s\n",
      "epoch 69 | loss: 1.42077 | train_balanced_accuracy: 0.40378 | valid_balanced_accuracy: 0.39926 |  1:43:24s\n",
      "epoch 70 | loss: 1.41887 | train_balanced_accuracy: 0.38711 | valid_balanced_accuracy: 0.38702 |  1:44:52s\n",
      "epoch 71 | loss: 1.41758 | train_balanced_accuracy: 0.39881 | valid_balanced_accuracy: 0.39401 |  1:46:21s\n",
      "epoch 72 | loss: 1.41358 | train_balanced_accuracy: 0.40402 | valid_balanced_accuracy: 0.39802 |  1:47:49s\n",
      "epoch 73 | loss: 1.42044 | train_balanced_accuracy: 0.38617 | valid_balanced_accuracy: 0.38104 |  1:49:17s\n",
      "epoch 74 | loss: 1.4188  | train_balanced_accuracy: 0.39558 | valid_balanced_accuracy: 0.38619 |  1:50:45s\n",
      "epoch 75 | loss: 1.41251 | train_balanced_accuracy: 0.38658 | valid_balanced_accuracy: 0.37564 |  1:52:13s\n",
      "epoch 76 | loss: 1.41452 | train_balanced_accuracy: 0.39169 | valid_balanced_accuracy: 0.38142 |  1:53:42s\n",
      "epoch 77 | loss: 1.4115  | train_balanced_accuracy: 0.40438 | valid_balanced_accuracy: 0.39804 |  1:55:11s\n",
      "epoch 78 | loss: 1.41579 | train_balanced_accuracy: 0.39888 | valid_balanced_accuracy: 0.39262 |  1:56:40s\n",
      "epoch 79 | loss: 1.41063 | train_balanced_accuracy: 0.40002 | valid_balanced_accuracy: 0.39875 |  1:58:09s\n",
      "\n",
      "Early stopping occured at epoch 79 with best_epoch = 69 and best_valid_balanced_accuracy = 0.39926\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'step_size': 50}, optimizer_params={'lr': 0.001}, n_steps=13, n_shared=1, n_independent=5, n_a=16, momentum=0.005, lambda_sparse=0.0001, clip_value=1.0, total=118.6min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'gamma': 1.8}, optimizer_params={'lr': 0.0001}, n_steps=3, n_shared=5, n_independent=1, n_a=16, momentum=0.02, lambda_sparse=0.001, clip_value=1.0 \n",
      "epoch 0  | loss: 3.97357 | train_balanced_accuracy: 0.04595 | valid_balanced_accuracy: 0.04682 |  0:00:27s\n",
      "epoch 1  | loss: 3.46719 | train_balanced_accuracy: 0.06795 | valid_balanced_accuracy: 0.06722 |  0:00:55s\n",
      "epoch 2  | loss: 3.13226 | train_balanced_accuracy: 0.08756 | valid_balanced_accuracy: 0.08631 |  0:01:22s\n",
      "epoch 3  | loss: 2.88806 | train_balanced_accuracy: 0.0844  | valid_balanced_accuracy: 0.08397 |  0:01:49s\n",
      "epoch 4  | loss: 2.68341 | train_balanced_accuracy: 0.08896 | valid_balanced_accuracy: 0.0898  |  0:02:16s\n",
      "epoch 5  | loss: 2.52405 | train_balanced_accuracy: 0.09685 | valid_balanced_accuracy: 0.09654 |  0:02:42s\n",
      "epoch 6  | loss: 2.38484 | train_balanced_accuracy: 0.10883 | valid_balanced_accuracy: 0.10885 |  0:03:10s\n",
      "epoch 7  | loss: 2.28158 | train_balanced_accuracy: 0.1096  | valid_balanced_accuracy: 0.1075  |  0:03:37s\n",
      "epoch 8  | loss: 2.20134 | train_balanced_accuracy: 0.11707 | valid_balanced_accuracy: 0.11678 |  0:04:05s\n",
      "epoch 9  | loss: 2.12996 | train_balanced_accuracy: 0.12446 | valid_balanced_accuracy: 0.12418 |  0:04:31s\n",
      "epoch 10 | loss: 2.06884 | train_balanced_accuracy: 0.13006 | valid_balanced_accuracy: 0.13028 |  0:04:58s\n",
      "epoch 11 | loss: 2.02026 | train_balanced_accuracy: 0.13632 | valid_balanced_accuracy: 0.13691 |  0:05:25s\n",
      "epoch 12 | loss: 1.97617 | train_balanced_accuracy: 0.14509 | valid_balanced_accuracy: 0.14513 |  0:05:52s\n",
      "epoch 13 | loss: 1.93859 | train_balanced_accuracy: 0.15044 | valid_balanced_accuracy: 0.15021 |  0:06:19s\n",
      "epoch 14 | loss: 1.90524 | train_balanced_accuracy: 0.1488  | valid_balanced_accuracy: 0.14876 |  0:06:46s\n",
      "epoch 15 | loss: 1.87589 | train_balanced_accuracy: 0.15922 | valid_balanced_accuracy: 0.15839 |  0:07:13s\n",
      "epoch 16 | loss: 1.84935 | train_balanced_accuracy: 0.15621 | valid_balanced_accuracy: 0.15618 |  0:07:40s\n",
      "epoch 17 | loss: 1.82608 | train_balanced_accuracy: 0.16392 | valid_balanced_accuracy: 0.16171 |  0:08:08s\n",
      "epoch 18 | loss: 1.80398 | train_balanced_accuracy: 0.16166 | valid_balanced_accuracy: 0.1608  |  0:08:36s\n",
      "epoch 19 | loss: 1.78655 | train_balanced_accuracy: 0.16422 | valid_balanced_accuracy: 0.16394 |  0:09:03s\n",
      "epoch 20 | loss: 1.76746 | train_balanced_accuracy: 0.17649 | valid_balanced_accuracy: 0.17646 |  0:09:31s\n",
      "epoch 21 | loss: 1.75307 | train_balanced_accuracy: 0.18777 | valid_balanced_accuracy: 0.18858 |  0:09:59s\n",
      "epoch 22 | loss: 1.74071 | train_balanced_accuracy: 0.19622 | valid_balanced_accuracy: 0.19778 |  0:10:26s\n",
      "epoch 23 | loss: 1.72707 | train_balanced_accuracy: 0.19649 | valid_balanced_accuracy: 0.19735 |  0:10:54s\n",
      "epoch 24 | loss: 1.71679 | train_balanced_accuracy: 0.19844 | valid_balanced_accuracy: 0.19863 |  0:11:21s\n",
      "epoch 25 | loss: 1.70728 | train_balanced_accuracy: 0.20107 | valid_balanced_accuracy: 0.20224 |  0:11:49s\n",
      "epoch 26 | loss: 1.69823 | train_balanced_accuracy: 0.20416 | valid_balanced_accuracy: 0.2058  |  0:12:16s\n",
      "epoch 27 | loss: 1.69019 | train_balanced_accuracy: 0.20827 | valid_balanced_accuracy: 0.20929 |  0:12:44s\n",
      "epoch 28 | loss: 1.68005 | train_balanced_accuracy: 0.20909 | valid_balanced_accuracy: 0.20942 |  0:13:12s\n",
      "epoch 29 | loss: 1.67507 | train_balanced_accuracy: 0.20517 | valid_balanced_accuracy: 0.20643 |  0:13:39s\n",
      "epoch 30 | loss: 1.66805 | train_balanced_accuracy: 0.2061  | valid_balanced_accuracy: 0.20631 |  0:14:07s\n",
      "epoch 31 | loss: 1.66162 | train_balanced_accuracy: 0.21489 | valid_balanced_accuracy: 0.21562 |  0:14:35s\n",
      "epoch 32 | loss: 1.65485 | train_balanced_accuracy: 0.21281 | valid_balanced_accuracy: 0.2136  |  0:15:02s\n",
      "epoch 33 | loss: 1.64977 | train_balanced_accuracy: 0.21355 | valid_balanced_accuracy: 0.21462 |  0:15:30s\n",
      "epoch 34 | loss: 1.64495 | train_balanced_accuracy: 0.2171  | valid_balanced_accuracy: 0.21818 |  0:15:57s\n",
      "epoch 35 | loss: 1.63763 | train_balanced_accuracy: 0.21939 | valid_balanced_accuracy: 0.22104 |  0:16:25s\n",
      "epoch 36 | loss: 1.6321  | train_balanced_accuracy: 0.21725 | valid_balanced_accuracy: 0.21911 |  0:16:52s\n",
      "epoch 37 | loss: 1.62729 | train_balanced_accuracy: 0.22117 | valid_balanced_accuracy: 0.22312 |  0:17:20s\n",
      "epoch 38 | loss: 1.62357 | train_balanced_accuracy: 0.22047 | valid_balanced_accuracy: 0.2223  |  0:17:48s\n",
      "epoch 39 | loss: 1.61873 | train_balanced_accuracy: 0.22365 | valid_balanced_accuracy: 0.22502 |  0:18:15s\n",
      "epoch 40 | loss: 1.6139  | train_balanced_accuracy: 0.2246  | valid_balanced_accuracy: 0.22571 |  0:18:43s\n",
      "epoch 41 | loss: 1.60987 | train_balanced_accuracy: 0.22807 | valid_balanced_accuracy: 0.22891 |  0:19:10s\n",
      "epoch 42 | loss: 1.60651 | train_balanced_accuracy: 0.23    | valid_balanced_accuracy: 0.23069 |  0:19:38s\n",
      "epoch 43 | loss: 1.60144 | train_balanced_accuracy: 0.23417 | valid_balanced_accuracy: 0.23389 |  0:20:05s\n",
      "epoch 44 | loss: 1.59796 | train_balanced_accuracy: 0.23425 | valid_balanced_accuracy: 0.23474 |  0:20:33s\n",
      "epoch 45 | loss: 1.59424 | train_balanced_accuracy: 0.23477 | valid_balanced_accuracy: 0.23343 |  0:21:00s\n",
      "epoch 46 | loss: 1.59036 | train_balanced_accuracy: 0.23574 | valid_balanced_accuracy: 0.23533 |  0:21:28s\n",
      "epoch 47 | loss: 1.58711 | train_balanced_accuracy: 0.23676 | valid_balanced_accuracy: 0.23526 |  0:21:55s\n",
      "epoch 48 | loss: 1.58453 | train_balanced_accuracy: 0.23644 | valid_balanced_accuracy: 0.23574 |  0:22:22s\n",
      "epoch 49 | loss: 1.58049 | train_balanced_accuracy: 0.23948 | valid_balanced_accuracy: 0.23797 |  0:22:50s\n",
      "epoch 50 | loss: 1.57803 | train_balanced_accuracy: 0.2452  | valid_balanced_accuracy: 0.24413 |  0:23:18s\n",
      "epoch 51 | loss: 1.5734  | train_balanced_accuracy: 0.24258 | valid_balanced_accuracy: 0.24152 |  0:23:45s\n",
      "epoch 52 | loss: 1.56969 | train_balanced_accuracy: 0.24338 | valid_balanced_accuracy: 0.24311 |  0:24:12s\n",
      "epoch 53 | loss: 1.56741 | train_balanced_accuracy: 0.24737 | valid_balanced_accuracy: 0.24568 |  0:24:40s\n",
      "epoch 54 | loss: 1.56427 | train_balanced_accuracy: 0.24683 | valid_balanced_accuracy: 0.24736 |  0:25:07s\n",
      "epoch 55 | loss: 1.56104 | train_balanced_accuracy: 0.24367 | valid_balanced_accuracy: 0.24407 |  0:25:35s\n",
      "epoch 56 | loss: 1.55965 | train_balanced_accuracy: 0.24821 | valid_balanced_accuracy: 0.24967 |  0:26:02s\n",
      "epoch 57 | loss: 1.55585 | train_balanced_accuracy: 0.24793 | valid_balanced_accuracy: 0.24823 |  0:26:29s\n",
      "epoch 58 | loss: 1.55521 | train_balanced_accuracy: 0.24959 | valid_balanced_accuracy: 0.24989 |  0:26:56s\n",
      "epoch 59 | loss: 1.55107 | train_balanced_accuracy: 0.25109 | valid_balanced_accuracy: 0.25149 |  0:27:23s\n",
      "epoch 60 | loss: 1.54979 | train_balanced_accuracy: 0.2538  | valid_balanced_accuracy: 0.25389 |  0:27:50s\n",
      "epoch 61 | loss: 1.54548 | train_balanced_accuracy: 0.25296 | valid_balanced_accuracy: 0.25468 |  0:28:16s\n",
      "epoch 62 | loss: 1.54311 | train_balanced_accuracy: 0.24991 | valid_balanced_accuracy: 0.25001 |  0:28:43s\n",
      "epoch 63 | loss: 1.54227 | train_balanced_accuracy: 0.25665 | valid_balanced_accuracy: 0.25619 |  0:29:10s\n",
      "epoch 64 | loss: 1.53942 | train_balanced_accuracy: 0.25295 | valid_balanced_accuracy: 0.25257 |  0:29:37s\n",
      "epoch 65 | loss: 1.53723 | train_balanced_accuracy: 0.2569  | valid_balanced_accuracy: 0.25735 |  0:30:04s\n",
      "epoch 66 | loss: 1.53359 | train_balanced_accuracy: 0.25533 | valid_balanced_accuracy: 0.25566 |  0:30:32s\n",
      "epoch 67 | loss: 1.53362 | train_balanced_accuracy: 0.25835 | valid_balanced_accuracy: 0.25913 |  0:30:59s\n",
      "epoch 68 | loss: 1.53057 | train_balanced_accuracy: 0.2579  | valid_balanced_accuracy: 0.25908 |  0:31:26s\n",
      "epoch 69 | loss: 1.52755 | train_balanced_accuracy: 0.2581  | valid_balanced_accuracy: 0.25958 |  0:31:52s\n",
      "epoch 70 | loss: 1.52644 | train_balanced_accuracy: 0.25989 | valid_balanced_accuracy: 0.26292 |  0:32:19s\n",
      "epoch 71 | loss: 1.52367 | train_balanced_accuracy: 0.26059 | valid_balanced_accuracy: 0.26183 |  0:32:46s\n",
      "epoch 72 | loss: 1.52245 | train_balanced_accuracy: 0.26066 | valid_balanced_accuracy: 0.26278 |  0:33:12s\n",
      "epoch 73 | loss: 1.51957 | train_balanced_accuracy: 0.2565  | valid_balanced_accuracy: 0.25746 |  0:33:39s\n",
      "epoch 74 | loss: 1.51786 | train_balanced_accuracy: 0.26379 | valid_balanced_accuracy: 0.26494 |  0:34:05s\n",
      "epoch 75 | loss: 1.51624 | train_balanced_accuracy: 0.26088 | valid_balanced_accuracy: 0.26253 |  0:34:32s\n",
      "epoch 76 | loss: 1.51343 | train_balanced_accuracy: 0.26574 | valid_balanced_accuracy: 0.26766 |  0:34:58s\n",
      "epoch 77 | loss: 1.51298 | train_balanced_accuracy: 0.26598 | valid_balanced_accuracy: 0.26667 |  0:35:25s\n",
      "epoch 78 | loss: 1.51098 | train_balanced_accuracy: 0.27206 | valid_balanced_accuracy: 0.27284 |  0:35:52s\n",
      "epoch 79 | loss: 1.50857 | train_balanced_accuracy: 0.26635 | valid_balanced_accuracy: 0.26852 |  0:36:19s\n",
      "epoch 80 | loss: 1.50761 | train_balanced_accuracy: 0.26924 | valid_balanced_accuracy: 0.26996 |  0:36:45s\n",
      "epoch 81 | loss: 1.50604 | train_balanced_accuracy: 0.27271 | valid_balanced_accuracy: 0.27171 |  0:37:12s\n",
      "epoch 82 | loss: 1.50495 | train_balanced_accuracy: 0.26921 | valid_balanced_accuracy: 0.26887 |  0:37:38s\n",
      "epoch 83 | loss: 1.50344 | train_balanced_accuracy: 0.27456 | valid_balanced_accuracy: 0.27251 |  0:38:05s\n",
      "epoch 84 | loss: 1.50132 | train_balanced_accuracy: 0.27447 | valid_balanced_accuracy: 0.27311 |  0:38:32s\n",
      "epoch 85 | loss: 1.49961 | train_balanced_accuracy: 0.27839 | valid_balanced_accuracy: 0.27712 |  0:38:58s\n",
      "epoch 86 | loss: 1.49805 | train_balanced_accuracy: 0.27732 | valid_balanced_accuracy: 0.27346 |  0:39:25s\n",
      "epoch 87 | loss: 1.49677 | train_balanced_accuracy: 0.27466 | valid_balanced_accuracy: 0.27371 |  0:39:51s\n",
      "epoch 88 | loss: 1.49531 | train_balanced_accuracy: 0.27812 | valid_balanced_accuracy: 0.2765  |  0:40:18s\n",
      "epoch 89 | loss: 1.49184 | train_balanced_accuracy: 0.2803  | valid_balanced_accuracy: 0.27624 |  0:40:44s\n",
      "epoch 90 | loss: 1.49337 | train_balanced_accuracy: 0.28501 | valid_balanced_accuracy: 0.28173 |  0:41:10s\n",
      "epoch 91 | loss: 1.49107 | train_balanced_accuracy: 0.2829  | valid_balanced_accuracy: 0.27939 |  0:41:37s\n",
      "epoch 92 | loss: 1.48895 | train_balanced_accuracy: 0.28528 | valid_balanced_accuracy: 0.28122 |  0:42:04s\n",
      "epoch 93 | loss: 1.48756 | train_balanced_accuracy: 0.28856 | valid_balanced_accuracy: 0.28405 |  0:42:30s\n",
      "epoch 94 | loss: 1.48535 | train_balanced_accuracy: 0.2889  | valid_balanced_accuracy: 0.28591 |  0:42:58s\n",
      "epoch 95 | loss: 1.48455 | train_balanced_accuracy: 0.2887  | valid_balanced_accuracy: 0.28541 |  0:43:25s\n",
      "epoch 96 | loss: 1.48441 | train_balanced_accuracy: 0.28991 | valid_balanced_accuracy: 0.28769 |  0:43:53s\n",
      "epoch 97 | loss: 1.48261 | train_balanced_accuracy: 0.29198 | valid_balanced_accuracy: 0.28798 |  0:44:20s\n",
      "epoch 98 | loss: 1.48263 | train_balanced_accuracy: 0.29606 | valid_balanced_accuracy: 0.29447 |  0:44:47s\n",
      "epoch 99 | loss: 1.48003 | train_balanced_accuracy: 0.30061 | valid_balanced_accuracy: 0.29713 |  0:45:13s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_valid_balanced_accuracy = 0.29713\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'gamma': 1.8}, optimizer_params={'lr': 0.0001}, n_steps=3, n_shared=5, n_independent=1, n_a=16, momentum=0.02, lambda_sparse=0.001, clip_value=1.0, total=45.4min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'gamma': 1.8}, optimizer_params={'lr': 0.0001}, n_steps=3, n_shared=5, n_independent=1, n_a=16, momentum=0.02, lambda_sparse=0.001, clip_value=1.0 \n",
      "epoch 0  | loss: 3.96492 | train_balanced_accuracy: 0.04875 | valid_balanced_accuracy: 0.05046 |  0:00:26s\n",
      "epoch 1  | loss: 3.45565 | train_balanced_accuracy: 0.06379 | valid_balanced_accuracy: 0.06268 |  0:00:53s\n",
      "epoch 2  | loss: 3.1185  | train_balanced_accuracy: 0.08905 | valid_balanced_accuracy: 0.08971 |  0:01:19s\n",
      "epoch 3  | loss: 2.87903 | train_balanced_accuracy: 0.08755 | valid_balanced_accuracy: 0.08734 |  0:01:46s\n",
      "epoch 4  | loss: 2.68289 | train_balanced_accuracy: 0.09074 | valid_balanced_accuracy: 0.08944 |  0:02:13s\n",
      "epoch 5  | loss: 2.53203 | train_balanced_accuracy: 0.09086 | valid_balanced_accuracy: 0.09056 |  0:02:40s\n",
      "epoch 6  | loss: 2.408   | train_balanced_accuracy: 0.09718 | valid_balanced_accuracy: 0.097   |  0:03:07s\n",
      "epoch 7  | loss: 2.30338 | train_balanced_accuracy: 0.10403 | valid_balanced_accuracy: 0.10321 |  0:03:34s\n",
      "epoch 8  | loss: 2.20813 | train_balanced_accuracy: 0.12165 | valid_balanced_accuracy: 0.11989 |  0:04:00s\n",
      "epoch 9  | loss: 2.12907 | train_balanced_accuracy: 0.12811 | valid_balanced_accuracy: 0.12602 |  0:04:28s\n",
      "epoch 10 | loss: 2.06042 | train_balanced_accuracy: 0.12916 | valid_balanced_accuracy: 0.12805 |  0:04:55s\n",
      "epoch 11 | loss: 2.00392 | train_balanced_accuracy: 0.14204 | valid_balanced_accuracy: 0.14092 |  0:05:23s\n",
      "epoch 12 | loss: 1.95883 | train_balanced_accuracy: 0.14898 | valid_balanced_accuracy: 0.14764 |  0:05:50s\n",
      "epoch 13 | loss: 1.92004 | train_balanced_accuracy: 0.14861 | valid_balanced_accuracy: 0.14847 |  0:06:17s\n",
      "epoch 14 | loss: 1.88796 | train_balanced_accuracy: 0.14858 | valid_balanced_accuracy: 0.14835 |  0:06:44s\n",
      "epoch 15 | loss: 1.86091 | train_balanced_accuracy: 0.15488 | valid_balanced_accuracy: 0.15447 |  0:07:11s\n",
      "epoch 16 | loss: 1.83524 | train_balanced_accuracy: 0.15913 | valid_balanced_accuracy: 0.15786 |  0:07:38s\n",
      "epoch 17 | loss: 1.81345 | train_balanced_accuracy: 0.16154 | valid_balanced_accuracy: 0.16139 |  0:08:06s\n",
      "epoch 18 | loss: 1.79382 | train_balanced_accuracy: 0.16745 | valid_balanced_accuracy: 0.1676  |  0:08:33s\n",
      "epoch 19 | loss: 1.77792 | train_balanced_accuracy: 0.17619 | valid_balanced_accuracy: 0.17486 |  0:08:59s\n",
      "epoch 20 | loss: 1.76155 | train_balanced_accuracy: 0.18147 | valid_balanced_accuracy: 0.17959 |  0:09:26s\n",
      "epoch 21 | loss: 1.74769 | train_balanced_accuracy: 0.18727 | valid_balanced_accuracy: 0.18584 |  0:09:53s\n",
      "epoch 22 | loss: 1.73549 | train_balanced_accuracy: 0.19443 | valid_balanced_accuracy: 0.19294 |  0:10:20s\n",
      "epoch 23 | loss: 1.72513 | train_balanced_accuracy: 0.19487 | valid_balanced_accuracy: 0.19455 |  0:10:47s\n",
      "epoch 24 | loss: 1.71457 | train_balanced_accuracy: 0.1956  | valid_balanced_accuracy: 0.19551 |  0:11:14s\n",
      "epoch 25 | loss: 1.70586 | train_balanced_accuracy: 0.19389 | valid_balanced_accuracy: 0.19344 |  0:11:41s\n",
      "epoch 26 | loss: 1.6992  | train_balanced_accuracy: 0.19625 | valid_balanced_accuracy: 0.19647 |  0:12:07s\n",
      "epoch 27 | loss: 1.68907 | train_balanced_accuracy: 0.19597 | valid_balanced_accuracy: 0.1964  |  0:12:34s\n",
      "epoch 28 | loss: 1.68226 | train_balanced_accuracy: 0.20596 | valid_balanced_accuracy: 0.20602 |  0:13:02s\n",
      "epoch 29 | loss: 1.67526 | train_balanced_accuracy: 0.20545 | valid_balanced_accuracy: 0.20552 |  0:13:29s\n",
      "epoch 30 | loss: 1.66993 | train_balanced_accuracy: 0.20569 | valid_balanced_accuracy: 0.20704 |  0:13:56s\n",
      "epoch 31 | loss: 1.66293 | train_balanced_accuracy: 0.20754 | valid_balanced_accuracy: 0.20819 |  0:14:23s\n",
      "epoch 32 | loss: 1.65568 | train_balanced_accuracy: 0.20619 | valid_balanced_accuracy: 0.20882 |  0:14:49s\n",
      "epoch 33 | loss: 1.65118 | train_balanced_accuracy: 0.21026 | valid_balanced_accuracy: 0.21198 |  0:15:16s\n",
      "epoch 34 | loss: 1.6465  | train_balanced_accuracy: 0.21132 | valid_balanced_accuracy: 0.21391 |  0:15:42s\n",
      "epoch 35 | loss: 1.64119 | train_balanced_accuracy: 0.21379 | valid_balanced_accuracy: 0.21415 |  0:16:09s\n",
      "epoch 36 | loss: 1.63535 | train_balanced_accuracy: 0.21902 | valid_balanced_accuracy: 0.2214  |  0:16:35s\n",
      "epoch 37 | loss: 1.63025 | train_balanced_accuracy: 0.22024 | valid_balanced_accuracy: 0.22409 |  0:17:02s\n",
      "epoch 38 | loss: 1.62648 | train_balanced_accuracy: 0.22137 | valid_balanced_accuracy: 0.22376 |  0:17:28s\n",
      "epoch 39 | loss: 1.6234  | train_balanced_accuracy: 0.21937 | valid_balanced_accuracy: 0.22214 |  0:17:54s\n",
      "epoch 40 | loss: 1.61774 | train_balanced_accuracy: 0.21769 | valid_balanced_accuracy: 0.21945 |  0:18:21s\n",
      "epoch 41 | loss: 1.61228 | train_balanced_accuracy: 0.2181  | valid_balanced_accuracy: 0.22091 |  0:18:47s\n",
      "epoch 42 | loss: 1.60919 | train_balanced_accuracy: 0.22189 | valid_balanced_accuracy: 0.22517 |  0:19:14s\n",
      "epoch 43 | loss: 1.60519 | train_balanced_accuracy: 0.22443 | valid_balanced_accuracy: 0.22665 |  0:19:40s\n",
      "epoch 44 | loss: 1.59983 | train_balanced_accuracy: 0.22563 | valid_balanced_accuracy: 0.22816 |  0:20:07s\n",
      "epoch 45 | loss: 1.59675 | train_balanced_accuracy: 0.23034 | valid_balanced_accuracy: 0.23273 |  0:20:34s\n",
      "epoch 46 | loss: 1.59472 | train_balanced_accuracy: 0.23135 | valid_balanced_accuracy: 0.23373 |  0:21:02s\n",
      "epoch 47 | loss: 1.59083 | train_balanced_accuracy: 0.22625 | valid_balanced_accuracy: 0.22815 |  0:21:29s\n",
      "epoch 48 | loss: 1.58564 | train_balanced_accuracy: 0.23754 | valid_balanced_accuracy: 0.23895 |  0:21:56s\n",
      "epoch 49 | loss: 1.58203 | train_balanced_accuracy: 0.23484 | valid_balanced_accuracy: 0.23512 |  0:22:22s\n",
      "epoch 50 | loss: 1.57882 | train_balanced_accuracy: 0.23796 | valid_balanced_accuracy: 0.23795 |  0:22:49s\n",
      "epoch 51 | loss: 1.57646 | train_balanced_accuracy: 0.23875 | valid_balanced_accuracy: 0.23929 |  0:23:16s\n",
      "epoch 52 | loss: 1.57304 | train_balanced_accuracy: 0.23825 | valid_balanced_accuracy: 0.2381  |  0:23:42s\n",
      "epoch 53 | loss: 1.57026 | train_balanced_accuracy: 0.2427  | valid_balanced_accuracy: 0.24209 |  0:24:09s\n",
      "epoch 54 | loss: 1.56495 | train_balanced_accuracy: 0.24038 | valid_balanced_accuracy: 0.23971 |  0:24:35s\n",
      "epoch 55 | loss: 1.56155 | train_balanced_accuracy: 0.24255 | valid_balanced_accuracy: 0.24204 |  0:25:02s\n",
      "epoch 56 | loss: 1.56159 | train_balanced_accuracy: 0.24283 | valid_balanced_accuracy: 0.24216 |  0:25:29s\n",
      "epoch 57 | loss: 1.55796 | train_balanced_accuracy: 0.24572 | valid_balanced_accuracy: 0.24619 |  0:25:56s\n",
      "epoch 58 | loss: 1.55356 | train_balanced_accuracy: 0.24316 | valid_balanced_accuracy: 0.24362 |  0:26:24s\n",
      "epoch 59 | loss: 1.54988 | train_balanced_accuracy: 0.24652 | valid_balanced_accuracy: 0.24554 |  0:26:51s\n",
      "epoch 60 | loss: 1.55052 | train_balanced_accuracy: 0.24644 | valid_balanced_accuracy: 0.24631 |  0:27:19s\n",
      "epoch 61 | loss: 1.54635 | train_balanced_accuracy: 0.24751 | valid_balanced_accuracy: 0.24624 |  0:27:46s\n",
      "epoch 62 | loss: 1.54232 | train_balanced_accuracy: 0.24767 | valid_balanced_accuracy: 0.24705 |  0:28:13s\n",
      "epoch 63 | loss: 1.54072 | train_balanced_accuracy: 0.2524  | valid_balanced_accuracy: 0.25294 |  0:28:40s\n",
      "epoch 64 | loss: 1.53987 | train_balanced_accuracy: 0.25041 | valid_balanced_accuracy: 0.25124 |  0:29:07s\n",
      "epoch 65 | loss: 1.53576 | train_balanced_accuracy: 0.25533 | valid_balanced_accuracy: 0.25608 |  0:29:33s\n",
      "epoch 66 | loss: 1.53261 | train_balanced_accuracy: 0.25621 | valid_balanced_accuracy: 0.25747 |  0:30:00s\n",
      "epoch 67 | loss: 1.53186 | train_balanced_accuracy: 0.25866 | valid_balanced_accuracy: 0.25942 |  0:30:27s\n",
      "epoch 68 | loss: 1.53007 | train_balanced_accuracy: 0.25671 | valid_balanced_accuracy: 0.25781 |  0:30:53s\n",
      "epoch 69 | loss: 1.52686 | train_balanced_accuracy: 0.25889 | valid_balanced_accuracy: 0.25889 |  0:31:20s\n",
      "epoch 70 | loss: 1.52537 | train_balanced_accuracy: 0.25682 | valid_balanced_accuracy: 0.25795 |  0:31:47s\n",
      "epoch 71 | loss: 1.52317 | train_balanced_accuracy: 0.25574 | valid_balanced_accuracy: 0.25415 |  0:32:13s\n",
      "epoch 72 | loss: 1.52066 | train_balanced_accuracy: 0.26023 | valid_balanced_accuracy: 0.26112 |  0:32:40s\n",
      "epoch 73 | loss: 1.51748 | train_balanced_accuracy: 0.25891 | valid_balanced_accuracy: 0.25946 |  0:33:07s\n",
      "epoch 74 | loss: 1.5168  | train_balanced_accuracy: 0.26212 | valid_balanced_accuracy: 0.26169 |  0:33:34s\n",
      "epoch 75 | loss: 1.51525 | train_balanced_accuracy: 0.26048 | valid_balanced_accuracy: 0.2606  |  0:34:02s\n",
      "epoch 76 | loss: 1.51176 | train_balanced_accuracy: 0.2659  | valid_balanced_accuracy: 0.26685 |  0:34:29s\n",
      "epoch 77 | loss: 1.51149 | train_balanced_accuracy: 0.26563 | valid_balanced_accuracy: 0.26525 |  0:34:56s\n",
      "epoch 78 | loss: 1.50912 | train_balanced_accuracy: 0.26844 | valid_balanced_accuracy: 0.26812 |  0:35:22s\n",
      "epoch 79 | loss: 1.50457 | train_balanced_accuracy: 0.26859 | valid_balanced_accuracy: 0.26829 |  0:35:49s\n",
      "epoch 80 | loss: 1.50439 | train_balanced_accuracy: 0.26961 | valid_balanced_accuracy: 0.26902 |  0:36:16s\n",
      "epoch 81 | loss: 1.50225 | train_balanced_accuracy: 0.26768 | valid_balanced_accuracy: 0.26786 |  0:36:42s\n",
      "epoch 82 | loss: 1.50166 | train_balanced_accuracy: 0.26922 | valid_balanced_accuracy: 0.2691  |  0:37:09s\n",
      "epoch 83 | loss: 1.50081 | train_balanced_accuracy: 0.27    | valid_balanced_accuracy: 0.26915 |  0:37:35s\n",
      "epoch 84 | loss: 1.49878 | train_balanced_accuracy: 0.27278 | valid_balanced_accuracy: 0.27095 |  0:38:02s\n",
      "epoch 85 | loss: 1.49697 | train_balanced_accuracy: 0.27513 | valid_balanced_accuracy: 0.2741  |  0:38:28s\n",
      "epoch 86 | loss: 1.49647 | train_balanced_accuracy: 0.27229 | valid_balanced_accuracy: 0.27126 |  0:38:55s\n",
      "epoch 87 | loss: 1.49546 | train_balanced_accuracy: 0.27025 | valid_balanced_accuracy: 0.26893 |  0:39:21s\n",
      "epoch 88 | loss: 1.49097 | train_balanced_accuracy: 0.27075 | valid_balanced_accuracy: 0.26997 |  0:39:48s\n",
      "epoch 89 | loss: 1.49048 | train_balanced_accuracy: 0.27556 | valid_balanced_accuracy: 0.27416 |  0:40:14s\n",
      "epoch 90 | loss: 1.48941 | train_balanced_accuracy: 0.27368 | valid_balanced_accuracy: 0.27059 |  0:40:40s\n",
      "epoch 91 | loss: 1.48826 | train_balanced_accuracy: 0.27346 | valid_balanced_accuracy: 0.27191 |  0:41:06s\n",
      "epoch 92 | loss: 1.4866  | train_balanced_accuracy: 0.2769  | valid_balanced_accuracy: 0.27482 |  0:41:32s\n",
      "epoch 93 | loss: 1.48613 | train_balanced_accuracy: 0.27539 | valid_balanced_accuracy: 0.27393 |  0:41:59s\n",
      "epoch 94 | loss: 1.48359 | train_balanced_accuracy: 0.27707 | valid_balanced_accuracy: 0.27442 |  0:42:25s\n",
      "epoch 95 | loss: 1.48145 | train_balanced_accuracy: 0.27697 | valid_balanced_accuracy: 0.27596 |  0:42:52s\n",
      "epoch 96 | loss: 1.48097 | train_balanced_accuracy: 0.27827 | valid_balanced_accuracy: 0.27709 |  0:43:19s\n",
      "epoch 97 | loss: 1.47923 | train_balanced_accuracy: 0.27897 | valid_balanced_accuracy: 0.27698 |  0:43:47s\n",
      "epoch 98 | loss: 1.48029 | train_balanced_accuracy: 0.28025 | valid_balanced_accuracy: 0.28019 |  0:44:14s\n",
      "epoch 99 | loss: 1.47758 | train_balanced_accuracy: 0.2768  | valid_balanced_accuracy: 0.27508 |  0:44:42s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 98 and best_valid_balanced_accuracy = 0.28019\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'gamma': 1.8}, optimizer_params={'lr': 0.0001}, n_steps=3, n_shared=5, n_independent=1, n_a=16, momentum=0.02, lambda_sparse=0.001, clip_value=1.0, total=44.9min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'gamma': 1.8}, optimizer_params={'lr': 0.0001}, n_steps=3, n_shared=5, n_independent=1, n_a=16, momentum=0.02, lambda_sparse=0.001, clip_value=1.0 \n",
      "epoch 0  | loss: 3.97008 | train_balanced_accuracy: 0.04645 | valid_balanced_accuracy: 0.04816 |  0:00:27s\n",
      "epoch 1  | loss: 3.45398 | train_balanced_accuracy: 0.06011 | valid_balanced_accuracy: 0.05924 |  0:00:54s\n",
      "epoch 2  | loss: 3.13594 | train_balanced_accuracy: 0.07371 | valid_balanced_accuracy: 0.07425 |  0:01:21s\n",
      "epoch 3  | loss: 2.89561 | train_balanced_accuracy: 0.08151 | valid_balanced_accuracy: 0.08157 |  0:01:48s\n",
      "epoch 4  | loss: 2.69834 | train_balanced_accuracy: 0.08917 | valid_balanced_accuracy: 0.08835 |  0:02:16s\n",
      "epoch 5  | loss: 2.53572 | train_balanced_accuracy: 0.10476 | valid_balanced_accuracy: 0.10574 |  0:02:43s\n",
      "epoch 6  | loss: 2.40057 | train_balanced_accuracy: 0.10695 | valid_balanced_accuracy: 0.10584 |  0:03:10s\n",
      "epoch 7  | loss: 2.29563 | train_balanced_accuracy: 0.10849 | valid_balanced_accuracy: 0.10971 |  0:03:38s\n",
      "epoch 8  | loss: 2.21209 | train_balanced_accuracy: 0.10558 | valid_balanced_accuracy: 0.10713 |  0:04:05s\n",
      "epoch 9  | loss: 2.14727 | train_balanced_accuracy: 0.10861 | valid_balanced_accuracy: 0.11039 |  0:04:33s\n",
      "epoch 10 | loss: 2.08587 | train_balanced_accuracy: 0.12376 | valid_balanced_accuracy: 0.12537 |  0:05:00s\n",
      "epoch 11 | loss: 2.03743 | train_balanced_accuracy: 0.12805 | valid_balanced_accuracy: 0.12889 |  0:05:28s\n",
      "epoch 12 | loss: 1.99298 | train_balanced_accuracy: 0.13639 | valid_balanced_accuracy: 0.13622 |  0:05:55s\n",
      "epoch 13 | loss: 1.95099 | train_balanced_accuracy: 0.14064 | valid_balanced_accuracy: 0.14001 |  0:06:22s\n",
      "epoch 14 | loss: 1.91536 | train_balanced_accuracy: 0.14595 | valid_balanced_accuracy: 0.14609 |  0:06:50s\n",
      "epoch 15 | loss: 1.88326 | train_balanced_accuracy: 0.15601 | valid_balanced_accuracy: 0.15591 |  0:07:17s\n",
      "epoch 16 | loss: 1.85614 | train_balanced_accuracy: 0.15532 | valid_balanced_accuracy: 0.15552 |  0:07:44s\n",
      "epoch 17 | loss: 1.83603 | train_balanced_accuracy: 0.15979 | valid_balanced_accuracy: 0.15936 |  0:08:11s\n",
      "epoch 18 | loss: 1.81464 | train_balanced_accuracy: 0.16087 | valid_balanced_accuracy: 0.16017 |  0:08:38s\n",
      "epoch 19 | loss: 1.79658 | train_balanced_accuracy: 0.16159 | valid_balanced_accuracy: 0.16163 |  0:09:04s\n",
      "epoch 20 | loss: 1.78109 | train_balanced_accuracy: 0.16536 | valid_balanced_accuracy: 0.16429 |  0:09:31s\n",
      "epoch 21 | loss: 1.76904 | train_balanced_accuracy: 0.17377 | valid_balanced_accuracy: 0.17253 |  0:09:58s\n",
      "epoch 22 | loss: 1.75408 | train_balanced_accuracy: 0.17221 | valid_balanced_accuracy: 0.17135 |  0:10:25s\n",
      "epoch 23 | loss: 1.7436  | train_balanced_accuracy: 0.18204 | valid_balanced_accuracy: 0.18052 |  0:10:51s\n",
      "epoch 24 | loss: 1.73224 | train_balanced_accuracy: 0.17512 | valid_balanced_accuracy: 0.17472 |  0:11:18s\n",
      "epoch 25 | loss: 1.72129 | train_balanced_accuracy: 0.18787 | valid_balanced_accuracy: 0.18754 |  0:11:45s\n",
      "epoch 26 | loss: 1.71303 | train_balanced_accuracy: 0.19446 | valid_balanced_accuracy: 0.19371 |  0:12:12s\n",
      "epoch 27 | loss: 1.70287 | train_balanced_accuracy: 0.19097 | valid_balanced_accuracy: 0.18986 |  0:12:38s\n",
      "epoch 28 | loss: 1.69551 | train_balanced_accuracy: 0.19545 | valid_balanced_accuracy: 0.1955  |  0:13:05s\n",
      "epoch 29 | loss: 1.68744 | train_balanced_accuracy: 0.19186 | valid_balanced_accuracy: 0.19131 |  0:13:32s\n",
      "epoch 30 | loss: 1.68171 | train_balanced_accuracy: 0.19712 | valid_balanced_accuracy: 0.19715 |  0:13:58s\n",
      "epoch 31 | loss: 1.67323 | train_balanced_accuracy: 0.20513 | valid_balanced_accuracy: 0.20589 |  0:14:25s\n",
      "epoch 32 | loss: 1.66741 | train_balanced_accuracy: 0.20231 | valid_balanced_accuracy: 0.20216 |  0:14:52s\n",
      "epoch 33 | loss: 1.66278 | train_balanced_accuracy: 0.20431 | valid_balanced_accuracy: 0.20318 |  0:15:19s\n",
      "epoch 34 | loss: 1.65667 | train_balanced_accuracy: 0.21228 | valid_balanced_accuracy: 0.21044 |  0:15:45s\n",
      "epoch 35 | loss: 1.65126 | train_balanced_accuracy: 0.21225 | valid_balanced_accuracy: 0.21094 |  0:16:12s\n",
      "epoch 36 | loss: 1.64527 | train_balanced_accuracy: 0.2142  | valid_balanced_accuracy: 0.21335 |  0:16:39s\n",
      "epoch 37 | loss: 1.64168 | train_balanced_accuracy: 0.21691 | valid_balanced_accuracy: 0.21709 |  0:17:05s\n",
      "epoch 38 | loss: 1.63745 | train_balanced_accuracy: 0.21744 | valid_balanced_accuracy: 0.2168  |  0:17:32s\n",
      "epoch 39 | loss: 1.63229 | train_balanced_accuracy: 0.21612 | valid_balanced_accuracy: 0.21609 |  0:17:59s\n",
      "epoch 40 | loss: 1.62868 | train_balanced_accuracy: 0.2169  | valid_balanced_accuracy: 0.21643 |  0:18:26s\n",
      "epoch 41 | loss: 1.62375 | train_balanced_accuracy: 0.21638 | valid_balanced_accuracy: 0.21571 |  0:18:53s\n",
      "epoch 42 | loss: 1.6206  | train_balanced_accuracy: 0.22259 | valid_balanced_accuracy: 0.22301 |  0:19:20s\n",
      "epoch 43 | loss: 1.61658 | train_balanced_accuracy: 0.22157 | valid_balanced_accuracy: 0.22225 |  0:19:47s\n",
      "epoch 44 | loss: 1.61315 | train_balanced_accuracy: 0.22158 | valid_balanced_accuracy: 0.22169 |  0:20:14s\n",
      "epoch 45 | loss: 1.60899 | train_balanced_accuracy: 0.22408 | valid_balanced_accuracy: 0.22493 |  0:20:41s\n",
      "epoch 46 | loss: 1.60703 | train_balanced_accuracy: 0.22611 | valid_balanced_accuracy: 0.22566 |  0:21:08s\n",
      "epoch 47 | loss: 1.60334 | train_balanced_accuracy: 0.22258 | valid_balanced_accuracy: 0.22135 |  0:21:35s\n",
      "epoch 48 | loss: 1.5996  | train_balanced_accuracy: 0.22678 | valid_balanced_accuracy: 0.22711 |  0:22:03s\n",
      "epoch 49 | loss: 1.59617 | train_balanced_accuracy: 0.22725 | valid_balanced_accuracy: 0.22757 |  0:22:30s\n",
      "epoch 50 | loss: 1.59339 | train_balanced_accuracy: 0.2304  | valid_balanced_accuracy: 0.23101 |  0:22:57s\n",
      "epoch 51 | loss: 1.59007 | train_balanced_accuracy: 0.22672 | valid_balanced_accuracy: 0.22768 |  0:23:24s\n",
      "epoch 52 | loss: 1.58676 | train_balanced_accuracy: 0.2349  | valid_balanced_accuracy: 0.23707 |  0:23:52s\n",
      "epoch 53 | loss: 1.58358 | train_balanced_accuracy: 0.23652 | valid_balanced_accuracy: 0.23709 |  0:24:19s\n",
      "epoch 54 | loss: 1.58046 | train_balanced_accuracy: 0.2322  | valid_balanced_accuracy: 0.23225 |  0:24:46s\n",
      "epoch 55 | loss: 1.57806 | train_balanced_accuracy: 0.23566 | valid_balanced_accuracy: 0.23614 |  0:25:13s\n",
      "epoch 56 | loss: 1.57501 | train_balanced_accuracy: 0.23435 | valid_balanced_accuracy: 0.23687 |  0:25:40s\n",
      "epoch 57 | loss: 1.57195 | train_balanced_accuracy: 0.23969 | valid_balanced_accuracy: 0.24036 |  0:26:07s\n",
      "epoch 58 | loss: 1.56966 | train_balanced_accuracy: 0.23964 | valid_balanced_accuracy: 0.24061 |  0:26:34s\n",
      "epoch 59 | loss: 1.56482 | train_balanced_accuracy: 0.23979 | valid_balanced_accuracy: 0.24061 |  0:27:01s\n",
      "epoch 60 | loss: 1.56508 | train_balanced_accuracy: 0.24004 | valid_balanced_accuracy: 0.2417  |  0:27:29s\n",
      "epoch 61 | loss: 1.56131 | train_balanced_accuracy: 0.24101 | valid_balanced_accuracy: 0.24081 |  0:27:56s\n",
      "epoch 62 | loss: 1.55695 | train_balanced_accuracy: 0.24323 | valid_balanced_accuracy: 0.24406 |  0:28:23s\n",
      "epoch 63 | loss: 1.55619 | train_balanced_accuracy: 0.24404 | valid_balanced_accuracy: 0.24552 |  0:28:50s\n",
      "epoch 64 | loss: 1.55289 | train_balanced_accuracy: 0.24486 | valid_balanced_accuracy: 0.24489 |  0:29:17s\n",
      "epoch 65 | loss: 1.54924 | train_balanced_accuracy: 0.24897 | valid_balanced_accuracy: 0.24941 |  0:29:44s\n",
      "epoch 66 | loss: 1.54737 | train_balanced_accuracy: 0.24673 | valid_balanced_accuracy: 0.24867 |  0:30:11s\n",
      "epoch 67 | loss: 1.54642 | train_balanced_accuracy: 0.25098 | valid_balanced_accuracy: 0.25219 |  0:30:38s\n",
      "epoch 68 | loss: 1.54263 | train_balanced_accuracy: 0.24865 | valid_balanced_accuracy: 0.24919 |  0:31:04s\n",
      "epoch 69 | loss: 1.54024 | train_balanced_accuracy: 0.25439 | valid_balanced_accuracy: 0.25512 |  0:31:31s\n",
      "epoch 70 | loss: 1.53853 | train_balanced_accuracy: 0.25139 | valid_balanced_accuracy: 0.25121 |  0:31:57s\n",
      "epoch 71 | loss: 1.53703 | train_balanced_accuracy: 0.25539 | valid_balanced_accuracy: 0.25614 |  0:32:24s\n",
      "epoch 72 | loss: 1.53473 | train_balanced_accuracy: 0.25746 | valid_balanced_accuracy: 0.25816 |  0:32:51s\n",
      "epoch 73 | loss: 1.53067 | train_balanced_accuracy: 0.25123 | valid_balanced_accuracy: 0.2528  |  0:33:17s\n",
      "epoch 74 | loss: 1.52932 | train_balanced_accuracy: 0.25889 | valid_balanced_accuracy: 0.25782 |  0:33:44s\n",
      "epoch 75 | loss: 1.52837 | train_balanced_accuracy: 0.25692 | valid_balanced_accuracy: 0.25798 |  0:34:12s\n",
      "epoch 76 | loss: 1.52413 | train_balanced_accuracy: 0.25949 | valid_balanced_accuracy: 0.25876 |  0:34:39s\n",
      "epoch 77 | loss: 1.52225 | train_balanced_accuracy: 0.26113 | valid_balanced_accuracy: 0.26108 |  0:35:06s\n",
      "epoch 78 | loss: 1.52122 | train_balanced_accuracy: 0.26596 | valid_balanced_accuracy: 0.26589 |  0:35:34s\n",
      "epoch 79 | loss: 1.51847 | train_balanced_accuracy: 0.26508 | valid_balanced_accuracy: 0.2646  |  0:36:01s\n",
      "epoch 80 | loss: 1.51663 | train_balanced_accuracy: 0.26637 | valid_balanced_accuracy: 0.26613 |  0:36:29s\n",
      "epoch 81 | loss: 1.51357 | train_balanced_accuracy: 0.26612 | valid_balanced_accuracy: 0.26451 |  0:36:56s\n",
      "epoch 82 | loss: 1.51315 | train_balanced_accuracy: 0.26702 | valid_balanced_accuracy: 0.26524 |  0:37:23s\n",
      "epoch 83 | loss: 1.51179 | train_balanced_accuracy: 0.26529 | valid_balanced_accuracy: 0.26574 |  0:37:50s\n",
      "epoch 84 | loss: 1.50868 | train_balanced_accuracy: 0.26639 | valid_balanced_accuracy: 0.26669 |  0:38:17s\n",
      "epoch 85 | loss: 1.50929 | train_balanced_accuracy: 0.26773 | valid_balanced_accuracy: 0.26615 |  0:38:44s\n",
      "epoch 86 | loss: 1.50671 | train_balanced_accuracy: 0.27025 | valid_balanced_accuracy: 0.27026 |  0:39:12s\n",
      "epoch 87 | loss: 1.50465 | train_balanced_accuracy: 0.27225 | valid_balanced_accuracy: 0.2746  |  0:39:38s\n",
      "epoch 88 | loss: 1.50197 | train_balanced_accuracy: 0.2709  | valid_balanced_accuracy: 0.27169 |  0:40:05s\n",
      "epoch 89 | loss: 1.50241 | train_balanced_accuracy: 0.27361 | valid_balanced_accuracy: 0.273   |  0:40:33s\n",
      "epoch 90 | loss: 1.49941 | train_balanced_accuracy: 0.27276 | valid_balanced_accuracy: 0.2721  |  0:41:00s\n",
      "epoch 91 | loss: 1.49743 | train_balanced_accuracy: 0.27451 | valid_balanced_accuracy: 0.27418 |  0:41:27s\n",
      "epoch 92 | loss: 1.49762 | train_balanced_accuracy: 0.27268 | valid_balanced_accuracy: 0.27255 |  0:41:54s\n",
      "epoch 93 | loss: 1.49533 | train_balanced_accuracy: 0.27969 | valid_balanced_accuracy: 0.27802 |  0:42:21s\n",
      "epoch 94 | loss: 1.49409 | train_balanced_accuracy: 0.27809 | valid_balanced_accuracy: 0.27781 |  0:42:48s\n",
      "epoch 95 | loss: 1.49301 | train_balanced_accuracy: 0.27701 | valid_balanced_accuracy: 0.27618 |  0:43:15s\n",
      "epoch 96 | loss: 1.49194 | train_balanced_accuracy: 0.27763 | valid_balanced_accuracy: 0.2781  |  0:43:42s\n",
      "epoch 97 | loss: 1.49075 | train_balanced_accuracy: 0.28111 | valid_balanced_accuracy: 0.28014 |  0:44:09s\n",
      "epoch 98 | loss: 1.49064 | train_balanced_accuracy: 0.27968 | valid_balanced_accuracy: 0.27876 |  0:44:36s\n",
      "epoch 99 | loss: 1.48738 | train_balanced_accuracy: 0.27977 | valid_balanced_accuracy: 0.27729 |  0:45:03s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 97 and best_valid_balanced_accuracy = 0.28014\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'gamma': 1.8}, optimizer_params={'lr': 0.0001}, n_steps=3, n_shared=5, n_independent=1, n_a=16, momentum=0.02, lambda_sparse=0.001, clip_value=1.0, total=45.2min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'gamma': 1.8}, optimizer_params={'lr': 0.0001}, n_steps=3, n_shared=5, n_independent=1, n_a=16, momentum=0.02, lambda_sparse=0.001, clip_value=1.0 \n",
      "epoch 0  | loss: 3.97018 | train_balanced_accuracy: 0.04957 | valid_balanced_accuracy: 0.05037 |  0:00:27s\n",
      "epoch 1  | loss: 3.46081 | train_balanced_accuracy: 0.06728 | valid_balanced_accuracy: 0.06838 |  0:00:54s\n",
      "epoch 2  | loss: 3.12895 | train_balanced_accuracy: 0.07089 | valid_balanced_accuracy: 0.07205 |  0:01:21s\n",
      "epoch 3  | loss: 2.88426 | train_balanced_accuracy: 0.07674 | valid_balanced_accuracy: 0.07488 |  0:01:48s\n",
      "epoch 4  | loss: 2.6842  | train_balanced_accuracy: 0.08194 | valid_balanced_accuracy: 0.08131 |  0:02:15s\n",
      "epoch 5  | loss: 2.52756 | train_balanced_accuracy: 0.09936 | valid_balanced_accuracy: 0.09912 |  0:02:42s\n",
      "epoch 6  | loss: 2.39637 | train_balanced_accuracy: 0.09949 | valid_balanced_accuracy: 0.09832 |  0:03:09s\n",
      "epoch 7  | loss: 2.29398 | train_balanced_accuracy: 0.10636 | valid_balanced_accuracy: 0.10583 |  0:03:37s\n",
      "epoch 8  | loss: 2.20661 | train_balanced_accuracy: 0.11197 | valid_balanced_accuracy: 0.11225 |  0:04:04s\n",
      "epoch 9  | loss: 2.13635 | train_balanced_accuracy: 0.1204  | valid_balanced_accuracy: 0.12023 |  0:04:31s\n",
      "epoch 10 | loss: 2.07384 | train_balanced_accuracy: 0.12846 | valid_balanced_accuracy: 0.12982 |  0:04:58s\n",
      "epoch 11 | loss: 2.02351 | train_balanced_accuracy: 0.13216 | valid_balanced_accuracy: 0.13318 |  0:05:25s\n",
      "epoch 12 | loss: 1.97825 | train_balanced_accuracy: 0.13701 | valid_balanced_accuracy: 0.13812 |  0:05:52s\n",
      "epoch 13 | loss: 1.93721 | train_balanced_accuracy: 0.14571 | valid_balanced_accuracy: 0.1462  |  0:06:19s\n",
      "epoch 14 | loss: 1.90208 | train_balanced_accuracy: 0.15177 | valid_balanced_accuracy: 0.1526  |  0:06:46s\n",
      "epoch 15 | loss: 1.87156 | train_balanced_accuracy: 0.15382 | valid_balanced_accuracy: 0.15415 |  0:07:13s\n",
      "epoch 16 | loss: 1.84388 | train_balanced_accuracy: 0.15867 | valid_balanced_accuracy: 0.15787 |  0:07:40s\n",
      "epoch 17 | loss: 1.81948 | train_balanced_accuracy: 0.16369 | valid_balanced_accuracy: 0.16449 |  0:08:07s\n",
      "epoch 18 | loss: 1.79757 | train_balanced_accuracy: 0.17166 | valid_balanced_accuracy: 0.17336 |  0:08:35s\n",
      "epoch 19 | loss: 1.77927 | train_balanced_accuracy: 0.17817 | valid_balanced_accuracy: 0.17927 |  0:09:02s\n",
      "epoch 20 | loss: 1.76376 | train_balanced_accuracy: 0.17547 | valid_balanced_accuracy: 0.17709 |  0:09:29s\n",
      "epoch 21 | loss: 1.75109 | train_balanced_accuracy: 0.18937 | valid_balanced_accuracy: 0.1894  |  0:09:56s\n",
      "epoch 22 | loss: 1.73741 | train_balanced_accuracy: 0.19244 | valid_balanced_accuracy: 0.19328 |  0:10:23s\n",
      "epoch 23 | loss: 1.72863 | train_balanced_accuracy: 0.18947 | valid_balanced_accuracy: 0.18998 |  0:10:50s\n",
      "epoch 24 | loss: 1.71811 | train_balanced_accuracy: 0.19701 | valid_balanced_accuracy: 0.19744 |  0:11:17s\n",
      "epoch 25 | loss: 1.70853 | train_balanced_accuracy: 0.2009  | valid_balanced_accuracy: 0.20084 |  0:11:44s\n",
      "epoch 26 | loss: 1.70193 | train_balanced_accuracy: 0.20305 | valid_balanced_accuracy: 0.20196 |  0:12:11s\n",
      "epoch 27 | loss: 1.6932  | train_balanced_accuracy: 0.20319 | valid_balanced_accuracy: 0.20337 |  0:12:38s\n",
      "epoch 28 | loss: 1.68621 | train_balanced_accuracy: 0.20829 | valid_balanced_accuracy: 0.20733 |  0:13:05s\n",
      "epoch 29 | loss: 1.67926 | train_balanced_accuracy: 0.20939 | valid_balanced_accuracy: 0.20801 |  0:13:32s\n",
      "epoch 30 | loss: 1.67609 | train_balanced_accuracy: 0.21089 | valid_balanced_accuracy: 0.2092  |  0:13:58s\n",
      "epoch 31 | loss: 1.66873 | train_balanced_accuracy: 0.21293 | valid_balanced_accuracy: 0.21112 |  0:14:25s\n",
      "epoch 32 | loss: 1.66218 | train_balanced_accuracy: 0.21345 | valid_balanced_accuracy: 0.21278 |  0:14:52s\n",
      "epoch 33 | loss: 1.65742 | train_balanced_accuracy: 0.21703 | valid_balanced_accuracy: 0.21514 |  0:15:19s\n",
      "epoch 34 | loss: 1.65246 | train_balanced_accuracy: 0.21766 | valid_balanced_accuracy: 0.21673 |  0:15:46s\n",
      "epoch 35 | loss: 1.64782 | train_balanced_accuracy: 0.21882 | valid_balanced_accuracy: 0.21785 |  0:16:13s\n",
      "epoch 36 | loss: 1.64216 | train_balanced_accuracy: 0.21773 | valid_balanced_accuracy: 0.21805 |  0:16:40s\n",
      "epoch 37 | loss: 1.63762 | train_balanced_accuracy: 0.22312 | valid_balanced_accuracy: 0.22252 |  0:17:07s\n",
      "epoch 38 | loss: 1.63128 | train_balanced_accuracy: 0.22366 | valid_balanced_accuracy: 0.22309 |  0:17:34s\n",
      "epoch 39 | loss: 1.62855 | train_balanced_accuracy: 0.22456 | valid_balanced_accuracy: 0.22424 |  0:18:01s\n",
      "epoch 40 | loss: 1.62447 | train_balanced_accuracy: 0.22692 | valid_balanced_accuracy: 0.22695 |  0:18:28s\n",
      "epoch 41 | loss: 1.61955 | train_balanced_accuracy: 0.22784 | valid_balanced_accuracy: 0.22705 |  0:18:55s\n",
      "epoch 42 | loss: 1.61579 | train_balanced_accuracy: 0.23077 | valid_balanced_accuracy: 0.2305  |  0:19:22s\n",
      "epoch 43 | loss: 1.61112 | train_balanced_accuracy: 0.2289  | valid_balanced_accuracy: 0.22665 |  0:19:48s\n",
      "epoch 44 | loss: 1.60842 | train_balanced_accuracy: 0.23258 | valid_balanced_accuracy: 0.23148 |  0:20:15s\n",
      "epoch 45 | loss: 1.60342 | train_balanced_accuracy: 0.23492 | valid_balanced_accuracy: 0.23346 |  0:20:41s\n",
      "epoch 46 | loss: 1.60084 | train_balanced_accuracy: 0.2343  | valid_balanced_accuracy: 0.23373 |  0:21:08s\n",
      "epoch 47 | loss: 1.5975  | train_balanced_accuracy: 0.23301 | valid_balanced_accuracy: 0.23266 |  0:21:35s\n",
      "epoch 48 | loss: 1.59204 | train_balanced_accuracy: 0.23839 | valid_balanced_accuracy: 0.23825 |  0:22:02s\n",
      "epoch 49 | loss: 1.58959 | train_balanced_accuracy: 0.23723 | valid_balanced_accuracy: 0.23563 |  0:22:29s\n",
      "epoch 50 | loss: 1.58537 | train_balanced_accuracy: 0.23918 | valid_balanced_accuracy: 0.23759 |  0:22:56s\n",
      "epoch 51 | loss: 1.58307 | train_balanced_accuracy: 0.24017 | valid_balanced_accuracy: 0.23898 |  0:23:23s\n",
      "epoch 52 | loss: 1.57906 | train_balanced_accuracy: 0.24273 | valid_balanced_accuracy: 0.24166 |  0:23:50s\n",
      "epoch 53 | loss: 1.57612 | train_balanced_accuracy: 0.24508 | valid_balanced_accuracy: 0.24294 |  0:24:17s\n",
      "epoch 54 | loss: 1.57304 | train_balanced_accuracy: 0.24597 | valid_balanced_accuracy: 0.24436 |  0:24:44s\n",
      "epoch 55 | loss: 1.57067 | train_balanced_accuracy: 0.24482 | valid_balanced_accuracy: 0.24556 |  0:25:10s\n",
      "epoch 56 | loss: 1.56802 | train_balanced_accuracy: 0.24474 | valid_balanced_accuracy: 0.24458 |  0:25:37s\n",
      "epoch 57 | loss: 1.56404 | train_balanced_accuracy: 0.24853 | valid_balanced_accuracy: 0.24813 |  0:26:04s\n",
      "epoch 58 | loss: 1.56136 | train_balanced_accuracy: 0.24858 | valid_balanced_accuracy: 0.24845 |  0:26:31s\n",
      "epoch 59 | loss: 1.56018 | train_balanced_accuracy: 0.24995 | valid_balanced_accuracy: 0.24994 |  0:26:59s\n",
      "epoch 60 | loss: 1.5559  | train_balanced_accuracy: 0.24923 | valid_balanced_accuracy: 0.24865 |  0:27:26s\n",
      "epoch 61 | loss: 1.55341 | train_balanced_accuracy: 0.24968 | valid_balanced_accuracy: 0.24908 |  0:27:53s\n",
      "epoch 62 | loss: 1.55156 | train_balanced_accuracy: 0.25229 | valid_balanced_accuracy: 0.25192 |  0:28:20s\n",
      "epoch 63 | loss: 1.54946 | train_balanced_accuracy: 0.25409 | valid_balanced_accuracy: 0.2531  |  0:28:47s\n",
      "epoch 64 | loss: 1.54723 | train_balanced_accuracy: 0.25448 | valid_balanced_accuracy: 0.25334 |  0:29:14s\n",
      "epoch 65 | loss: 1.54324 | train_balanced_accuracy: 0.25886 | valid_balanced_accuracy: 0.25556 |  0:29:41s\n",
      "epoch 66 | loss: 1.54053 | train_balanced_accuracy: 0.25874 | valid_balanced_accuracy: 0.25662 |  0:30:09s\n",
      "epoch 67 | loss: 1.53991 | train_balanced_accuracy: 0.25857 | valid_balanced_accuracy: 0.25899 |  0:30:35s\n",
      "epoch 68 | loss: 1.53552 | train_balanced_accuracy: 0.25956 | valid_balanced_accuracy: 0.25848 |  0:31:02s\n",
      "epoch 69 | loss: 1.53428 | train_balanced_accuracy: 0.26473 | valid_balanced_accuracy: 0.26353 |  0:31:30s\n",
      "epoch 70 | loss: 1.53267 | train_balanced_accuracy: 0.26666 | valid_balanced_accuracy: 0.26528 |  0:31:57s\n",
      "epoch 71 | loss: 1.53076 | train_balanced_accuracy: 0.26783 | valid_balanced_accuracy: 0.26372 |  0:32:25s\n",
      "epoch 72 | loss: 1.52806 | train_balanced_accuracy: 0.27136 | valid_balanced_accuracy: 0.26912 |  0:32:52s\n",
      "epoch 73 | loss: 1.52453 | train_balanced_accuracy: 0.26923 | valid_balanced_accuracy: 0.26382 |  0:33:20s\n",
      "epoch 74 | loss: 1.52521 | train_balanced_accuracy: 0.27697 | valid_balanced_accuracy: 0.27448 |  0:33:48s\n",
      "epoch 75 | loss: 1.5203  | train_balanced_accuracy: 0.27511 | valid_balanced_accuracy: 0.2697  |  0:34:15s\n",
      "epoch 76 | loss: 1.51953 | train_balanced_accuracy: 0.27851 | valid_balanced_accuracy: 0.2741  |  0:34:43s\n",
      "epoch 77 | loss: 1.51601 | train_balanced_accuracy: 0.27902 | valid_balanced_accuracy: 0.27445 |  0:35:11s\n",
      "epoch 78 | loss: 1.51492 | train_balanced_accuracy: 0.28274 | valid_balanced_accuracy: 0.27786 |  0:35:38s\n",
      "epoch 79 | loss: 1.51302 | train_balanced_accuracy: 0.28362 | valid_balanced_accuracy: 0.28096 |  0:36:05s\n",
      "epoch 80 | loss: 1.51171 | train_balanced_accuracy: 0.28495 | valid_balanced_accuracy: 0.28046 |  0:36:31s\n",
      "epoch 81 | loss: 1.50833 | train_balanced_accuracy: 0.28262 | valid_balanced_accuracy: 0.27885 |  0:36:58s\n",
      "epoch 82 | loss: 1.50851 | train_balanced_accuracy: 0.28574 | valid_balanced_accuracy: 0.28091 |  0:37:25s\n",
      "epoch 83 | loss: 1.50787 | train_balanced_accuracy: 0.28618 | valid_balanced_accuracy: 0.28276 |  0:37:52s\n",
      "epoch 84 | loss: 1.50377 | train_balanced_accuracy: 0.2882  | valid_balanced_accuracy: 0.28495 |  0:38:18s\n",
      "epoch 85 | loss: 1.50295 | train_balanced_accuracy: 0.28938 | valid_balanced_accuracy: 0.28556 |  0:38:45s\n",
      "epoch 86 | loss: 1.50229 | train_balanced_accuracy: 0.28901 | valid_balanced_accuracy: 0.28536 |  0:39:12s\n",
      "epoch 87 | loss: 1.50079 | train_balanced_accuracy: 0.29042 | valid_balanced_accuracy: 0.28488 |  0:39:39s\n",
      "epoch 88 | loss: 1.49927 | train_balanced_accuracy: 0.29109 | valid_balanced_accuracy: 0.28687 |  0:40:05s\n",
      "epoch 89 | loss: 1.49701 | train_balanced_accuracy: 0.29265 | valid_balanced_accuracy: 0.28798 |  0:40:32s\n",
      "epoch 90 | loss: 1.49376 | train_balanced_accuracy: 0.29706 | valid_balanced_accuracy: 0.29043 |  0:40:59s\n",
      "epoch 91 | loss: 1.49308 | train_balanced_accuracy: 0.29566 | valid_balanced_accuracy: 0.28903 |  0:41:25s\n",
      "epoch 92 | loss: 1.49394 | train_balanced_accuracy: 0.29412 | valid_balanced_accuracy: 0.28998 |  0:41:52s\n",
      "epoch 93 | loss: 1.49081 | train_balanced_accuracy: 0.29489 | valid_balanced_accuracy: 0.29031 |  0:42:18s\n",
      "epoch 94 | loss: 1.49088 | train_balanced_accuracy: 0.29998 | valid_balanced_accuracy: 0.29301 |  0:42:44s\n",
      "epoch 95 | loss: 1.48796 | train_balanced_accuracy: 0.29676 | valid_balanced_accuracy: 0.29493 |  0:43:11s\n",
      "epoch 96 | loss: 1.48669 | train_balanced_accuracy: 0.29486 | valid_balanced_accuracy: 0.28884 |  0:43:37s\n",
      "epoch 97 | loss: 1.48547 | train_balanced_accuracy: 0.30082 | valid_balanced_accuracy: 0.29648 |  0:44:04s\n",
      "epoch 98 | loss: 1.48524 | train_balanced_accuracy: 0.30065 | valid_balanced_accuracy: 0.29704 |  0:44:32s\n",
      "epoch 99 | loss: 1.48411 | train_balanced_accuracy: 0.29809 | valid_balanced_accuracy: 0.29121 |  0:44:59s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 98 and best_valid_balanced_accuracy = 0.29704\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'gamma': 1.8}, optimizer_params={'lr': 0.0001}, n_steps=3, n_shared=5, n_independent=1, n_a=16, momentum=0.02, lambda_sparse=0.001, clip_value=1.0, total=45.1min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'gamma': 1.8}, optimizer_params={'lr': 0.0001}, n_steps=3, n_shared=5, n_independent=1, n_a=16, momentum=0.02, lambda_sparse=0.001, clip_value=1.0 \n",
      "epoch 0  | loss: 3.97573 | train_balanced_accuracy: 0.05174 | valid_balanced_accuracy: 0.05414 |  0:00:27s\n",
      "epoch 1  | loss: 3.47829 | train_balanced_accuracy: 0.06943 | valid_balanced_accuracy: 0.06915 |  0:00:55s\n",
      "epoch 2  | loss: 3.15724 | train_balanced_accuracy: 0.0698  | valid_balanced_accuracy: 0.06975 |  0:01:23s\n",
      "epoch 3  | loss: 2.92499 | train_balanced_accuracy: 0.07994 | valid_balanced_accuracy: 0.07673 |  0:01:51s\n",
      "epoch 4  | loss: 2.71782 | train_balanced_accuracy: 0.08708 | valid_balanced_accuracy: 0.08691 |  0:02:18s\n",
      "epoch 5  | loss: 2.55861 | train_balanced_accuracy: 0.10095 | valid_balanced_accuracy: 0.10034 |  0:02:46s\n",
      "epoch 6  | loss: 2.42877 | train_balanced_accuracy: 0.10063 | valid_balanced_accuracy: 0.09984 |  0:03:14s\n",
      "epoch 7  | loss: 2.31483 | train_balanced_accuracy: 0.1198  | valid_balanced_accuracy: 0.12183 |  0:03:41s\n",
      "epoch 8  | loss: 2.22089 | train_balanced_accuracy: 0.12033 | valid_balanced_accuracy: 0.12192 |  0:04:09s\n",
      "epoch 9  | loss: 2.14912 | train_balanced_accuracy: 0.13245 | valid_balanced_accuracy: 0.13434 |  0:04:37s\n",
      "epoch 10 | loss: 2.08448 | train_balanced_accuracy: 0.14078 | valid_balanced_accuracy: 0.14162 |  0:05:05s\n",
      "epoch 11 | loss: 2.03163 | train_balanced_accuracy: 0.1452  | valid_balanced_accuracy: 0.14508 |  0:05:32s\n",
      "epoch 12 | loss: 1.9866  | train_balanced_accuracy: 0.14688 | valid_balanced_accuracy: 0.1471  |  0:06:00s\n",
      "epoch 13 | loss: 1.94737 | train_balanced_accuracy: 0.1563  | valid_balanced_accuracy: 0.15854 |  0:06:28s\n",
      "epoch 14 | loss: 1.911   | train_balanced_accuracy: 0.15301 | valid_balanced_accuracy: 0.15405 |  0:06:55s\n",
      "epoch 15 | loss: 1.88015 | train_balanced_accuracy: 0.16498 | valid_balanced_accuracy: 0.16581 |  0:07:22s\n",
      "epoch 16 | loss: 1.85396 | train_balanced_accuracy: 0.16104 | valid_balanced_accuracy: 0.1619  |  0:07:49s\n",
      "epoch 17 | loss: 1.82962 | train_balanced_accuracy: 0.16797 | valid_balanced_accuracy: 0.16802 |  0:08:16s\n",
      "epoch 18 | loss: 1.80821 | train_balanced_accuracy: 0.17319 | valid_balanced_accuracy: 0.17374 |  0:08:43s\n",
      "epoch 19 | loss: 1.78962 | train_balanced_accuracy: 0.18288 | valid_balanced_accuracy: 0.18235 |  0:09:09s\n",
      "epoch 20 | loss: 1.77412 | train_balanced_accuracy: 0.17749 | valid_balanced_accuracy: 0.17638 |  0:09:36s\n",
      "epoch 21 | loss: 1.7605  | train_balanced_accuracy: 0.1761  | valid_balanced_accuracy: 0.17491 |  0:10:02s\n",
      "epoch 22 | loss: 1.74715 | train_balanced_accuracy: 0.18142 | valid_balanced_accuracy: 0.18006 |  0:10:29s\n",
      "epoch 23 | loss: 1.738   | train_balanced_accuracy: 0.18573 | valid_balanced_accuracy: 0.18464 |  0:10:55s\n",
      "epoch 24 | loss: 1.72834 | train_balanced_accuracy: 0.18338 | valid_balanced_accuracy: 0.18279 |  0:11:22s\n",
      "epoch 25 | loss: 1.71653 | train_balanced_accuracy: 0.1853  | valid_balanced_accuracy: 0.18338 |  0:11:49s\n",
      "epoch 26 | loss: 1.70737 | train_balanced_accuracy: 0.19695 | valid_balanced_accuracy: 0.1951  |  0:12:16s\n",
      "epoch 27 | loss: 1.70004 | train_balanced_accuracy: 0.18874 | valid_balanced_accuracy: 0.18928 |  0:12:42s\n",
      "epoch 28 | loss: 1.69192 | train_balanced_accuracy: 0.19659 | valid_balanced_accuracy: 0.19571 |  0:13:09s\n",
      "epoch 29 | loss: 1.68523 | train_balanced_accuracy: 0.1947  | valid_balanced_accuracy: 0.1939  |  0:13:37s\n",
      "epoch 30 | loss: 1.67985 | train_balanced_accuracy: 0.20171 | valid_balanced_accuracy: 0.20005 |  0:14:04s\n",
      "epoch 31 | loss: 1.67231 | train_balanced_accuracy: 0.20545 | valid_balanced_accuracy: 0.20315 |  0:14:31s\n",
      "epoch 32 | loss: 1.66615 | train_balanced_accuracy: 0.20813 | valid_balanced_accuracy: 0.20618 |  0:14:57s\n",
      "epoch 33 | loss: 1.6608  | train_balanced_accuracy: 0.21584 | valid_balanced_accuracy: 0.21389 |  0:15:25s\n",
      "epoch 34 | loss: 1.65377 | train_balanced_accuracy: 0.21726 | valid_balanced_accuracy: 0.21564 |  0:15:51s\n",
      "epoch 35 | loss: 1.64879 | train_balanced_accuracy: 0.21735 | valid_balanced_accuracy: 0.21465 |  0:16:18s\n",
      "epoch 36 | loss: 1.64395 | train_balanced_accuracy: 0.21952 | valid_balanced_accuracy: 0.21696 |  0:16:45s\n",
      "epoch 37 | loss: 1.63967 | train_balanced_accuracy: 0.22522 | valid_balanced_accuracy: 0.2233  |  0:17:11s\n",
      "epoch 38 | loss: 1.63389 | train_balanced_accuracy: 0.22968 | valid_balanced_accuracy: 0.22866 |  0:17:38s\n",
      "epoch 39 | loss: 1.6289  | train_balanced_accuracy: 0.22968 | valid_balanced_accuracy: 0.2287  |  0:18:05s\n",
      "epoch 40 | loss: 1.62486 | train_balanced_accuracy: 0.23237 | valid_balanced_accuracy: 0.23122 |  0:18:31s\n",
      "epoch 41 | loss: 1.62074 | train_balanced_accuracy: 0.22926 | valid_balanced_accuracy: 0.22857 |  0:18:58s\n",
      "epoch 42 | loss: 1.61622 | train_balanced_accuracy: 0.23615 | valid_balanced_accuracy: 0.23523 |  0:19:24s\n",
      "epoch 43 | loss: 1.61123 | train_balanced_accuracy: 0.23559 | valid_balanced_accuracy: 0.23502 |  0:19:51s\n",
      "epoch 44 | loss: 1.60888 | train_balanced_accuracy: 0.2345  | valid_balanced_accuracy: 0.23406 |  0:20:18s\n",
      "epoch 45 | loss: 1.60612 | train_balanced_accuracy: 0.2363  | valid_balanced_accuracy: 0.23574 |  0:20:44s\n",
      "epoch 46 | loss: 1.60349 | train_balanced_accuracy: 0.2384  | valid_balanced_accuracy: 0.23877 |  0:21:11s\n",
      "epoch 47 | loss: 1.59964 | train_balanced_accuracy: 0.23762 | valid_balanced_accuracy: 0.23647 |  0:21:38s\n",
      "epoch 48 | loss: 1.59476 | train_balanced_accuracy: 0.24027 | valid_balanced_accuracy: 0.23914 |  0:22:05s\n",
      "epoch 49 | loss: 1.59217 | train_balanced_accuracy: 0.24017 | valid_balanced_accuracy: 0.23808 |  0:22:32s\n",
      "epoch 50 | loss: 1.58919 | train_balanced_accuracy: 0.24053 | valid_balanced_accuracy: 0.23986 |  0:22:59s\n",
      "epoch 51 | loss: 1.58624 | train_balanced_accuracy: 0.24182 | valid_balanced_accuracy: 0.24105 |  0:23:27s\n",
      "epoch 52 | loss: 1.58266 | train_balanced_accuracy: 0.24433 | valid_balanced_accuracy: 0.24331 |  0:23:54s\n",
      "epoch 53 | loss: 1.57877 | train_balanced_accuracy: 0.24376 | valid_balanced_accuracy: 0.24221 |  0:24:21s\n",
      "epoch 54 | loss: 1.57637 | train_balanced_accuracy: 0.24483 | valid_balanced_accuracy: 0.24339 |  0:24:49s\n",
      "epoch 55 | loss: 1.57378 | train_balanced_accuracy: 0.24737 | valid_balanced_accuracy: 0.24444 |  0:25:16s\n",
      "epoch 56 | loss: 1.5711  | train_balanced_accuracy: 0.24752 | valid_balanced_accuracy: 0.24397 |  0:25:43s\n",
      "epoch 57 | loss: 1.56736 | train_balanced_accuracy: 0.25124 | valid_balanced_accuracy: 0.2476  |  0:26:11s\n",
      "epoch 58 | loss: 1.56573 | train_balanced_accuracy: 0.25048 | valid_balanced_accuracy: 0.24791 |  0:26:38s\n",
      "epoch 59 | loss: 1.56155 | train_balanced_accuracy: 0.25174 | valid_balanced_accuracy: 0.24953 |  0:27:05s\n",
      "epoch 60 | loss: 1.5594  | train_balanced_accuracy: 0.25133 | valid_balanced_accuracy: 0.2508  |  0:27:32s\n",
      "epoch 61 | loss: 1.55683 | train_balanced_accuracy: 0.25363 | valid_balanced_accuracy: 0.25259 |  0:27:59s\n",
      "epoch 62 | loss: 1.55535 | train_balanced_accuracy: 0.25714 | valid_balanced_accuracy: 0.25728 |  0:28:26s\n",
      "epoch 63 | loss: 1.55194 | train_balanced_accuracy: 0.25042 | valid_balanced_accuracy: 0.25198 |  0:28:53s\n",
      "epoch 64 | loss: 1.54936 | train_balanced_accuracy: 0.2548  | valid_balanced_accuracy: 0.25501 |  0:29:20s\n",
      "epoch 65 | loss: 1.54731 | train_balanced_accuracy: 0.25931 | valid_balanced_accuracy: 0.25952 |  0:29:47s\n",
      "epoch 66 | loss: 1.54503 | train_balanced_accuracy: 0.25964 | valid_balanced_accuracy: 0.2596  |  0:30:14s\n",
      "epoch 67 | loss: 1.54347 | train_balanced_accuracy: 0.26056 | valid_balanced_accuracy: 0.26036 |  0:30:41s\n",
      "epoch 68 | loss: 1.54088 | train_balanced_accuracy: 0.26016 | valid_balanced_accuracy: 0.26089 |  0:31:08s\n",
      "epoch 69 | loss: 1.53791 | train_balanced_accuracy: 0.26135 | valid_balanced_accuracy: 0.26183 |  0:31:35s\n",
      "epoch 70 | loss: 1.53623 | train_balanced_accuracy: 0.26235 | valid_balanced_accuracy: 0.262   |  0:32:02s\n",
      "epoch 71 | loss: 1.53561 | train_balanced_accuracy: 0.26317 | valid_balanced_accuracy: 0.26354 |  0:32:29s\n",
      "epoch 72 | loss: 1.53329 | train_balanced_accuracy: 0.26544 | valid_balanced_accuracy: 0.26597 |  0:32:56s\n",
      "epoch 73 | loss: 1.53009 | train_balanced_accuracy: 0.26364 | valid_balanced_accuracy: 0.26385 |  0:33:23s\n",
      "epoch 74 | loss: 1.52782 | train_balanced_accuracy: 0.26419 | valid_balanced_accuracy: 0.26356 |  0:33:50s\n",
      "epoch 75 | loss: 1.52568 | train_balanced_accuracy: 0.26494 | valid_balanced_accuracy: 0.26366 |  0:34:17s\n",
      "epoch 76 | loss: 1.5254  | train_balanced_accuracy: 0.26487 | valid_balanced_accuracy: 0.26398 |  0:34:44s\n",
      "epoch 77 | loss: 1.52171 | train_balanced_accuracy: 0.26893 | valid_balanced_accuracy: 0.2663  |  0:35:11s\n",
      "epoch 78 | loss: 1.52181 | train_balanced_accuracy: 0.27591 | valid_balanced_accuracy: 0.27452 |  0:35:38s\n",
      "epoch 79 | loss: 1.51973 | train_balanced_accuracy: 0.26963 | valid_balanced_accuracy: 0.27034 |  0:36:05s\n",
      "epoch 80 | loss: 1.51708 | train_balanced_accuracy: 0.27309 | valid_balanced_accuracy: 0.27243 |  0:36:32s\n",
      "epoch 81 | loss: 1.51494 | train_balanced_accuracy: 0.27357 | valid_balanced_accuracy: 0.27413 |  0:36:59s\n",
      "epoch 82 | loss: 1.51444 | train_balanced_accuracy: 0.27291 | valid_balanced_accuracy: 0.27099 |  0:37:26s\n",
      "epoch 83 | loss: 1.51355 | train_balanced_accuracy: 0.27439 | valid_balanced_accuracy: 0.27122 |  0:37:54s\n",
      "epoch 84 | loss: 1.51101 | train_balanced_accuracy: 0.28042 | valid_balanced_accuracy: 0.27919 |  0:38:21s\n",
      "epoch 85 | loss: 1.50855 | train_balanced_accuracy: 0.28115 | valid_balanced_accuracy: 0.28125 |  0:38:48s\n",
      "epoch 86 | loss: 1.5081  | train_balanced_accuracy: 0.28411 | valid_balanced_accuracy: 0.28105 |  0:39:15s\n",
      "epoch 87 | loss: 1.50661 | train_balanced_accuracy: 0.28326 | valid_balanced_accuracy: 0.27933 |  0:39:42s\n",
      "epoch 88 | loss: 1.50459 | train_balanced_accuracy: 0.28437 | valid_balanced_accuracy: 0.28073 |  0:40:09s\n",
      "epoch 89 | loss: 1.50433 | train_balanced_accuracy: 0.28561 | valid_balanced_accuracy: 0.28355 |  0:40:36s\n",
      "epoch 90 | loss: 1.50081 | train_balanced_accuracy: 0.28813 | valid_balanced_accuracy: 0.28691 |  0:41:03s\n",
      "epoch 91 | loss: 1.49876 | train_balanced_accuracy: 0.28819 | valid_balanced_accuracy: 0.28839 |  0:41:30s\n",
      "epoch 92 | loss: 1.49895 | train_balanced_accuracy: 0.2885  | valid_balanced_accuracy: 0.28925 |  0:41:57s\n",
      "epoch 93 | loss: 1.49726 | train_balanced_accuracy: 0.2909  | valid_balanced_accuracy: 0.28602 |  0:42:24s\n",
      "epoch 94 | loss: 1.49497 | train_balanced_accuracy: 0.29735 | valid_balanced_accuracy: 0.29502 |  0:42:51s\n",
      "epoch 95 | loss: 1.49252 | train_balanced_accuracy: 0.29252 | valid_balanced_accuracy: 0.29523 |  0:43:17s\n",
      "epoch 96 | loss: 1.49304 | train_balanced_accuracy: 0.29345 | valid_balanced_accuracy: 0.2937  |  0:43:44s\n",
      "epoch 97 | loss: 1.49141 | train_balanced_accuracy: 0.29472 | valid_balanced_accuracy: 0.29564 |  0:44:11s\n",
      "epoch 98 | loss: 1.49051 | train_balanced_accuracy: 0.29456 | valid_balanced_accuracy: 0.29409 |  0:44:37s\n",
      "epoch 99 | loss: 1.48866 | train_balanced_accuracy: 0.29874 | valid_balanced_accuracy: 0.29697 |  0:45:04s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 99 and best_valid_balanced_accuracy = 0.29697\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'gamma': 1.8}, optimizer_params={'lr': 0.0001}, n_steps=3, n_shared=5, n_independent=1, n_a=16, momentum=0.02, lambda_sparse=0.001, clip_value=1.0, total=45.2min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'gamma': 2.2}, optimizer_params={'lr': 0.001}, n_steps=13, n_shared=1, n_independent=3, n_a=32, momentum=0.02, lambda_sparse=0.1, clip_value=0.01 \n",
      "epoch 0  | loss: 4.45281 | train_balanced_accuracy: 0.14759 | valid_balanced_accuracy: 0.15201 |  0:01:05s\n",
      "epoch 1  | loss: 2.07516 | train_balanced_accuracy: 0.21288 | valid_balanced_accuracy: 0.21086 |  0:02:11s\n",
      "epoch 2  | loss: 1.8337  | train_balanced_accuracy: 0.23234 | valid_balanced_accuracy: 0.22847 |  0:03:16s\n",
      "epoch 3  | loss: 1.74416 | train_balanced_accuracy: 0.24615 | valid_balanced_accuracy: 0.2463  |  0:04:21s\n",
      "epoch 4  | loss: 1.6794  | train_balanced_accuracy: 0.23521 | valid_balanced_accuracy: 0.23552 |  0:05:27s\n",
      "epoch 5  | loss: 1.65257 | train_balanced_accuracy: 0.26401 | valid_balanced_accuracy: 0.26208 |  0:06:32s\n",
      "epoch 6  | loss: 1.63641 | train_balanced_accuracy: 0.26669 | valid_balanced_accuracy: 0.26568 |  0:07:37s\n",
      "epoch 7  | loss: 1.64379 | train_balanced_accuracy: 0.28299 | valid_balanced_accuracy: 0.28055 |  0:08:43s\n",
      "epoch 8  | loss: 1.62328 | train_balanced_accuracy: 0.26615 | valid_balanced_accuracy: 0.26953 |  0:09:48s\n",
      "epoch 9  | loss: 1.59395 | train_balanced_accuracy: 0.28567 | valid_balanced_accuracy: 0.2855  |  0:10:54s\n",
      "epoch 10 | loss: 1.577   | train_balanced_accuracy: 0.31027 | valid_balanced_accuracy: 0.31684 |  0:12:00s\n",
      "epoch 11 | loss: 1.56865 | train_balanced_accuracy: 0.29441 | valid_balanced_accuracy: 0.29104 |  0:13:05s\n",
      "epoch 12 | loss: 1.569   | train_balanced_accuracy: 0.29932 | valid_balanced_accuracy: 0.30089 |  0:14:11s\n",
      "epoch 13 | loss: 1.55585 | train_balanced_accuracy: 0.32227 | valid_balanced_accuracy: 0.32335 |  0:15:17s\n",
      "epoch 14 | loss: 1.54461 | train_balanced_accuracy: 0.33342 | valid_balanced_accuracy: 0.33757 |  0:16:23s\n",
      "epoch 15 | loss: 1.53616 | train_balanced_accuracy: 0.31665 | valid_balanced_accuracy: 0.31523 |  0:17:28s\n",
      "epoch 16 | loss: 1.52007 | train_balanced_accuracy: 0.32239 | valid_balanced_accuracy: 0.32499 |  0:18:35s\n",
      "epoch 17 | loss: 1.51797 | train_balanced_accuracy: 0.31188 | valid_balanced_accuracy: 0.31418 |  0:19:40s\n",
      "epoch 18 | loss: 1.5087  | train_balanced_accuracy: 0.36658 | valid_balanced_accuracy: 0.36905 |  0:20:46s\n",
      "epoch 19 | loss: 1.51265 | train_balanced_accuracy: 0.32696 | valid_balanced_accuracy: 0.32714 |  0:21:52s\n",
      "epoch 20 | loss: 1.51504 | train_balanced_accuracy: 0.31476 | valid_balanced_accuracy: 0.31583 |  0:22:58s\n",
      "epoch 21 | loss: 1.50236 | train_balanced_accuracy: 0.31632 | valid_balanced_accuracy: 0.32136 |  0:24:04s\n",
      "epoch 22 | loss: 1.50228 | train_balanced_accuracy: 0.34785 | valid_balanced_accuracy: 0.35499 |  0:25:10s\n",
      "epoch 23 | loss: 1.48925 | train_balanced_accuracy: 0.36419 | valid_balanced_accuracy: 0.36782 |  0:26:15s\n",
      "epoch 24 | loss: 1.47494 | train_balanced_accuracy: 0.37062 | valid_balanced_accuracy: 0.36853 |  0:27:22s\n",
      "epoch 25 | loss: 1.46769 | train_balanced_accuracy: 0.35727 | valid_balanced_accuracy: 0.3603  |  0:28:29s\n",
      "epoch 26 | loss: 1.46866 | train_balanced_accuracy: 0.3791  | valid_balanced_accuracy: 0.3858  |  0:29:35s\n",
      "epoch 27 | loss: 1.46518 | train_balanced_accuracy: 0.38313 | valid_balanced_accuracy: 0.39285 |  0:30:42s\n",
      "epoch 28 | loss: 1.45717 | train_balanced_accuracy: 0.36946 | valid_balanced_accuracy: 0.38107 |  0:31:49s\n",
      "epoch 29 | loss: 1.45038 | train_balanced_accuracy: 0.38345 | valid_balanced_accuracy: 0.39038 |  0:32:54s\n",
      "epoch 30 | loss: 1.44266 | train_balanced_accuracy: 0.34348 | valid_balanced_accuracy: 0.34681 |  0:34:00s\n",
      "epoch 31 | loss: 1.44935 | train_balanced_accuracy: 0.36871 | valid_balanced_accuracy: 0.36916 |  0:35:05s\n",
      "epoch 32 | loss: 1.44877 | train_balanced_accuracy: 0.37703 | valid_balanced_accuracy: 0.38531 |  0:36:10s\n",
      "epoch 33 | loss: 1.45362 | train_balanced_accuracy: 0.37792 | valid_balanced_accuracy: 0.38479 |  0:37:16s\n",
      "epoch 34 | loss: 1.44865 | train_balanced_accuracy: 0.39155 | valid_balanced_accuracy: 0.39973 |  0:38:23s\n",
      "epoch 35 | loss: 1.43834 | train_balanced_accuracy: 0.39299 | valid_balanced_accuracy: 0.39562 |  0:39:28s\n",
      "epoch 36 | loss: 1.43626 | train_balanced_accuracy: 0.3846  | valid_balanced_accuracy: 0.40106 |  0:40:34s\n",
      "epoch 37 | loss: 1.42965 | train_balanced_accuracy: 0.40441 | valid_balanced_accuracy: 0.41505 |  0:41:39s\n",
      "epoch 38 | loss: 1.42212 | train_balanced_accuracy: 0.36697 | valid_balanced_accuracy: 0.384   |  0:42:45s\n",
      "epoch 39 | loss: 1.42259 | train_balanced_accuracy: 0.39829 | valid_balanced_accuracy: 0.41208 |  0:43:50s\n",
      "epoch 40 | loss: 1.42123 | train_balanced_accuracy: 0.39553 | valid_balanced_accuracy: 0.40092 |  0:44:57s\n",
      "epoch 41 | loss: 1.41548 | train_balanced_accuracy: 0.37573 | valid_balanced_accuracy: 0.37918 |  0:46:03s\n",
      "epoch 42 | loss: 1.40846 | train_balanced_accuracy: 0.40936 | valid_balanced_accuracy: 0.41263 |  0:47:09s\n",
      "epoch 43 | loss: 1.40342 | train_balanced_accuracy: 0.40864 | valid_balanced_accuracy: 0.40579 |  0:48:16s\n",
      "epoch 44 | loss: 1.39977 | train_balanced_accuracy: 0.42277 | valid_balanced_accuracy: 0.40818 |  0:49:24s\n",
      "epoch 45 | loss: 1.39818 | train_balanced_accuracy: 0.39483 | valid_balanced_accuracy: 0.39269 |  0:50:31s\n",
      "epoch 46 | loss: 1.3954  | train_balanced_accuracy: 0.40688 | valid_balanced_accuracy: 0.40212 |  0:51:37s\n",
      "epoch 47 | loss: 1.39141 | train_balanced_accuracy: 0.38189 | valid_balanced_accuracy: 0.3843  |  0:52:42s\n",
      "\n",
      "Early stopping occured at epoch 47 with best_epoch = 37 and best_valid_balanced_accuracy = 0.41505\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'gamma': 2.2}, optimizer_params={'lr': 0.001}, n_steps=13, n_shared=1, n_independent=3, n_a=32, momentum=0.02, lambda_sparse=0.1, clip_value=0.01, total=53.1min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'gamma': 2.2}, optimizer_params={'lr': 0.001}, n_steps=13, n_shared=1, n_independent=3, n_a=32, momentum=0.02, lambda_sparse=0.1, clip_value=0.01 \n",
      "epoch 0  | loss: 4.47618 | train_balanced_accuracy: 0.16721 | valid_balanced_accuracy: 0.16876 |  0:01:06s\n",
      "epoch 1  | loss: 2.05817 | train_balanced_accuracy: 0.19957 | valid_balanced_accuracy: 0.19683 |  0:02:12s\n",
      "epoch 2  | loss: 1.81291 | train_balanced_accuracy: 0.2266  | valid_balanced_accuracy: 0.22539 |  0:03:18s\n",
      "epoch 3  | loss: 1.75251 | train_balanced_accuracy: 0.23278 | valid_balanced_accuracy: 0.2344  |  0:04:24s\n",
      "epoch 4  | loss: 1.70609 | train_balanced_accuracy: 0.24318 | valid_balanced_accuracy: 0.24287 |  0:05:30s\n",
      "epoch 5  | loss: 1.65821 | train_balanced_accuracy: 0.25666 | valid_balanced_accuracy: 0.25455 |  0:06:36s\n",
      "epoch 6  | loss: 1.63883 | train_balanced_accuracy: 0.24605 | valid_balanced_accuracy: 0.24377 |  0:07:42s\n",
      "epoch 7  | loss: 1.62778 | train_balanced_accuracy: 0.25556 | valid_balanced_accuracy: 0.2559  |  0:08:48s\n",
      "epoch 8  | loss: 1.60744 | train_balanced_accuracy: 0.26257 | valid_balanced_accuracy: 0.26443 |  0:09:54s\n",
      "epoch 9  | loss: 1.59153 | train_balanced_accuracy: 0.25996 | valid_balanced_accuracy: 0.26069 |  0:11:00s\n",
      "epoch 10 | loss: 1.57807 | train_balanced_accuracy: 0.28142 | valid_balanced_accuracy: 0.2761  |  0:12:07s\n",
      "epoch 11 | loss: 1.55939 | train_balanced_accuracy: 0.29372 | valid_balanced_accuracy: 0.29306 |  0:13:13s\n",
      "epoch 12 | loss: 1.55545 | train_balanced_accuracy: 0.29839 | valid_balanced_accuracy: 0.29402 |  0:14:19s\n",
      "epoch 13 | loss: 1.55178 | train_balanced_accuracy: 0.32183 | valid_balanced_accuracy: 0.32047 |  0:15:25s\n",
      "epoch 14 | loss: 1.54176 | train_balanced_accuracy: 0.32279 | valid_balanced_accuracy: 0.32419 |  0:16:31s\n",
      "epoch 15 | loss: 1.53663 | train_balanced_accuracy: 0.3118  | valid_balanced_accuracy: 0.3013  |  0:17:37s\n",
      "epoch 16 | loss: 1.52833 | train_balanced_accuracy: 0.31867 | valid_balanced_accuracy: 0.32005 |  0:18:43s\n",
      "epoch 17 | loss: 1.52599 | train_balanced_accuracy: 0.31107 | valid_balanced_accuracy: 0.30655 |  0:19:49s\n",
      "epoch 18 | loss: 1.52128 | train_balanced_accuracy: 0.32372 | valid_balanced_accuracy: 0.31432 |  0:20:55s\n",
      "epoch 19 | loss: 1.51147 | train_balanced_accuracy: 0.30648 | valid_balanced_accuracy: 0.30241 |  0:22:01s\n",
      "epoch 20 | loss: 1.50747 | train_balanced_accuracy: 0.32249 | valid_balanced_accuracy: 0.31402 |  0:23:06s\n",
      "epoch 21 | loss: 1.50214 | train_balanced_accuracy: 0.30531 | valid_balanced_accuracy: 0.30054 |  0:24:13s\n",
      "epoch 22 | loss: 1.50875 | train_balanced_accuracy: 0.31814 | valid_balanced_accuracy: 0.31249 |  0:25:19s\n",
      "epoch 23 | loss: 1.50623 | train_balanced_accuracy: 0.34657 | valid_balanced_accuracy: 0.34905 |  0:26:24s\n",
      "epoch 24 | loss: 1.49218 | train_balanced_accuracy: 0.34331 | valid_balanced_accuracy: 0.34082 |  0:27:29s\n",
      "epoch 25 | loss: 1.49133 | train_balanced_accuracy: 0.3586  | valid_balanced_accuracy: 0.36136 |  0:28:34s\n",
      "epoch 26 | loss: 1.4877  | train_balanced_accuracy: 0.33825 | valid_balanced_accuracy: 0.3347  |  0:29:40s\n",
      "epoch 27 | loss: 1.49146 | train_balanced_accuracy: 0.33424 | valid_balanced_accuracy: 0.33531 |  0:30:46s\n",
      "epoch 28 | loss: 1.49423 | train_balanced_accuracy: 0.34911 | valid_balanced_accuracy: 0.35072 |  0:31:52s\n",
      "epoch 29 | loss: 1.49192 | train_balanced_accuracy: 0.35387 | valid_balanced_accuracy: 0.35329 |  0:32:58s\n",
      "epoch 30 | loss: 1.49113 | train_balanced_accuracy: 0.35408 | valid_balanced_accuracy: 0.36178 |  0:34:05s\n",
      "epoch 31 | loss: 1.48976 | train_balanced_accuracy: 0.356   | valid_balanced_accuracy: 0.35998 |  0:35:11s\n",
      "epoch 32 | loss: 1.48143 | train_balanced_accuracy: 0.34419 | valid_balanced_accuracy: 0.34427 |  0:36:17s\n",
      "epoch 33 | loss: 1.48335 | train_balanced_accuracy: 0.33037 | valid_balanced_accuracy: 0.34294 |  0:37:23s\n",
      "epoch 34 | loss: 1.4901  | train_balanced_accuracy: 0.33443 | valid_balanced_accuracy: 0.34056 |  0:38:29s\n",
      "epoch 35 | loss: 1.48659 | train_balanced_accuracy: 0.31775 | valid_balanced_accuracy: 0.31972 |  0:39:35s\n",
      "epoch 36 | loss: 1.47808 | train_balanced_accuracy: 0.35808 | valid_balanced_accuracy: 0.36574 |  0:40:40s\n",
      "epoch 37 | loss: 1.46702 | train_balanced_accuracy: 0.37392 | valid_balanced_accuracy: 0.37818 |  0:41:45s\n",
      "epoch 38 | loss: 1.4628  | train_balanced_accuracy: 0.36218 | valid_balanced_accuracy: 0.36051 |  0:42:50s\n",
      "epoch 39 | loss: 1.45678 | train_balanced_accuracy: 0.3746  | valid_balanced_accuracy: 0.36924 |  0:43:55s\n",
      "epoch 40 | loss: 1.45258 | train_balanced_accuracy: 0.34653 | valid_balanced_accuracy: 0.3375  |  0:45:00s\n",
      "epoch 41 | loss: 1.4484  | train_balanced_accuracy: 0.37182 | valid_balanced_accuracy: 0.36674 |  0:46:05s\n",
      "epoch 42 | loss: 1.4497  | train_balanced_accuracy: 0.36107 | valid_balanced_accuracy: 0.35335 |  0:47:10s\n",
      "epoch 43 | loss: 1.45418 | train_balanced_accuracy: 0.36294 | valid_balanced_accuracy: 0.35143 |  0:48:15s\n",
      "epoch 44 | loss: 1.45401 | train_balanced_accuracy: 0.37406 | valid_balanced_accuracy: 0.36735 |  0:49:20s\n",
      "epoch 45 | loss: 1.4561  | train_balanced_accuracy: 0.38695 | valid_balanced_accuracy: 0.37706 |  0:50:25s\n",
      "epoch 46 | loss: 1.44895 | train_balanced_accuracy: 0.38497 | valid_balanced_accuracy: 0.369   |  0:51:30s\n",
      "epoch 47 | loss: 1.44317 | train_balanced_accuracy: 0.38188 | valid_balanced_accuracy: 0.37314 |  0:52:35s\n",
      "\n",
      "Early stopping occured at epoch 47 with best_epoch = 37 and best_valid_balanced_accuracy = 0.37818\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'gamma': 2.2}, optimizer_params={'lr': 0.001}, n_steps=13, n_shared=1, n_independent=3, n_a=32, momentum=0.02, lambda_sparse=0.1, clip_value=0.01, total=52.9min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'gamma': 2.2}, optimizer_params={'lr': 0.001}, n_steps=13, n_shared=1, n_independent=3, n_a=32, momentum=0.02, lambda_sparse=0.1, clip_value=0.01 \n",
      "epoch 0  | loss: 4.7865  | train_balanced_accuracy: 0.13008 | valid_balanced_accuracy: 0.13104 |  0:01:05s\n",
      "epoch 1  | loss: 2.08803 | train_balanced_accuracy: 0.20463 | valid_balanced_accuracy: 0.20624 |  0:02:10s\n",
      "epoch 2  | loss: 1.82766 | train_balanced_accuracy: 0.22176 | valid_balanced_accuracy: 0.22297 |  0:03:15s\n",
      "epoch 3  | loss: 1.73806 | train_balanced_accuracy: 0.24079 | valid_balanced_accuracy: 0.24275 |  0:04:20s\n",
      "epoch 4  | loss: 1.67295 | train_balanced_accuracy: 0.253   | valid_balanced_accuracy: 0.25594 |  0:05:26s\n",
      "epoch 5  | loss: 1.64167 | train_balanced_accuracy: 0.25905 | valid_balanced_accuracy: 0.25384 |  0:06:31s\n",
      "epoch 6  | loss: 1.63003 | train_balanced_accuracy: 0.24413 | valid_balanced_accuracy: 0.24117 |  0:07:36s\n",
      "epoch 7  | loss: 1.61405 | train_balanced_accuracy: 0.26378 | valid_balanced_accuracy: 0.26757 |  0:08:41s\n",
      "epoch 8  | loss: 1.59455 | train_balanced_accuracy: 0.25633 | valid_balanced_accuracy: 0.25528 |  0:09:46s\n",
      "epoch 9  | loss: 1.57352 | train_balanced_accuracy: 0.29426 | valid_balanced_accuracy: 0.29249 |  0:10:52s\n",
      "epoch 10 | loss: 1.55731 | train_balanced_accuracy: 0.28708 | valid_balanced_accuracy: 0.29128 |  0:11:57s\n",
      "epoch 11 | loss: 1.54746 | train_balanced_accuracy: 0.30589 | valid_balanced_accuracy: 0.30181 |  0:13:02s\n",
      "epoch 12 | loss: 1.52948 | train_balanced_accuracy: 0.30194 | valid_balanced_accuracy: 0.30274 |  0:14:08s\n",
      "epoch 13 | loss: 1.52452 | train_balanced_accuracy: 0.31988 | valid_balanced_accuracy: 0.31536 |  0:15:13s\n",
      "epoch 14 | loss: 1.51833 | train_balanced_accuracy: 0.31168 | valid_balanced_accuracy: 0.31399 |  0:16:19s\n",
      "epoch 15 | loss: 1.52088 | train_balanced_accuracy: 0.31845 | valid_balanced_accuracy: 0.30702 |  0:17:24s\n",
      "epoch 16 | loss: 1.52222 | train_balanced_accuracy: 0.32173 | valid_balanced_accuracy: 0.3248  |  0:18:30s\n",
      "epoch 17 | loss: 1.53751 | train_balanced_accuracy: 0.30123 | valid_balanced_accuracy: 0.30801 |  0:19:35s\n",
      "epoch 18 | loss: 1.52188 | train_balanced_accuracy: 0.3064  | valid_balanced_accuracy: 0.30879 |  0:20:41s\n",
      "epoch 19 | loss: 1.51852 | train_balanced_accuracy: 0.32099 | valid_balanced_accuracy: 0.32833 |  0:21:48s\n",
      "epoch 20 | loss: 1.52819 | train_balanced_accuracy: 0.31966 | valid_balanced_accuracy: 0.3205  |  0:22:54s\n",
      "epoch 21 | loss: 1.52388 | train_balanced_accuracy: 0.3294  | valid_balanced_accuracy: 0.33407 |  0:24:00s\n",
      "epoch 22 | loss: 1.51041 | train_balanced_accuracy: 0.32203 | valid_balanced_accuracy: 0.31516 |  0:25:06s\n",
      "epoch 23 | loss: 1.50378 | train_balanced_accuracy: 0.33704 | valid_balanced_accuracy: 0.33788 |  0:26:13s\n",
      "epoch 24 | loss: 1.50602 | train_balanced_accuracy: 0.32214 | valid_balanced_accuracy: 0.31244 |  0:27:18s\n",
      "epoch 25 | loss: 1.50509 | train_balanced_accuracy: 0.31673 | valid_balanced_accuracy: 0.31676 |  0:28:24s\n",
      "epoch 26 | loss: 1.49898 | train_balanced_accuracy: 0.32397 | valid_balanced_accuracy: 0.31864 |  0:29:30s\n",
      "epoch 27 | loss: 1.48845 | train_balanced_accuracy: 0.32979 | valid_balanced_accuracy: 0.331   |  0:30:35s\n",
      "epoch 28 | loss: 1.48534 | train_balanced_accuracy: 0.33585 | valid_balanced_accuracy: 0.32858 |  0:31:41s\n",
      "epoch 29 | loss: 1.48216 | train_balanced_accuracy: 0.33926 | valid_balanced_accuracy: 0.33253 |  0:32:47s\n",
      "epoch 30 | loss: 1.48459 | train_balanced_accuracy: 0.3474  | valid_balanced_accuracy: 0.34668 |  0:33:53s\n",
      "epoch 31 | loss: 1.48533 | train_balanced_accuracy: 0.33297 | valid_balanced_accuracy: 0.32736 |  0:34:58s\n",
      "epoch 32 | loss: 1.47434 | train_balanced_accuracy: 0.3456  | valid_balanced_accuracy: 0.35002 |  0:36:04s\n",
      "epoch 33 | loss: 1.46779 | train_balanced_accuracy: 0.36121 | valid_balanced_accuracy: 0.36709 |  0:37:09s\n",
      "epoch 34 | loss: 1.45942 | train_balanced_accuracy: 0.38246 | valid_balanced_accuracy: 0.38527 |  0:38:15s\n",
      "epoch 35 | loss: 1.4533  | train_balanced_accuracy: 0.3562  | valid_balanced_accuracy: 0.35974 |  0:39:23s\n",
      "epoch 36 | loss: 1.44774 | train_balanced_accuracy: 0.365   | valid_balanced_accuracy: 0.36152 |  0:40:29s\n",
      "epoch 37 | loss: 1.45093 | train_balanced_accuracy: 0.36407 | valid_balanced_accuracy: 0.36018 |  0:41:35s\n",
      "epoch 38 | loss: 1.44658 | train_balanced_accuracy: 0.34676 | valid_balanced_accuracy: 0.34978 |  0:42:40s\n",
      "epoch 39 | loss: 1.44403 | train_balanced_accuracy: 0.36693 | valid_balanced_accuracy: 0.36807 |  0:43:46s\n",
      "epoch 40 | loss: 1.43543 | train_balanced_accuracy: 0.36288 | valid_balanced_accuracy: 0.36351 |  0:44:51s\n",
      "epoch 41 | loss: 1.43462 | train_balanced_accuracy: 0.36842 | valid_balanced_accuracy: 0.36621 |  0:45:57s\n",
      "epoch 42 | loss: 1.43047 | train_balanced_accuracy: 0.37535 | valid_balanced_accuracy: 0.37265 |  0:47:03s\n",
      "epoch 43 | loss: 1.43355 | train_balanced_accuracy: 0.37152 | valid_balanced_accuracy: 0.37141 |  0:48:10s\n",
      "epoch 44 | loss: 1.43192 | train_balanced_accuracy: 0.39014 | valid_balanced_accuracy: 0.38156 |  0:49:16s\n",
      "\n",
      "Early stopping occured at epoch 44 with best_epoch = 34 and best_valid_balanced_accuracy = 0.38527\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'gamma': 2.2}, optimizer_params={'lr': 0.001}, n_steps=13, n_shared=1, n_independent=3, n_a=32, momentum=0.02, lambda_sparse=0.1, clip_value=0.01, total=49.6min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'gamma': 2.2}, optimizer_params={'lr': 0.001}, n_steps=13, n_shared=1, n_independent=3, n_a=32, momentum=0.02, lambda_sparse=0.1, clip_value=0.01 \n",
      "epoch 0  | loss: 4.59186 | train_balanced_accuracy: 0.16403 | valid_balanced_accuracy: 0.16452 |  0:01:06s\n",
      "epoch 1  | loss: 2.06874 | train_balanced_accuracy: 0.2058  | valid_balanced_accuracy: 0.20676 |  0:02:12s\n",
      "epoch 2  | loss: 1.8699  | train_balanced_accuracy: 0.20741 | valid_balanced_accuracy: 0.20696 |  0:03:18s\n",
      "epoch 3  | loss: 1.77755 | train_balanced_accuracy: 0.23163 | valid_balanced_accuracy: 0.23071 |  0:04:24s\n",
      "epoch 4  | loss: 1.71664 | train_balanced_accuracy: 0.24298 | valid_balanced_accuracy: 0.25055 |  0:05:30s\n",
      "epoch 5  | loss: 1.67942 | train_balanced_accuracy: 0.23658 | valid_balanced_accuracy: 0.24205 |  0:06:38s\n",
      "epoch 6  | loss: 1.65255 | train_balanced_accuracy: 0.25006 | valid_balanced_accuracy: 0.25353 |  0:07:44s\n",
      "epoch 7  | loss: 1.62276 | train_balanced_accuracy: 0.26199 | valid_balanced_accuracy: 0.25517 |  0:08:50s\n",
      "epoch 8  | loss: 1.60709 | train_balanced_accuracy: 0.25606 | valid_balanced_accuracy: 0.2501  |  0:09:56s\n",
      "epoch 9  | loss: 1.59894 | train_balanced_accuracy: 0.27219 | valid_balanced_accuracy: 0.27141 |  0:11:01s\n",
      "epoch 10 | loss: 1.58705 | train_balanced_accuracy: 0.27203 | valid_balanced_accuracy: 0.27428 |  0:12:07s\n",
      "epoch 11 | loss: 1.57485 | train_balanced_accuracy: 0.28274 | valid_balanced_accuracy: 0.28565 |  0:13:13s\n",
      "epoch 12 | loss: 1.57438 | train_balanced_accuracy: 0.30253 | valid_balanced_accuracy: 0.29191 |  0:14:19s\n",
      "epoch 13 | loss: 1.56544 | train_balanced_accuracy: 0.28071 | valid_balanced_accuracy: 0.28004 |  0:15:24s\n",
      "epoch 14 | loss: 1.55533 | train_balanced_accuracy: 0.32025 | valid_balanced_accuracy: 0.30978 |  0:16:30s\n",
      "epoch 15 | loss: 1.5429  | train_balanced_accuracy: 0.30835 | valid_balanced_accuracy: 0.30596 |  0:17:36s\n",
      "epoch 16 | loss: 1.53567 | train_balanced_accuracy: 0.32949 | valid_balanced_accuracy: 0.31985 |  0:18:42s\n",
      "epoch 17 | loss: 1.5269  | train_balanced_accuracy: 0.33001 | valid_balanced_accuracy: 0.32603 |  0:19:47s\n",
      "epoch 18 | loss: 1.5263  | train_balanced_accuracy: 0.31855 | valid_balanced_accuracy: 0.31453 |  0:20:53s\n",
      "epoch 19 | loss: 1.53566 | train_balanced_accuracy: 0.32107 | valid_balanced_accuracy: 0.31636 |  0:21:59s\n",
      "epoch 20 | loss: 1.52467 | train_balanced_accuracy: 0.32726 | valid_balanced_accuracy: 0.33084 |  0:23:07s\n",
      "epoch 21 | loss: 1.5183  | train_balanced_accuracy: 0.35454 | valid_balanced_accuracy: 0.34299 |  0:24:16s\n",
      "epoch 22 | loss: 1.50485 | train_balanced_accuracy: 0.34509 | valid_balanced_accuracy: 0.34624 |  0:25:23s\n",
      "epoch 23 | loss: 1.50451 | train_balanced_accuracy: 0.35273 | valid_balanced_accuracy: 0.34618 |  0:26:31s\n",
      "epoch 24 | loss: 1.4977  | train_balanced_accuracy: 0.32953 | valid_balanced_accuracy: 0.31923 |  0:27:39s\n",
      "epoch 25 | loss: 1.49473 | train_balanced_accuracy: 0.34563 | valid_balanced_accuracy: 0.34033 |  0:28:45s\n",
      "epoch 26 | loss: 1.48507 | train_balanced_accuracy: 0.36603 | valid_balanced_accuracy: 0.36645 |  0:29:52s\n",
      "epoch 27 | loss: 1.47648 | train_balanced_accuracy: 0.3393  | valid_balanced_accuracy: 0.33562 |  0:30:59s\n",
      "epoch 28 | loss: 1.47113 | train_balanced_accuracy: 0.35454 | valid_balanced_accuracy: 0.35351 |  0:32:04s\n",
      "epoch 29 | loss: 1.45983 | train_balanced_accuracy: 0.36266 | valid_balanced_accuracy: 0.36278 |  0:33:11s\n",
      "epoch 30 | loss: 1.45042 | train_balanced_accuracy: 0.35025 | valid_balanced_accuracy: 0.35511 |  0:34:19s\n",
      "epoch 31 | loss: 1.44504 | train_balanced_accuracy: 0.37304 | valid_balanced_accuracy: 0.36679 |  0:35:27s\n",
      "epoch 32 | loss: 1.43576 | train_balanced_accuracy: 0.36289 | valid_balanced_accuracy: 0.35669 |  0:36:35s\n",
      "epoch 33 | loss: 1.43394 | train_balanced_accuracy: 0.38018 | valid_balanced_accuracy: 0.37279 |  0:37:43s\n",
      "epoch 34 | loss: 1.42581 | train_balanced_accuracy: 0.38652 | valid_balanced_accuracy: 0.38124 |  0:38:51s\n",
      "epoch 35 | loss: 1.42987 | train_balanced_accuracy: 0.3698  | valid_balanced_accuracy: 0.36612 |  0:39:56s\n",
      "epoch 36 | loss: 1.42597 | train_balanced_accuracy: 0.38551 | valid_balanced_accuracy: 0.38481 |  0:41:03s\n",
      "epoch 37 | loss: 1.41986 | train_balanced_accuracy: 0.39289 | valid_balanced_accuracy: 0.37799 |  0:42:09s\n",
      "epoch 38 | loss: 1.41942 | train_balanced_accuracy: 0.39782 | valid_balanced_accuracy: 0.39126 |  0:43:17s\n",
      "epoch 39 | loss: 1.42398 | train_balanced_accuracy: 0.40465 | valid_balanced_accuracy: 0.39613 |  0:44:25s\n",
      "epoch 40 | loss: 1.41925 | train_balanced_accuracy: 0.39459 | valid_balanced_accuracy: 0.38746 |  0:45:31s\n",
      "epoch 41 | loss: 1.41863 | train_balanced_accuracy: 0.4032  | valid_balanced_accuracy: 0.39331 |  0:46:37s\n",
      "epoch 42 | loss: 1.41601 | train_balanced_accuracy: 0.39199 | valid_balanced_accuracy: 0.37232 |  0:47:43s\n",
      "epoch 43 | loss: 1.42223 | train_balanced_accuracy: 0.39801 | valid_balanced_accuracy: 0.3834  |  0:48:51s\n",
      "epoch 44 | loss: 1.41634 | train_balanced_accuracy: 0.41623 | valid_balanced_accuracy: 0.40447 |  0:49:59s\n",
      "epoch 45 | loss: 1.42079 | train_balanced_accuracy: 0.38606 | valid_balanced_accuracy: 0.38071 |  0:51:08s\n",
      "epoch 46 | loss: 1.41937 | train_balanced_accuracy: 0.38689 | valid_balanced_accuracy: 0.37997 |  0:52:16s\n",
      "epoch 47 | loss: 1.41156 | train_balanced_accuracy: 0.38822 | valid_balanced_accuracy: 0.38041 |  0:53:24s\n",
      "epoch 48 | loss: 1.40864 | train_balanced_accuracy: 0.39621 | valid_balanced_accuracy: 0.38317 |  0:54:32s\n",
      "epoch 49 | loss: 1.41315 | train_balanced_accuracy: 0.38338 | valid_balanced_accuracy: 0.38756 |  0:55:40s\n",
      "epoch 50 | loss: 1.41798 | train_balanced_accuracy: 0.38751 | valid_balanced_accuracy: 0.38021 |  0:56:47s\n",
      "epoch 51 | loss: 1.40984 | train_balanced_accuracy: 0.40852 | valid_balanced_accuracy: 0.39371 |  0:57:55s\n",
      "epoch 52 | loss: 1.4021  | train_balanced_accuracy: 0.39073 | valid_balanced_accuracy: 0.37652 |  0:59:02s\n",
      "epoch 53 | loss: 1.39708 | train_balanced_accuracy: 0.39302 | valid_balanced_accuracy: 0.38841 |  1:00:09s\n",
      "epoch 54 | loss: 1.39865 | train_balanced_accuracy: 0.40395 | valid_balanced_accuracy: 0.39604 |  1:01:15s\n",
      "\n",
      "Early stopping occured at epoch 54 with best_epoch = 44 and best_valid_balanced_accuracy = 0.40447\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'gamma': 2.2}, optimizer_params={'lr': 0.001}, n_steps=13, n_shared=1, n_independent=3, n_a=32, momentum=0.02, lambda_sparse=0.1, clip_value=0.01, total=61.6min\n",
      "Device used : cuda\n",
      "[CV] scheduler_params={'gamma': 2.2}, optimizer_params={'lr': 0.001}, n_steps=13, n_shared=1, n_independent=3, n_a=32, momentum=0.02, lambda_sparse=0.1, clip_value=0.01 \n",
      "epoch 0  | loss: 4.67733 | train_balanced_accuracy: 0.13724 | valid_balanced_accuracy: 0.13536 |  0:01:06s\n",
      "epoch 1  | loss: 2.15981 | train_balanced_accuracy: 0.1935  | valid_balanced_accuracy: 0.19237 |  0:02:11s\n",
      "epoch 2  | loss: 1.82458 | train_balanced_accuracy: 0.24284 | valid_balanced_accuracy: 0.24205 |  0:03:16s\n",
      "epoch 3  | loss: 1.7261  | train_balanced_accuracy: 0.2545  | valid_balanced_accuracy: 0.25596 |  0:04:22s\n",
      "epoch 4  | loss: 1.67118 | train_balanced_accuracy: 0.24486 | valid_balanced_accuracy: 0.23974 |  0:05:28s\n",
      "epoch 5  | loss: 1.62915 | train_balanced_accuracy: 0.27365 | valid_balanced_accuracy: 0.27205 |  0:06:34s\n",
      "epoch 6  | loss: 1.60701 | train_balanced_accuracy: 0.29415 | valid_balanced_accuracy: 0.29168 |  0:07:40s\n",
      "epoch 7  | loss: 1.58317 | train_balanced_accuracy: 0.28706 | valid_balanced_accuracy: 0.28963 |  0:08:46s\n",
      "epoch 8  | loss: 1.5639  | train_balanced_accuracy: 0.29191 | valid_balanced_accuracy: 0.29264 |  0:09:54s\n",
      "epoch 9  | loss: 1.56241 | train_balanced_accuracy: 0.29295 | valid_balanced_accuracy: 0.28997 |  0:11:03s\n",
      "epoch 10 | loss: 1.56191 | train_balanced_accuracy: 0.29198 | valid_balanced_accuracy: 0.29976 |  0:12:11s\n",
      "epoch 11 | loss: 1.54632 | train_balanced_accuracy: 0.30034 | valid_balanced_accuracy: 0.2965  |  0:13:19s\n",
      "epoch 12 | loss: 1.53395 | train_balanced_accuracy: 0.31943 | valid_balanced_accuracy: 0.32264 |  0:14:27s\n",
      "epoch 13 | loss: 1.52381 | train_balanced_accuracy: 0.34613 | valid_balanced_accuracy: 0.34385 |  0:15:34s\n",
      "epoch 14 | loss: 1.5306  | train_balanced_accuracy: 0.3216  | valid_balanced_accuracy: 0.31999 |  0:16:42s\n",
      "epoch 15 | loss: 1.54232 | train_balanced_accuracy: 0.32859 | valid_balanced_accuracy: 0.34468 |  0:17:50s\n",
      "epoch 16 | loss: 1.52126 | train_balanced_accuracy: 0.32558 | valid_balanced_accuracy: 0.32848 |  0:18:56s\n",
      "epoch 17 | loss: 1.5098  | train_balanced_accuracy: 0.33378 | valid_balanced_accuracy: 0.33494 |  0:20:01s\n",
      "epoch 18 | loss: 1.51189 | train_balanced_accuracy: 0.35114 | valid_balanced_accuracy: 0.34656 |  0:21:09s\n",
      "epoch 19 | loss: 1.50511 | train_balanced_accuracy: 0.33956 | valid_balanced_accuracy: 0.33644 |  0:22:16s\n",
      "epoch 20 | loss: 1.5009  | train_balanced_accuracy: 0.3461  | valid_balanced_accuracy: 0.32929 |  0:23:24s\n",
      "epoch 21 | loss: 1.49647 | train_balanced_accuracy: 0.35155 | valid_balanced_accuracy: 0.34819 |  0:24:32s\n",
      "epoch 22 | loss: 1.48797 | train_balanced_accuracy: 0.35628 | valid_balanced_accuracy: 0.35643 |  0:25:40s\n",
      "epoch 23 | loss: 1.48213 | train_balanced_accuracy: 0.34117 | valid_balanced_accuracy: 0.34323 |  0:26:48s\n",
      "epoch 24 | loss: 1.48519 | train_balanced_accuracy: 0.37433 | valid_balanced_accuracy: 0.37842 |  0:27:56s\n",
      "epoch 25 | loss: 1.48218 | train_balanced_accuracy: 0.36366 | valid_balanced_accuracy: 0.3685  |  0:29:04s\n",
      "epoch 26 | loss: 1.47344 | train_balanced_accuracy: 0.39887 | valid_balanced_accuracy: 0.39998 |  0:30:12s\n",
      "epoch 27 | loss: 1.47264 | train_balanced_accuracy: 0.36985 | valid_balanced_accuracy: 0.36677 |  0:31:20s\n",
      "epoch 28 | loss: 1.47202 | train_balanced_accuracy: 0.37008 | valid_balanced_accuracy: 0.37113 |  0:32:29s\n",
      "epoch 29 | loss: 1.48555 | train_balanced_accuracy: 0.36754 | valid_balanced_accuracy: 0.36973 |  0:33:37s\n",
      "epoch 30 | loss: 1.48815 | train_balanced_accuracy: 0.3644  | valid_balanced_accuracy: 0.36748 |  0:34:45s\n",
      "epoch 31 | loss: 1.47608 | train_balanced_accuracy: 0.36266 | valid_balanced_accuracy: 0.36319 |  0:35:54s\n",
      "epoch 32 | loss: 1.48583 | train_balanced_accuracy: 0.35021 | valid_balanced_accuracy: 0.34631 |  0:37:02s\n",
      "epoch 33 | loss: 1.48589 | train_balanced_accuracy: 0.35715 | valid_balanced_accuracy: 0.35523 |  0:38:11s\n",
      "epoch 34 | loss: 1.4697  | train_balanced_accuracy: 0.37669 | valid_balanced_accuracy: 0.38601 |  0:39:20s\n",
      "epoch 35 | loss: 1.46865 | train_balanced_accuracy: 0.36621 | valid_balanced_accuracy: 0.36598 |  0:40:26s\n",
      "epoch 36 | loss: 1.4591  | train_balanced_accuracy: 0.38503 | valid_balanced_accuracy: 0.38655 |  0:41:33s\n",
      "\n",
      "Early stopping occured at epoch 36 with best_epoch = 26 and best_valid_balanced_accuracy = 0.39998\n",
      "Best weights from best epoch are automatically used!\n",
      "[CV]  scheduler_params={'gamma': 2.2}, optimizer_params={'lr': 0.001}, n_steps=13, n_shared=1, n_independent=3, n_a=32, momentum=0.02, lambda_sparse=0.1, clip_value=0.01, total=41.9min\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed: 3308.5min finished\n",
      "epoch 0  | loss: 2.61482 | train_balanced_accuracy: 0.17791 | valid_balanced_accuracy: 0.17846 |  0:01:30s\n",
      "epoch 1  | loss: 1.68434 | train_balanced_accuracy: 0.25626 | valid_balanced_accuracy: 0.25484 |  0:03:01s\n",
      "epoch 2  | loss: 1.48564 | train_balanced_accuracy: 0.30494 | valid_balanced_accuracy: 0.30448 |  0:04:32s\n",
      "epoch 3  | loss: 1.37475 | train_balanced_accuracy: 0.39165 | valid_balanced_accuracy: 0.39411 |  0:06:03s\n",
      "epoch 4  | loss: 1.30618 | train_balanced_accuracy: 0.44751 | valid_balanced_accuracy: 0.43872 |  0:07:34s\n",
      "epoch 5  | loss: 1.26262 | train_balanced_accuracy: 0.49599 | valid_balanced_accuracy: 0.48561 |  0:09:05s\n",
      "epoch 6  | loss: 1.22955 | train_balanced_accuracy: 0.47919 | valid_balanced_accuracy: 0.47389 |  0:10:35s\n",
      "epoch 7  | loss: 1.20181 | train_balanced_accuracy: 0.53766 | valid_balanced_accuracy: 0.52732 |  0:12:05s\n",
      "epoch 8  | loss: 1.17922 | train_balanced_accuracy: 0.53381 | valid_balanced_accuracy: 0.51897 |  0:13:35s\n",
      "epoch 9  | loss: 1.15799 | train_balanced_accuracy: 0.5416  | valid_balanced_accuracy: 0.53127 |  0:15:05s\n",
      "epoch 10 | loss: 1.13979 | train_balanced_accuracy: 0.56094 | valid_balanced_accuracy: 0.54895 |  0:16:35s\n",
      "epoch 11 | loss: 1.12317 | train_balanced_accuracy: 0.55572 | valid_balanced_accuracy: 0.5422  |  0:18:05s\n",
      "epoch 12 | loss: 1.10874 | train_balanced_accuracy: 0.57283 | valid_balanced_accuracy: 0.5548  |  0:19:34s\n",
      "epoch 13 | loss: 1.09142 | train_balanced_accuracy: 0.58593 | valid_balanced_accuracy: 0.56109 |  0:21:04s\n",
      "epoch 14 | loss: 1.07686 | train_balanced_accuracy: 0.59374 | valid_balanced_accuracy: 0.56523 |  0:22:34s\n",
      "epoch 15 | loss: 1.06458 | train_balanced_accuracy: 0.61311 | valid_balanced_accuracy: 0.57986 |  0:24:03s\n",
      "epoch 16 | loss: 1.05303 | train_balanced_accuracy: 0.61891 | valid_balanced_accuracy: 0.59912 |  0:25:33s\n",
      "epoch 17 | loss: 1.03787 | train_balanced_accuracy: 0.62536 | valid_balanced_accuracy: 0.59645 |  0:27:03s\n",
      "epoch 18 | loss: 1.03158 | train_balanced_accuracy: 0.64708 | valid_balanced_accuracy: 0.62467 |  0:28:34s\n",
      "epoch 19 | loss: 1.02212 | train_balanced_accuracy: 0.63324 | valid_balanced_accuracy: 0.60472 |  0:30:04s\n",
      "epoch 20 | loss: 1.01694 | train_balanced_accuracy: 0.63458 | valid_balanced_accuracy: 0.61544 |  0:31:34s\n",
      "epoch 21 | loss: 0.99856 | train_balanced_accuracy: 0.644   | valid_balanced_accuracy: 0.61739 |  0:33:07s\n",
      "epoch 22 | loss: 0.99343 | train_balanced_accuracy: 0.65859 | valid_balanced_accuracy: 0.63221 |  0:34:39s\n",
      "epoch 23 | loss: 0.98277 | train_balanced_accuracy: 0.65276 | valid_balanced_accuracy: 0.62577 |  0:36:11s\n",
      "epoch 24 | loss: 0.97656 | train_balanced_accuracy: 0.65607 | valid_balanced_accuracy: 0.62576 |  0:37:43s\n",
      "epoch 25 | loss: 0.97116 | train_balanced_accuracy: 0.65839 | valid_balanced_accuracy: 0.626   |  0:39:15s\n",
      "epoch 26 | loss: 0.96266 | train_balanced_accuracy: 0.63828 | valid_balanced_accuracy: 0.61404 |  0:40:46s\n",
      "epoch 27 | loss: 0.95735 | train_balanced_accuracy: 0.6754  | valid_balanced_accuracy: 0.63893 |  0:42:18s\n",
      "epoch 28 | loss: 0.9566  | train_balanced_accuracy: 0.67155 | valid_balanced_accuracy: 0.64347 |  0:43:50s\n",
      "epoch 29 | loss: 0.94203 | train_balanced_accuracy: 0.6809  | valid_balanced_accuracy: 0.65074 |  0:45:22s\n",
      "epoch 30 | loss: 0.93257 | train_balanced_accuracy: 0.67448 | valid_balanced_accuracy: 0.63576 |  0:46:54s\n",
      "epoch 31 | loss: 0.92862 | train_balanced_accuracy: 0.69438 | valid_balanced_accuracy: 0.65897 |  0:48:26s\n",
      "epoch 32 | loss: 0.92171 | train_balanced_accuracy: 0.68693 | valid_balanced_accuracy: 0.65997 |  0:49:57s\n",
      "epoch 33 | loss: 0.92092 | train_balanced_accuracy: 0.683   | valid_balanced_accuracy: 0.65704 |  0:51:29s\n",
      "epoch 34 | loss: 0.91561 | train_balanced_accuracy: 0.69464 | valid_balanced_accuracy: 0.65015 |  0:53:01s\n",
      "epoch 35 | loss: 0.90326 | train_balanced_accuracy: 0.69843 | valid_balanced_accuracy: 0.65812 |  0:54:33s\n",
      "epoch 36 | loss: 0.89346 | train_balanced_accuracy: 0.70298 | valid_balanced_accuracy: 0.65184 |  0:56:05s\n",
      "epoch 37 | loss: 0.88889 | train_balanced_accuracy: 0.70838 | valid_balanced_accuracy: 0.66696 |  0:57:37s\n",
      "epoch 38 | loss: 0.88328 | train_balanced_accuracy: 0.70299 | valid_balanced_accuracy: 0.66193 |  0:59:08s\n",
      "epoch 39 | loss: 0.88645 | train_balanced_accuracy: 0.69561 | valid_balanced_accuracy: 0.6526  |  1:00:40s\n",
      "epoch 40 | loss: 0.90063 | train_balanced_accuracy: 0.70686 | valid_balanced_accuracy: 0.67961 |  1:02:12s\n",
      "epoch 41 | loss: 0.89199 | train_balanced_accuracy: 0.68816 | valid_balanced_accuracy: 0.64254 |  1:03:44s\n",
      "epoch 42 | loss: 0.88075 | train_balanced_accuracy: 0.69587 | valid_balanced_accuracy: 0.6544  |  1:05:16s\n",
      "epoch 43 | loss: 0.87555 | train_balanced_accuracy: 0.69133 | valid_balanced_accuracy: 0.64966 |  1:06:48s\n",
      "epoch 44 | loss: 0.86492 | train_balanced_accuracy: 0.69635 | valid_balanced_accuracy: 0.65031 |  1:08:20s\n",
      "epoch 45 | loss: 0.85945 | train_balanced_accuracy: 0.71261 | valid_balanced_accuracy: 0.66001 |  1:09:52s\n",
      "epoch 46 | loss: 0.8566  | train_balanced_accuracy: 0.72547 | valid_balanced_accuracy: 0.68833 |  1:11:24s\n",
      "epoch 47 | loss: 0.84792 | train_balanced_accuracy: 0.72611 | valid_balanced_accuracy: 0.66508 |  1:12:56s\n",
      "epoch 48 | loss: 0.84078 | train_balanced_accuracy: 0.71728 | valid_balanced_accuracy: 0.66676 |  1:14:28s\n",
      "epoch 49 | loss: 0.83664 | train_balanced_accuracy: 0.73132 | valid_balanced_accuracy: 0.6909  |  1:16:00s\n",
      "epoch 50 | loss: 0.82865 | train_balanced_accuracy: 0.72183 | valid_balanced_accuracy: 0.68085 |  1:17:32s\n",
      "epoch 51 | loss: 0.85979 | train_balanced_accuracy: 0.73638 | valid_balanced_accuracy: 0.69222 |  1:19:04s\n",
      "epoch 52 | loss: 0.87266 | train_balanced_accuracy: 0.73051 | valid_balanced_accuracy: 0.67948 |  1:20:36s\n",
      "epoch 53 | loss: 0.85346 | train_balanced_accuracy: 0.73991 | valid_balanced_accuracy: 0.68737 |  1:22:08s\n",
      "epoch 54 | loss: 0.8447  | train_balanced_accuracy: 0.73705 | valid_balanced_accuracy: 0.68654 |  1:23:40s\n",
      "epoch 55 | loss: 0.83651 | train_balanced_accuracy: 0.71682 | valid_balanced_accuracy: 0.6617  |  1:25:12s\n",
      "epoch 56 | loss: 0.83433 | train_balanced_accuracy: 0.73545 | valid_balanced_accuracy: 0.67354 |  1:26:44s\n",
      "epoch 57 | loss: 0.82359 | train_balanced_accuracy: 0.73101 | valid_balanced_accuracy: 0.67317 |  1:28:16s\n",
      "epoch 58 | loss: 0.82489 | train_balanced_accuracy: 0.7389  | valid_balanced_accuracy: 0.66124 |  1:29:48s\n",
      "epoch 59 | loss: 0.81699 | train_balanced_accuracy: 0.74909 | valid_balanced_accuracy: 0.67795 |  1:31:19s\n",
      "epoch 60 | loss: 0.81177 | train_balanced_accuracy: 0.72489 | valid_balanced_accuracy: 0.67556 |  1:32:51s\n",
      "epoch 61 | loss: 0.81092 | train_balanced_accuracy: 0.73741 | valid_balanced_accuracy: 0.68764 |  1:34:23s\n",
      "\n",
      "Early stopping occured at epoch 61 with best_epoch = 51 and best_valid_balanced_accuracy = 0.69222\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'scheduler_params': {'step_size': 100},\n",
       " 'optimizer_params': {'lr': 0.001},\n",
       " 'n_steps': 5,\n",
       " 'n_shared': 4,\n",
       " 'n_independent': 9,\n",
       " 'n_a': 64,\n",
       " 'momentum': 0.02,\n",
       " 'lambda_sparse': 0.0001,\n",
       " 'clip_value': 0.5}"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'scheduler_params': {'step_size': 100},\n",
    " 'optimizer_params': {'lr': 0.001},\n",
    " 'n_steps': 5,\n",
    " 'n_shared': 4,\n",
    " 'n_independent': 9,\n",
    " 'n_a': 64,\n",
    " 'momentum': 0.02,\n",
    " 'lambda_sparse': 0.0001,\n",
    " 'clip_value': 0.5}"
   ]
  },
  {
   "source": [
    "pd.DataFrame.from_dict(rand_search.cv_results_ ).sort_values(by=['mean_test_score'], ascending=False).head(n=8)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "1    4287.086841   1256.568140         5.156577        0.081830   \n",
       "5    4617.684546    519.186678         5.645451        0.075080   \n",
       "7    7416.739716   1390.676540         6.473474        0.090547   \n",
       "9    3104.275796    381.179701         4.840958        0.018833   \n",
       "8    2707.560916     10.195978         2.103405        0.041678   \n",
       "3    4611.880223    471.175585         3.673793        0.060979   \n",
       "0    3137.621446   2083.453239         4.915102        0.086165   \n",
       "6    4264.526344    947.611768         3.794573        0.067246   \n",
       "\n",
       "  param_scheduler_params param_optimizer_params param_n_steps param_n_shared  \\\n",
       "1     {'step_size': 100}          {'lr': 0.001}             5              4   \n",
       "5      {'step_size': 10}           {'lr': 0.01}             8              5   \n",
       "7      {'step_size': 50}          {'lr': 0.001}            13              1   \n",
       "9         {'gamma': 2.2}          {'lr': 0.001}            13              1   \n",
       "8         {'gamma': 1.8}         {'lr': 0.0001}             3              5   \n",
       "3     {'step_size': 100}          {'lr': 1e-05}            11              1   \n",
       "0           {'gamma': 4}          {'lr': 1e-05}            13              1   \n",
       "6         {'gamma': 1.8}          {'lr': 1e-05}             8              4   \n",
       "\n",
       "  param_n_independent param_n_a param_momentum param_lambda_sparse  \\\n",
       "1                   9        64           0.02              0.0001   \n",
       "5                   4        32           0.02               0.001   \n",
       "7                   5        16          0.005              0.0001   \n",
       "9                   3        32           0.02                 0.1   \n",
       "8                   1        16           0.02               0.001   \n",
       "3                   2        64           0.05                   1   \n",
       "0                   3        64              1              0.0001   \n",
       "6                   1        16           0.05                0.01   \n",
       "\n",
       "  param_clip_value                                             params  \\\n",
       "1              0.5  {'scheduler_params': {'step_size': 100}, 'opti...   \n",
       "5               10  {'scheduler_params': {'step_size': 10}, 'optim...   \n",
       "7                1  {'scheduler_params': {'step_size': 50}, 'optim...   \n",
       "9             0.01  {'scheduler_params': {'gamma': 2.2}, 'optimize...   \n",
       "8                1  {'scheduler_params': {'gamma': 1.8}, 'optimize...   \n",
       "3              0.5  {'scheduler_params': {'step_size': 100}, 'opti...   \n",
       "0              0.5  {'scheduler_params': {'gamma': 4}, 'optimizer_...   \n",
       "6             0.01  {'scheduler_params': {'gamma': 1.8}, 'optimize...   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  split3_test_score  \\\n",
       "1           0.587833           0.660374           0.634304           0.653722   \n",
       "5           0.535187           0.622008           0.592963           0.563562   \n",
       "7           0.388530           0.435094           0.377682           0.487336   \n",
       "9           0.413071           0.375962           0.379060           0.402894   \n",
       "8           0.297821           0.280484           0.275137           0.298015   \n",
       "3           0.221926           0.227413           0.196623           0.225381   \n",
       "0           0.052507           0.152857           0.185352           0.152530   \n",
       "6           0.062429           0.062485           0.082819           0.081501   \n",
       "\n",
       "   split4_test_score  mean_test_score  std_test_score  rank_test_score  \\\n",
       "1           0.676414         0.642530        0.030500                1   \n",
       "5           0.649921         0.592728        0.040719                2   \n",
       "7           0.399400         0.417608        0.039847                3   \n",
       "9           0.398001         0.393798        0.014193                4   \n",
       "8           0.296687         0.289629        0.009807                5   \n",
       "3           0.226288         0.219526        0.011597                6   \n",
       "0           0.050208         0.118691        0.056261                7   \n",
       "6           0.079875         0.073822        0.009326                8   \n",
       "\n",
       "   split0_train_score  split1_train_score  split2_train_score  \\\n",
       "1            0.613380            0.691298            0.675599   \n",
       "5            0.558782            0.620514            0.611252   \n",
       "7            0.397032            0.448991            0.375400   \n",
       "9            0.406883            0.375021            0.383196   \n",
       "8            0.299968            0.280256            0.280890   \n",
       "3            0.227173            0.227456            0.197798   \n",
       "0            0.052475            0.155367            0.185305   \n",
       "6            0.063256            0.063415            0.083446   \n",
       "\n",
       "   split3_train_score  split4_train_score  mean_train_score  std_train_score  \n",
       "1            0.706655            0.721255          0.681637         0.037375  \n",
       "5            0.594325            0.672667          0.611508         0.037140  \n",
       "7            0.492513            0.402724          0.423332         0.042091  \n",
       "9            0.413753            0.399174          0.395605         0.014471  \n",
       "8            0.299941            0.298394          0.291890         0.009260  \n",
       "3            0.227704            0.223083          0.220643         0.011548  \n",
       "0            0.152542            0.049013          0.118940         0.056864  \n",
       "6            0.081015            0.079626          0.074152         0.008916  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_scheduler_params</th>\n      <th>param_optimizer_params</th>\n      <th>param_n_steps</th>\n      <th>param_n_shared</th>\n      <th>param_n_independent</th>\n      <th>param_n_a</th>\n      <th>param_momentum</th>\n      <th>param_lambda_sparse</th>\n      <th>param_clip_value</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n      <th>split0_train_score</th>\n      <th>split1_train_score</th>\n      <th>split2_train_score</th>\n      <th>split3_train_score</th>\n      <th>split4_train_score</th>\n      <th>mean_train_score</th>\n      <th>std_train_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>4287.086841</td>\n      <td>1256.568140</td>\n      <td>5.156577</td>\n      <td>0.081830</td>\n      <td>{'step_size': 100}</td>\n      <td>{'lr': 0.001}</td>\n      <td>5</td>\n      <td>4</td>\n      <td>9</td>\n      <td>64</td>\n      <td>0.02</td>\n      <td>0.0001</td>\n      <td>0.5</td>\n      <td>{'scheduler_params': {'step_size': 100}, 'opti...</td>\n      <td>0.587833</td>\n      <td>0.660374</td>\n      <td>0.634304</td>\n      <td>0.653722</td>\n      <td>0.676414</td>\n      <td>0.642530</td>\n      <td>0.030500</td>\n      <td>1</td>\n      <td>0.613380</td>\n      <td>0.691298</td>\n      <td>0.675599</td>\n      <td>0.706655</td>\n      <td>0.721255</td>\n      <td>0.681637</td>\n      <td>0.037375</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>4617.684546</td>\n      <td>519.186678</td>\n      <td>5.645451</td>\n      <td>0.075080</td>\n      <td>{'step_size': 10}</td>\n      <td>{'lr': 0.01}</td>\n      <td>8</td>\n      <td>5</td>\n      <td>4</td>\n      <td>32</td>\n      <td>0.02</td>\n      <td>0.001</td>\n      <td>10</td>\n      <td>{'scheduler_params': {'step_size': 10}, 'optim...</td>\n      <td>0.535187</td>\n      <td>0.622008</td>\n      <td>0.592963</td>\n      <td>0.563562</td>\n      <td>0.649921</td>\n      <td>0.592728</td>\n      <td>0.040719</td>\n      <td>2</td>\n      <td>0.558782</td>\n      <td>0.620514</td>\n      <td>0.611252</td>\n      <td>0.594325</td>\n      <td>0.672667</td>\n      <td>0.611508</td>\n      <td>0.037140</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7416.739716</td>\n      <td>1390.676540</td>\n      <td>6.473474</td>\n      <td>0.090547</td>\n      <td>{'step_size': 50}</td>\n      <td>{'lr': 0.001}</td>\n      <td>13</td>\n      <td>1</td>\n      <td>5</td>\n      <td>16</td>\n      <td>0.005</td>\n      <td>0.0001</td>\n      <td>1</td>\n      <td>{'scheduler_params': {'step_size': 50}, 'optim...</td>\n      <td>0.388530</td>\n      <td>0.435094</td>\n      <td>0.377682</td>\n      <td>0.487336</td>\n      <td>0.399400</td>\n      <td>0.417608</td>\n      <td>0.039847</td>\n      <td>3</td>\n      <td>0.397032</td>\n      <td>0.448991</td>\n      <td>0.375400</td>\n      <td>0.492513</td>\n      <td>0.402724</td>\n      <td>0.423332</td>\n      <td>0.042091</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>3104.275796</td>\n      <td>381.179701</td>\n      <td>4.840958</td>\n      <td>0.018833</td>\n      <td>{'gamma': 2.2}</td>\n      <td>{'lr': 0.001}</td>\n      <td>13</td>\n      <td>1</td>\n      <td>3</td>\n      <td>32</td>\n      <td>0.02</td>\n      <td>0.1</td>\n      <td>0.01</td>\n      <td>{'scheduler_params': {'gamma': 2.2}, 'optimize...</td>\n      <td>0.413071</td>\n      <td>0.375962</td>\n      <td>0.379060</td>\n      <td>0.402894</td>\n      <td>0.398001</td>\n      <td>0.393798</td>\n      <td>0.014193</td>\n      <td>4</td>\n      <td>0.406883</td>\n      <td>0.375021</td>\n      <td>0.383196</td>\n      <td>0.413753</td>\n      <td>0.399174</td>\n      <td>0.395605</td>\n      <td>0.014471</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2707.560916</td>\n      <td>10.195978</td>\n      <td>2.103405</td>\n      <td>0.041678</td>\n      <td>{'gamma': 1.8}</td>\n      <td>{'lr': 0.0001}</td>\n      <td>3</td>\n      <td>5</td>\n      <td>1</td>\n      <td>16</td>\n      <td>0.02</td>\n      <td>0.001</td>\n      <td>1</td>\n      <td>{'scheduler_params': {'gamma': 1.8}, 'optimize...</td>\n      <td>0.297821</td>\n      <td>0.280484</td>\n      <td>0.275137</td>\n      <td>0.298015</td>\n      <td>0.296687</td>\n      <td>0.289629</td>\n      <td>0.009807</td>\n      <td>5</td>\n      <td>0.299968</td>\n      <td>0.280256</td>\n      <td>0.280890</td>\n      <td>0.299941</td>\n      <td>0.298394</td>\n      <td>0.291890</td>\n      <td>0.009260</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4611.880223</td>\n      <td>471.175585</td>\n      <td>3.673793</td>\n      <td>0.060979</td>\n      <td>{'step_size': 100}</td>\n      <td>{'lr': 1e-05}</td>\n      <td>11</td>\n      <td>1</td>\n      <td>2</td>\n      <td>64</td>\n      <td>0.05</td>\n      <td>1</td>\n      <td>0.5</td>\n      <td>{'scheduler_params': {'step_size': 100}, 'opti...</td>\n      <td>0.221926</td>\n      <td>0.227413</td>\n      <td>0.196623</td>\n      <td>0.225381</td>\n      <td>0.226288</td>\n      <td>0.219526</td>\n      <td>0.011597</td>\n      <td>6</td>\n      <td>0.227173</td>\n      <td>0.227456</td>\n      <td>0.197798</td>\n      <td>0.227704</td>\n      <td>0.223083</td>\n      <td>0.220643</td>\n      <td>0.011548</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>3137.621446</td>\n      <td>2083.453239</td>\n      <td>4.915102</td>\n      <td>0.086165</td>\n      <td>{'gamma': 4}</td>\n      <td>{'lr': 1e-05}</td>\n      <td>13</td>\n      <td>1</td>\n      <td>3</td>\n      <td>64</td>\n      <td>1</td>\n      <td>0.0001</td>\n      <td>0.5</td>\n      <td>{'scheduler_params': {'gamma': 4}, 'optimizer_...</td>\n      <td>0.052507</td>\n      <td>0.152857</td>\n      <td>0.185352</td>\n      <td>0.152530</td>\n      <td>0.050208</td>\n      <td>0.118691</td>\n      <td>0.056261</td>\n      <td>7</td>\n      <td>0.052475</td>\n      <td>0.155367</td>\n      <td>0.185305</td>\n      <td>0.152542</td>\n      <td>0.049013</td>\n      <td>0.118940</td>\n      <td>0.056864</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>4264.526344</td>\n      <td>947.611768</td>\n      <td>3.794573</td>\n      <td>0.067246</td>\n      <td>{'gamma': 1.8}</td>\n      <td>{'lr': 1e-05}</td>\n      <td>8</td>\n      <td>4</td>\n      <td>1</td>\n      <td>16</td>\n      <td>0.05</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>{'scheduler_params': {'gamma': 1.8}, 'optimize...</td>\n      <td>0.062429</td>\n      <td>0.062485</td>\n      <td>0.082819</td>\n      <td>0.081501</td>\n      <td>0.079875</td>\n      <td>0.073822</td>\n      <td>0.009326</td>\n      <td>8</td>\n      <td>0.063256</td>\n      <td>0.063415</td>\n      <td>0.083446</td>\n      <td>0.081015</td>\n      <td>0.079626</td>\n      <td>0.074152</td>\n      <td>0.008916</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 26
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(rand_search.cv_results_ ).to_csv('../models/results/random_search1_tabnet.csv')"
   ]
  },
  {
   "source": [
    "        * Model Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device used : cuda\n"
     ]
    }
   ],
   "source": [
    "tab_net = TabNetClassifier(**rand_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0  | loss: 3.79265 | train_balanced_accuracy: 0.05441 | valid_balanced_accuracy: 0.05289 |  0:01:21s\n",
      "epoch 1  | loss: 2.74821 | train_balanced_accuracy: 0.11202 | valid_balanced_accuracy: 0.11334 |  0:02:42s\n",
      "epoch 2  | loss: 2.27104 | train_balanced_accuracy: 0.11751 | valid_balanced_accuracy: 0.11996 |  0:04:03s\n",
      "epoch 3  | loss: 2.1082  | train_balanced_accuracy: 0.14668 | valid_balanced_accuracy: 0.14963 |  0:05:25s\n",
      "epoch 4  | loss: 1.93652 | train_balanced_accuracy: 0.17071 | valid_balanced_accuracy: 0.17348 |  0:06:46s\n",
      "epoch 5  | loss: 1.75609 | train_balanced_accuracy: 0.2006  | valid_balanced_accuracy: 0.20455 |  0:08:08s\n",
      "epoch 6  | loss: 1.62182 | train_balanced_accuracy: 0.21834 | valid_balanced_accuracy: 0.22135 |  0:09:29s\n",
      "epoch 7  | loss: 1.5236  | train_balanced_accuracy: 0.2551  | valid_balanced_accuracy: 0.25802 |  0:10:51s\n",
      "epoch 8  | loss: 1.46933 | train_balanced_accuracy: 0.28253 | valid_balanced_accuracy: 0.2818  |  0:12:12s\n",
      "epoch 9  | loss: 1.43349 | train_balanced_accuracy: 0.29467 | valid_balanced_accuracy: 0.29659 |  0:13:33s\n",
      "epoch 10 | loss: 1.40362 | train_balanced_accuracy: 0.3209  | valid_balanced_accuracy: 0.32214 |  0:14:55s\n",
      "epoch 11 | loss: 1.3699  | train_balanced_accuracy: 0.33946 | valid_balanced_accuracy: 0.34207 |  0:16:16s\n",
      "epoch 12 | loss: 1.33438 | train_balanced_accuracy: 0.35201 | valid_balanced_accuracy: 0.35346 |  0:17:37s\n",
      "epoch 13 | loss: 1.30964 | train_balanced_accuracy: 0.3516  | valid_balanced_accuracy: 0.34871 |  0:18:59s\n",
      "epoch 14 | loss: 1.28987 | train_balanced_accuracy: 0.39374 | valid_balanced_accuracy: 0.39361 |  0:20:20s\n",
      "epoch 15 | loss: 1.27515 | train_balanced_accuracy: 0.39213 | valid_balanced_accuracy: 0.38905 |  0:21:41s\n",
      "epoch 16 | loss: 1.25697 | train_balanced_accuracy: 0.41547 | valid_balanced_accuracy: 0.41594 |  0:23:03s\n",
      "epoch 17 | loss: 1.24439 | train_balanced_accuracy: 0.42569 | valid_balanced_accuracy: 0.42213 |  0:24:24s\n",
      "epoch 18 | loss: 1.22974 | train_balanced_accuracy: 0.42902 | valid_balanced_accuracy: 0.42571 |  0:25:45s\n",
      "epoch 19 | loss: 1.22623 | train_balanced_accuracy: 0.43153 | valid_balanced_accuracy: 0.42583 |  0:27:06s\n",
      "epoch 20 | loss: 1.20273 | train_balanced_accuracy: 0.44262 | valid_balanced_accuracy: 0.43579 |  0:28:27s\n",
      "epoch 21 | loss: 1.18858 | train_balanced_accuracy: 0.44806 | valid_balanced_accuracy: 0.44445 |  0:29:48s\n",
      "epoch 22 | loss: 1.18453 | train_balanced_accuracy: 0.44963 | valid_balanced_accuracy: 0.44195 |  0:31:08s\n",
      "epoch 23 | loss: 1.19276 | train_balanced_accuracy: 0.45595 | valid_balanced_accuracy: 0.45342 |  0:32:28s\n",
      "epoch 24 | loss: 1.18518 | train_balanced_accuracy: 0.46011 | valid_balanced_accuracy: 0.45882 |  0:33:48s\n",
      "epoch 25 | loss: 1.17613 | train_balanced_accuracy: 0.45355 | valid_balanced_accuracy: 0.44856 |  0:35:08s\n",
      "epoch 26 | loss: 1.17031 | train_balanced_accuracy: 0.46919 | valid_balanced_accuracy: 0.46547 |  0:36:28s\n",
      "epoch 27 | loss: 1.16765 | train_balanced_accuracy: 0.46989 | valid_balanced_accuracy: 0.47057 |  0:37:46s\n",
      "epoch 28 | loss: 1.16155 | train_balanced_accuracy: 0.46781 | valid_balanced_accuracy: 0.46902 |  0:39:05s\n",
      "epoch 29 | loss: 1.1566  | train_balanced_accuracy: 0.48006 | valid_balanced_accuracy: 0.47434 |  0:40:24s\n",
      "epoch 30 | loss: 1.15033 | train_balanced_accuracy: 0.49661 | valid_balanced_accuracy: 0.49513 |  0:41:43s\n",
      "epoch 31 | loss: 1.1402  | train_balanced_accuracy: 0.49304 | valid_balanced_accuracy: 0.49172 |  0:43:02s\n",
      "epoch 32 | loss: 1.13756 | train_balanced_accuracy: 0.49876 | valid_balanced_accuracy: 0.50012 |  0:44:21s\n",
      "epoch 33 | loss: 1.12934 | train_balanced_accuracy: 0.51493 | valid_balanced_accuracy: 0.51259 |  0:45:40s\n",
      "epoch 34 | loss: 1.12345 | train_balanced_accuracy: 0.51261 | valid_balanced_accuracy: 0.51298 |  0:46:58s\n",
      "epoch 35 | loss: 1.11569 | train_balanced_accuracy: 0.52535 | valid_balanced_accuracy: 0.52547 |  0:48:16s\n",
      "epoch 36 | loss: 1.10839 | train_balanced_accuracy: 0.52092 | valid_balanced_accuracy: 0.51278 |  0:49:34s\n",
      "epoch 37 | loss: 1.10276 | train_balanced_accuracy: 0.52155 | valid_balanced_accuracy: 0.51894 |  0:50:53s\n",
      "epoch 38 | loss: 1.10134 | train_balanced_accuracy: 0.53027 | valid_balanced_accuracy: 0.52863 |  0:52:11s\n",
      "epoch 39 | loss: 1.09663 | train_balanced_accuracy: 0.53439 | valid_balanced_accuracy: 0.52834 |  0:53:29s\n",
      "epoch 40 | loss: 1.09841 | train_balanced_accuracy: 0.52486 | valid_balanced_accuracy: 0.52477 |  0:54:47s\n",
      "epoch 41 | loss: 1.09278 | train_balanced_accuracy: 0.53794 | valid_balanced_accuracy: 0.5352  |  0:56:05s\n",
      "epoch 42 | loss: 1.08974 | train_balanced_accuracy: 0.54668 | valid_balanced_accuracy: 0.54554 |  0:57:24s\n",
      "epoch 43 | loss: 1.08665 | train_balanced_accuracy: 0.54652 | valid_balanced_accuracy: 0.54244 |  0:58:42s\n",
      "epoch 44 | loss: 1.08404 | train_balanced_accuracy: 0.53828 | valid_balanced_accuracy: 0.53386 |  1:00:00s\n",
      "epoch 45 | loss: 1.08271 | train_balanced_accuracy: 0.53925 | valid_balanced_accuracy: 0.53794 |  1:01:18s\n",
      "epoch 46 | loss: 1.07956 | train_balanced_accuracy: 0.54344 | valid_balanced_accuracy: 0.53953 |  1:02:36s\n",
      "epoch 47 | loss: 1.08663 | train_balanced_accuracy: 0.51499 | valid_balanced_accuracy: 0.51745 |  1:03:54s\n",
      "epoch 48 | loss: 1.08352 | train_balanced_accuracy: 0.525   | valid_balanced_accuracy: 0.51735 |  1:05:12s\n",
      "epoch 49 | loss: 1.07807 | train_balanced_accuracy: 0.52268 | valid_balanced_accuracy: 0.51728 |  1:06:30s\n",
      "epoch 50 | loss: 1.07238 | train_balanced_accuracy: 0.55376 | valid_balanced_accuracy: 0.54545 |  1:07:48s\n",
      "epoch 51 | loss: 1.07238 | train_balanced_accuracy: 0.54636 | valid_balanced_accuracy: 0.53972 |  1:09:06s\n",
      "epoch 52 | loss: 1.07038 | train_balanced_accuracy: 0.54609 | valid_balanced_accuracy: 0.53679 |  1:10:24s\n",
      "epoch 53 | loss: 1.06854 | train_balanced_accuracy: 0.55585 | valid_balanced_accuracy: 0.54594 |  1:11:43s\n",
      "epoch 54 | loss: 1.06914 | train_balanced_accuracy: 0.56175 | valid_balanced_accuracy: 0.55665 |  1:13:01s\n",
      "epoch 55 | loss: 1.06653 | train_balanced_accuracy: 0.5695  | valid_balanced_accuracy: 0.56344 |  1:14:19s\n",
      "epoch 56 | loss: 1.05991 | train_balanced_accuracy: 0.5712  | valid_balanced_accuracy: 0.56381 |  1:15:37s\n",
      "epoch 57 | loss: 1.05602 | train_balanced_accuracy: 0.57311 | valid_balanced_accuracy: 0.56239 |  1:16:55s\n",
      "epoch 58 | loss: 1.05171 | train_balanced_accuracy: 0.5736  | valid_balanced_accuracy: 0.56074 |  1:18:13s\n",
      "epoch 59 | loss: 1.05374 | train_balanced_accuracy: 0.5787  | valid_balanced_accuracy: 0.5689  |  1:19:31s\n",
      "epoch 60 | loss: 1.04728 | train_balanced_accuracy: 0.58337 | valid_balanced_accuracy: 0.57375 |  1:20:49s\n",
      "epoch 61 | loss: 1.04604 | train_balanced_accuracy: 0.57546 | valid_balanced_accuracy: 0.56776 |  1:22:07s\n",
      "epoch 62 | loss: 1.04149 | train_balanced_accuracy: 0.58477 | valid_balanced_accuracy: 0.57243 |  1:23:25s\n",
      "epoch 63 | loss: 1.04099 | train_balanced_accuracy: 0.57791 | valid_balanced_accuracy: 0.56728 |  1:24:43s\n",
      "epoch 64 | loss: 1.03536 | train_balanced_accuracy: 0.58434 | valid_balanced_accuracy: 0.57438 |  1:26:02s\n",
      "epoch 65 | loss: 1.04042 | train_balanced_accuracy: 0.58395 | valid_balanced_accuracy: 0.57667 |  1:27:20s\n",
      "epoch 66 | loss: 1.03786 | train_balanced_accuracy: 0.58518 | valid_balanced_accuracy: 0.57196 |  1:28:38s\n",
      "epoch 67 | loss: 1.03477 | train_balanced_accuracy: 0.57773 | valid_balanced_accuracy: 0.56425 |  1:29:56s\n",
      "epoch 68 | loss: 1.03532 | train_balanced_accuracy: 0.59142 | valid_balanced_accuracy: 0.57857 |  1:31:14s\n",
      "epoch 69 | loss: 1.03865 | train_balanced_accuracy: 0.59265 | valid_balanced_accuracy: 0.58082 |  1:32:33s\n",
      "epoch 70 | loss: 1.04007 | train_balanced_accuracy: 0.58374 | valid_balanced_accuracy: 0.57038 |  1:33:51s\n",
      "epoch 71 | loss: 1.03275 | train_balanced_accuracy: 0.59205 | valid_balanced_accuracy: 0.58434 |  1:35:09s\n",
      "epoch 72 | loss: 1.03243 | train_balanced_accuracy: 0.58869 | valid_balanced_accuracy: 0.58046 |  1:36:28s\n",
      "epoch 73 | loss: 1.03693 | train_balanced_accuracy: 0.58584 | valid_balanced_accuracy: 0.56901 |  1:37:46s\n",
      "epoch 74 | loss: 1.03407 | train_balanced_accuracy: 0.58848 | valid_balanced_accuracy: 0.57661 |  1:39:04s\n",
      "epoch 75 | loss: 1.03549 | train_balanced_accuracy: 0.59536 | valid_balanced_accuracy: 0.57806 |  1:40:22s\n",
      "epoch 76 | loss: 1.03611 | train_balanced_accuracy: 0.58175 | valid_balanced_accuracy: 0.57014 |  1:41:40s\n",
      "epoch 77 | loss: 1.03745 | train_balanced_accuracy: 0.58793 | valid_balanced_accuracy: 0.57938 |  1:42:58s\n",
      "epoch 78 | loss: 1.03146 | train_balanced_accuracy: 0.58099 | valid_balanced_accuracy: 0.57173 |  1:44:16s\n",
      "epoch 79 | loss: 1.03148 | train_balanced_accuracy: 0.58961 | valid_balanced_accuracy: 0.58139 |  1:45:35s\n",
      "epoch 80 | loss: 1.02871 | train_balanced_accuracy: 0.5859  | valid_balanced_accuracy: 0.57978 |  1:46:52s\n",
      "epoch 81 | loss: 1.02791 | train_balanced_accuracy: 0.59595 | valid_balanced_accuracy: 0.5881  |  1:48:11s\n",
      "epoch 82 | loss: 1.02643 | train_balanced_accuracy: 0.58574 | valid_balanced_accuracy: 0.57675 |  1:49:29s\n",
      "epoch 83 | loss: 1.02856 | train_balanced_accuracy: 0.59621 | valid_balanced_accuracy: 0.58503 |  1:50:47s\n",
      "epoch 84 | loss: 1.02898 | train_balanced_accuracy: 0.59489 | valid_balanced_accuracy: 0.5862  |  1:52:05s\n",
      "epoch 85 | loss: 1.02893 | train_balanced_accuracy: 0.59381 | valid_balanced_accuracy: 0.58251 |  1:53:23s\n",
      "epoch 86 | loss: 1.02361 | train_balanced_accuracy: 0.60062 | valid_balanced_accuracy: 0.58959 |  1:54:41s\n",
      "epoch 87 | loss: 1.02635 | train_balanced_accuracy: 0.59581 | valid_balanced_accuracy: 0.58575 |  1:55:59s\n",
      "epoch 88 | loss: 1.02406 | train_balanced_accuracy: 0.60198 | valid_balanced_accuracy: 0.5915  |  1:57:17s\n",
      "epoch 89 | loss: 1.02596 | train_balanced_accuracy: 0.59497 | valid_balanced_accuracy: 0.58433 |  1:58:36s\n",
      "epoch 90 | loss: 1.01988 | train_balanced_accuracy: 0.59311 | valid_balanced_accuracy: 0.58287 |  1:59:55s\n",
      "epoch 91 | loss: 1.02243 | train_balanced_accuracy: 0.59819 | valid_balanced_accuracy: 0.58563 |  2:01:14s\n",
      "epoch 92 | loss: 1.01956 | train_balanced_accuracy: 0.60101 | valid_balanced_accuracy: 0.58919 |  2:02:34s\n",
      "epoch 93 | loss: 1.02134 | train_balanced_accuracy: 0.59894 | valid_balanced_accuracy: 0.59182 |  2:03:54s\n",
      "epoch 94 | loss: 1.02061 | train_balanced_accuracy: 0.60073 | valid_balanced_accuracy: 0.59708 |  2:05:14s\n",
      "epoch 95 | loss: 1.02173 | train_balanced_accuracy: 0.59989 | valid_balanced_accuracy: 0.59076 |  2:06:35s\n",
      "epoch 96 | loss: 1.01585 | train_balanced_accuracy: 0.60646 | valid_balanced_accuracy: 0.60053 |  2:07:56s\n",
      "epoch 97 | loss: 1.01253 | train_balanced_accuracy: 0.59548 | valid_balanced_accuracy: 0.5867  |  2:09:17s\n",
      "epoch 98 | loss: 1.01047 | train_balanced_accuracy: 0.59631 | valid_balanced_accuracy: 0.59206 |  2:10:36s\n",
      "epoch 99 | loss: 1.00665 | train_balanced_accuracy: 0.60333 | valid_balanced_accuracy: 0.59813 |  2:11:55s\n",
      "epoch 100| loss: 1.00767 | train_balanced_accuracy: 0.61498 | valid_balanced_accuracy: 0.60627 |  2:13:13s\n",
      "epoch 101| loss: 1.0062  | train_balanced_accuracy: 0.61189 | valid_balanced_accuracy: 0.60301 |  2:14:32s\n",
      "epoch 102| loss: 1.00746 | train_balanced_accuracy: 0.59937 | valid_balanced_accuracy: 0.59948 |  2:15:51s\n",
      "epoch 103| loss: 1.00828 | train_balanced_accuracy: 0.61239 | valid_balanced_accuracy: 0.60649 |  2:17:10s\n",
      "epoch 104| loss: 1.00127 | train_balanced_accuracy: 0.59807 | valid_balanced_accuracy: 0.59674 |  2:18:28s\n",
      "epoch 105| loss: 0.99701 | train_balanced_accuracy: 0.6132  | valid_balanced_accuracy: 0.6076  |  2:19:46s\n",
      "epoch 106| loss: 0.99408 | train_balanced_accuracy: 0.61134 | valid_balanced_accuracy: 0.60869 |  2:21:05s\n",
      "epoch 107| loss: 0.99421 | train_balanced_accuracy: 0.61205 | valid_balanced_accuracy: 0.60742 |  2:22:23s\n",
      "epoch 108| loss: 0.99126 | train_balanced_accuracy: 0.6167  | valid_balanced_accuracy: 0.61244 |  2:23:41s\n",
      "epoch 109| loss: 0.99234 | train_balanced_accuracy: 0.60952 | valid_balanced_accuracy: 0.60023 |  2:25:00s\n",
      "epoch 110| loss: 0.99388 | train_balanced_accuracy: 0.61675 | valid_balanced_accuracy: 0.61432 |  2:26:19s\n",
      "epoch 111| loss: 0.99126 | train_balanced_accuracy: 0.61483 | valid_balanced_accuracy: 0.61022 |  2:27:38s\n",
      "epoch 112| loss: 0.9912  | train_balanced_accuracy: 0.60124 | valid_balanced_accuracy: 0.595   |  2:28:57s\n",
      "epoch 113| loss: 0.99503 | train_balanced_accuracy: 0.61189 | valid_balanced_accuracy: 0.60193 |  2:30:16s\n",
      "epoch 114| loss: 0.99015 | train_balanced_accuracy: 0.60522 | valid_balanced_accuracy: 0.59815 |  2:31:34s\n",
      "epoch 115| loss: 0.99004 | train_balanced_accuracy: 0.62117 | valid_balanced_accuracy: 0.6179  |  2:32:54s\n",
      "epoch 116| loss: 0.9874  | train_balanced_accuracy: 0.61815 | valid_balanced_accuracy: 0.60973 |  2:34:13s\n",
      "epoch 117| loss: 0.9854  | train_balanced_accuracy: 0.61801 | valid_balanced_accuracy: 0.61228 |  2:35:32s\n",
      "epoch 118| loss: 0.9841  | train_balanced_accuracy: 0.61917 | valid_balanced_accuracy: 0.61412 |  2:36:51s\n",
      "epoch 119| loss: 0.9777  | train_balanced_accuracy: 0.62208 | valid_balanced_accuracy: 0.61742 |  2:38:09s\n",
      "epoch 120| loss: 0.9826  | train_balanced_accuracy: 0.62021 | valid_balanced_accuracy: 0.61663 |  2:39:27s\n",
      "epoch 121| loss: 0.98271 | train_balanced_accuracy: 0.62837 | valid_balanced_accuracy: 0.6234  |  2:40:46s\n",
      "epoch 122| loss: 0.97903 | train_balanced_accuracy: 0.61572 | valid_balanced_accuracy: 0.61122 |  2:42:05s\n",
      "epoch 123| loss: 0.97737 | train_balanced_accuracy: 0.61869 | valid_balanced_accuracy: 0.6125  |  2:43:24s\n",
      "epoch 124| loss: 0.97549 | train_balanced_accuracy: 0.62357 | valid_balanced_accuracy: 0.61297 |  2:44:42s\n",
      "epoch 125| loss: 0.97349 | train_balanced_accuracy: 0.6173  | valid_balanced_accuracy: 0.60684 |  2:46:01s\n",
      "epoch 126| loss: 0.97161 | train_balanced_accuracy: 0.63247 | valid_balanced_accuracy: 0.62752 |  2:47:19s\n",
      "epoch 127| loss: 0.96981 | train_balanced_accuracy: 0.62561 | valid_balanced_accuracy: 0.61742 |  2:48:38s\n",
      "epoch 128| loss: 0.96845 | train_balanced_accuracy: 0.62608 | valid_balanced_accuracy: 0.61427 |  2:49:56s\n",
      "epoch 129| loss: 0.96368 | train_balanced_accuracy: 0.63213 | valid_balanced_accuracy: 0.6213  |  2:51:15s\n",
      "epoch 130| loss: 0.96364 | train_balanced_accuracy: 0.62592 | valid_balanced_accuracy: 0.61493 |  2:52:34s\n",
      "epoch 131| loss: 0.9646  | train_balanced_accuracy: 0.63198 | valid_balanced_accuracy: 0.62192 |  2:53:53s\n",
      "epoch 132| loss: 0.96691 | train_balanced_accuracy: 0.62418 | valid_balanced_accuracy: 0.61352 |  2:55:11s\n",
      "epoch 133| loss: 0.96281 | train_balanced_accuracy: 0.62595 | valid_balanced_accuracy: 0.62134 |  2:56:30s\n",
      "epoch 134| loss: 0.96135 | train_balanced_accuracy: 0.63389 | valid_balanced_accuracy: 0.62426 |  2:57:49s\n",
      "epoch 135| loss: 0.96307 | train_balanced_accuracy: 0.62653 | valid_balanced_accuracy: 0.61672 |  2:59:08s\n",
      "epoch 136| loss: 0.96161 | train_balanced_accuracy: 0.63805 | valid_balanced_accuracy: 0.632   |  3:00:26s\n",
      "epoch 137| loss: 0.95853 | train_balanced_accuracy: 0.63981 | valid_balanced_accuracy: 0.62851 |  3:01:45s\n",
      "epoch 138| loss: 0.95628 | train_balanced_accuracy: 0.6351  | valid_balanced_accuracy: 0.62395 |  3:03:03s\n",
      "epoch 139| loss: 0.95614 | train_balanced_accuracy: 0.63831 | valid_balanced_accuracy: 0.62839 |  3:04:21s\n",
      "epoch 140| loss: 0.95558 | train_balanced_accuracy: 0.64172 | valid_balanced_accuracy: 0.62671 |  3:05:41s\n",
      "epoch 141| loss: 0.95275 | train_balanced_accuracy: 0.63277 | valid_balanced_accuracy: 0.62498 |  3:07:02s\n",
      "epoch 142| loss: 0.95056 | train_balanced_accuracy: 0.64041 | valid_balanced_accuracy: 0.63054 |  3:08:24s\n",
      "epoch 143| loss: 0.94922 | train_balanced_accuracy: 0.6453  | valid_balanced_accuracy: 0.63142 |  3:09:46s\n",
      "epoch 144| loss: 0.94689 | train_balanced_accuracy: 0.64797 | valid_balanced_accuracy: 0.63247 |  3:11:07s\n",
      "epoch 145| loss: 0.94689 | train_balanced_accuracy: 0.64331 | valid_balanced_accuracy: 0.62728 |  3:12:28s\n",
      "epoch 146| loss: 0.94823 | train_balanced_accuracy: 0.64408 | valid_balanced_accuracy: 0.63005 |  3:13:49s\n",
      "epoch 147| loss: 0.94387 | train_balanced_accuracy: 0.64333 | valid_balanced_accuracy: 0.62868 |  3:15:11s\n",
      "epoch 148| loss: 0.94299 | train_balanced_accuracy: 0.64629 | valid_balanced_accuracy: 0.63704 |  3:16:32s\n",
      "epoch 149| loss: 0.9429  | train_balanced_accuracy: 0.64647 | valid_balanced_accuracy: 0.63288 |  3:17:54s\n",
      "epoch 150| loss: 0.94235 | train_balanced_accuracy: 0.64797 | valid_balanced_accuracy: 0.6373  |  3:19:15s\n",
      "epoch 151| loss: 0.94185 | train_balanced_accuracy: 0.64782 | valid_balanced_accuracy: 0.63254 |  3:20:35s\n",
      "epoch 152| loss: 0.94365 | train_balanced_accuracy: 0.64204 | valid_balanced_accuracy: 0.62788 |  3:21:55s\n",
      "epoch 153| loss: 0.93984 | train_balanced_accuracy: 0.64992 | valid_balanced_accuracy: 0.63391 |  3:23:15s\n",
      "epoch 154| loss: 0.94178 | train_balanced_accuracy: 0.6433  | valid_balanced_accuracy: 0.62749 |  3:24:34s\n",
      "epoch 155| loss: 0.94017 | train_balanced_accuracy: 0.64771 | valid_balanced_accuracy: 0.63236 |  3:25:53s\n",
      "epoch 156| loss: 0.94111 | train_balanced_accuracy: 0.64369 | valid_balanced_accuracy: 0.63327 |  3:27:14s\n",
      "epoch 157| loss: 0.93892 | train_balanced_accuracy: 0.64695 | valid_balanced_accuracy: 0.62927 |  3:28:33s\n",
      "epoch 158| loss: 0.93544 | train_balanced_accuracy: 0.65106 | valid_balanced_accuracy: 0.63349 |  3:29:52s\n",
      "epoch 159| loss: 0.93601 | train_balanced_accuracy: 0.64911 | valid_balanced_accuracy: 0.64066 |  3:31:10s\n",
      "epoch 160| loss: 0.93376 | train_balanced_accuracy: 0.64557 | valid_balanced_accuracy: 0.63068 |  3:32:29s\n",
      "epoch 161| loss: 0.93305 | train_balanced_accuracy: 0.64288 | valid_balanced_accuracy: 0.63122 |  3:33:47s\n",
      "epoch 162| loss: 0.93519 | train_balanced_accuracy: 0.6559  | valid_balanced_accuracy: 0.63987 |  3:35:06s\n",
      "epoch 163| loss: 0.93161 | train_balanced_accuracy: 0.65017 | valid_balanced_accuracy: 0.63547 |  3:36:24s\n",
      "epoch 164| loss: 0.93025 | train_balanced_accuracy: 0.64949 | valid_balanced_accuracy: 0.63312 |  3:37:43s\n",
      "epoch 165| loss: 0.92929 | train_balanced_accuracy: 0.65152 | valid_balanced_accuracy: 0.63822 |  3:39:01s\n",
      "epoch 166| loss: 0.9296  | train_balanced_accuracy: 0.65154 | valid_balanced_accuracy: 0.63543 |  3:40:20s\n",
      "epoch 167| loss: 0.92943 | train_balanced_accuracy: 0.65473 | valid_balanced_accuracy: 0.64005 |  3:41:38s\n",
      "epoch 168| loss: 0.9271  | train_balanced_accuracy: 0.65345 | valid_balanced_accuracy: 0.63444 |  3:42:59s\n",
      "epoch 169| loss: 0.92626 | train_balanced_accuracy: 0.65255 | valid_balanced_accuracy: 0.63461 |  3:44:20s\n",
      "epoch 170| loss: 0.92496 | train_balanced_accuracy: 0.65563 | valid_balanced_accuracy: 0.64323 |  3:45:40s\n",
      "epoch 171| loss: 0.92488 | train_balanced_accuracy: 0.65177 | valid_balanced_accuracy: 0.63187 |  3:47:01s\n",
      "epoch 172| loss: 0.92383 | train_balanced_accuracy: 0.6502  | valid_balanced_accuracy: 0.63154 |  3:48:22s\n",
      "epoch 173| loss: 0.92164 | train_balanced_accuracy: 0.64011 | valid_balanced_accuracy: 0.61806 |  3:49:44s\n",
      "epoch 174| loss: 0.92315 | train_balanced_accuracy: 0.65816 | valid_balanced_accuracy: 0.64116 |  3:51:06s\n",
      "epoch 175| loss: 0.92282 | train_balanced_accuracy: 0.65383 | valid_balanced_accuracy: 0.63864 |  3:52:27s\n",
      "epoch 176| loss: 0.92133 | train_balanced_accuracy: 0.65867 | valid_balanced_accuracy: 0.63904 |  3:53:48s\n",
      "epoch 177| loss: 0.92283 | train_balanced_accuracy: 0.65725 | valid_balanced_accuracy: 0.63615 |  3:55:07s\n",
      "epoch 178| loss: 0.92402 | train_balanced_accuracy: 0.65185 | valid_balanced_accuracy: 0.63406 |  3:56:25s\n",
      "epoch 179| loss: 0.92302 | train_balanced_accuracy: 0.65225 | valid_balanced_accuracy: 0.63296 |  3:57:44s\n",
      "epoch 180| loss: 0.92169 | train_balanced_accuracy: 0.65886 | valid_balanced_accuracy: 0.63962 |  3:59:03s\n",
      "epoch 181| loss: 0.92388 | train_balanced_accuracy: 0.64674 | valid_balanced_accuracy: 0.62849 |  4:00:21s\n",
      "epoch 182| loss: 0.92789 | train_balanced_accuracy: 0.63348 | valid_balanced_accuracy: 0.61574 |  4:01:40s\n",
      "epoch 183| loss: 0.92474 | train_balanced_accuracy: 0.62342 | valid_balanced_accuracy: 0.60658 |  4:02:59s\n",
      "epoch 184| loss: 0.92351 | train_balanced_accuracy: 0.63858 | valid_balanced_accuracy: 0.62575 |  4:04:17s\n",
      "epoch 185| loss: 0.91912 | train_balanced_accuracy: 0.66093 | valid_balanced_accuracy: 0.64365 |  4:05:36s\n",
      "epoch 186| loss: 0.91672 | train_balanced_accuracy: 0.65424 | valid_balanced_accuracy: 0.6398  |  4:06:54s\n",
      "epoch 187| loss: 0.91318 | train_balanced_accuracy: 0.65907 | valid_balanced_accuracy: 0.64056 |  4:08:13s\n",
      "epoch 188| loss: 0.91084 | train_balanced_accuracy: 0.65723 | valid_balanced_accuracy: 0.64328 |  4:09:31s\n",
      "epoch 189| loss: 0.91141 | train_balanced_accuracy: 0.65726 | valid_balanced_accuracy: 0.6365  |  4:10:49s\n",
      "epoch 190| loss: 0.91889 | train_balanced_accuracy: 0.66182 | valid_balanced_accuracy: 0.64191 |  4:12:08s\n",
      "epoch 191| loss: 0.92905 | train_balanced_accuracy: 0.65166 | valid_balanced_accuracy: 0.6332  |  4:13:26s\n",
      "epoch 192| loss: 0.91867 | train_balanced_accuracy: 0.65906 | valid_balanced_accuracy: 0.64198 |  4:14:45s\n",
      "epoch 193| loss: 0.91401 | train_balanced_accuracy: 0.66137 | valid_balanced_accuracy: 0.64568 |  4:16:03s\n",
      "epoch 194| loss: 0.91683 | train_balanced_accuracy: 0.66039 | valid_balanced_accuracy: 0.64474 |  4:17:22s\n",
      "epoch 195| loss: 0.91544 | train_balanced_accuracy: 0.65747 | valid_balanced_accuracy: 0.6422  |  4:18:40s\n",
      "epoch 196| loss: 0.9137  | train_balanced_accuracy: 0.65917 | valid_balanced_accuracy: 0.64346 |  4:19:59s\n",
      "epoch 197| loss: 0.90919 | train_balanced_accuracy: 0.65983 | valid_balanced_accuracy: 0.64151 |  4:21:17s\n",
      "epoch 198| loss: 0.91059 | train_balanced_accuracy: 0.65883 | valid_balanced_accuracy: 0.64199 |  4:22:36s\n",
      "epoch 199| loss: 0.90939 | train_balanced_accuracy: 0.66104 | valid_balanced_accuracy: 0.64317 |  4:23:54s\n",
      "epoch 200| loss: 0.90819 | train_balanced_accuracy: 0.66097 | valid_balanced_accuracy: 0.64125 |  4:25:12s\n",
      "epoch 201| loss: 0.90421 | train_balanced_accuracy: 0.65263 | valid_balanced_accuracy: 0.63387 |  4:26:31s\n",
      "epoch 202| loss: 0.90584 | train_balanced_accuracy: 0.64519 | valid_balanced_accuracy: 0.62937 |  4:27:50s\n",
      "epoch 203| loss: 0.9074  | train_balanced_accuracy: 0.65143 | valid_balanced_accuracy: 0.63042 |  4:29:08s\n",
      "epoch 204| loss: 0.90763 | train_balanced_accuracy: 0.65885 | valid_balanced_accuracy: 0.6357  |  4:30:26s\n",
      "epoch 205| loss: 0.90544 | train_balanced_accuracy: 0.65736 | valid_balanced_accuracy: 0.63756 |  4:31:45s\n",
      "epoch 206| loss: 0.90854 | train_balanced_accuracy: 0.65239 | valid_balanced_accuracy: 0.62906 |  4:33:03s\n",
      "epoch 207| loss: 0.90738 | train_balanced_accuracy: 0.65662 | valid_balanced_accuracy: 0.63534 |  4:34:22s\n",
      "epoch 208| loss: 0.908   | train_balanced_accuracy: 0.66152 | valid_balanced_accuracy: 0.64192 |  4:35:40s\n",
      "epoch 209| loss: 0.90617 | train_balanced_accuracy: 0.65628 | valid_balanced_accuracy: 0.6337  |  4:36:59s\n",
      "epoch 210| loss: 0.90355 | train_balanced_accuracy: 0.66621 | valid_balanced_accuracy: 0.64871 |  4:38:17s\n",
      "epoch 211| loss: 0.90412 | train_balanced_accuracy: 0.66041 | valid_balanced_accuracy: 0.63757 |  4:39:36s\n",
      "epoch 212| loss: 0.90315 | train_balanced_accuracy: 0.65666 | valid_balanced_accuracy: 0.63304 |  4:40:54s\n",
      "epoch 213| loss: 0.90009 | train_balanced_accuracy: 0.66263 | valid_balanced_accuracy: 0.6352  |  4:42:13s\n",
      "epoch 214| loss: 0.89856 | train_balanced_accuracy: 0.66511 | valid_balanced_accuracy: 0.63932 |  4:43:31s\n",
      "epoch 215| loss: 0.89795 | train_balanced_accuracy: 0.66675 | valid_balanced_accuracy: 0.64137 |  4:44:50s\n",
      "epoch 216| loss: 0.89423 | train_balanced_accuracy: 0.66597 | valid_balanced_accuracy: 0.64323 |  4:46:08s\n",
      "epoch 217| loss: 0.895   | train_balanced_accuracy: 0.66587 | valid_balanced_accuracy: 0.64249 |  4:47:27s\n",
      "epoch 218| loss: 0.89727 | train_balanced_accuracy: 0.66776 | valid_balanced_accuracy: 0.64547 |  4:48:45s\n",
      "epoch 219| loss: 0.89534 | train_balanced_accuracy: 0.66906 | valid_balanced_accuracy: 0.63827 |  4:50:04s\n",
      "epoch 220| loss: 0.89444 | train_balanced_accuracy: 0.66107 | valid_balanced_accuracy: 0.63707 |  4:51:22s\n",
      "epoch 221| loss: 0.89399 | train_balanced_accuracy: 0.66432 | valid_balanced_accuracy: 0.64082 |  4:52:40s\n",
      "epoch 222| loss: 0.89453 | train_balanced_accuracy: 0.67125 | valid_balanced_accuracy: 0.64305 |  4:53:59s\n",
      "epoch 223| loss: 0.90499 | train_balanced_accuracy: 0.6683  | valid_balanced_accuracy: 0.64705 |  4:55:17s\n",
      "epoch 224| loss: 0.90617 | train_balanced_accuracy: 0.65984 | valid_balanced_accuracy: 0.63566 |  4:56:36s\n",
      "epoch 225| loss: 0.90629 | train_balanced_accuracy: 0.66999 | valid_balanced_accuracy: 0.64699 |  4:57:54s\n",
      "epoch 226| loss: 0.90287 | train_balanced_accuracy: 0.66171 | valid_balanced_accuracy: 0.63876 |  4:59:13s\n",
      "epoch 227| loss: 0.89857 | train_balanced_accuracy: 0.67019 | valid_balanced_accuracy: 0.63594 |  5:00:31s\n",
      "epoch 228| loss: 0.89478 | train_balanced_accuracy: 0.66995 | valid_balanced_accuracy: 0.64196 |  5:01:50s\n",
      "epoch 229| loss: 0.89193 | train_balanced_accuracy: 0.67054 | valid_balanced_accuracy: 0.64314 |  5:03:08s\n",
      "epoch 230| loss: 0.88986 | train_balanced_accuracy: 0.67158 | valid_balanced_accuracy: 0.64374 |  5:04:27s\n",
      "epoch 231| loss: 0.89323 | train_balanced_accuracy: 0.67565 | valid_balanced_accuracy: 0.64926 |  5:05:45s\n",
      "epoch 232| loss: 0.89051 | train_balanced_accuracy: 0.67281 | valid_balanced_accuracy: 0.64528 |  5:07:04s\n",
      "epoch 233| loss: 0.89486 | train_balanced_accuracy: 0.67438 | valid_balanced_accuracy: 0.64663 |  5:08:23s\n",
      "epoch 234| loss: 0.8902  | train_balanced_accuracy: 0.67356 | valid_balanced_accuracy: 0.65093 |  5:09:41s\n",
      "epoch 235| loss: 0.8881  | train_balanced_accuracy: 0.6692  | valid_balanced_accuracy: 0.64338 |  5:11:00s\n",
      "epoch 236| loss: 0.88672 | train_balanced_accuracy: 0.6767  | valid_balanced_accuracy: 0.65066 |  5:12:19s\n",
      "epoch 237| loss: 0.88837 | train_balanced_accuracy: 0.67165 | valid_balanced_accuracy: 0.64324 |  5:13:38s\n",
      "epoch 238| loss: 0.88924 | train_balanced_accuracy: 0.66806 | valid_balanced_accuracy: 0.64408 |  5:14:57s\n",
      "epoch 239| loss: 0.88994 | train_balanced_accuracy: 0.67043 | valid_balanced_accuracy: 0.6405  |  5:16:15s\n",
      "epoch 240| loss: 0.89046 | train_balanced_accuracy: 0.67621 | valid_balanced_accuracy: 0.64746 |  5:17:34s\n",
      "epoch 241| loss: 0.88973 | train_balanced_accuracy: 0.67348 | valid_balanced_accuracy: 0.64576 |  5:18:52s\n",
      "epoch 242| loss: 0.88907 | train_balanced_accuracy: 0.67595 | valid_balanced_accuracy: 0.64808 |  5:20:11s\n",
      "epoch 243| loss: 0.88722 | train_balanced_accuracy: 0.67689 | valid_balanced_accuracy: 0.65455 |  5:21:30s\n",
      "epoch 244| loss: 0.88394 | train_balanced_accuracy: 0.67489 | valid_balanced_accuracy: 0.65287 |  5:22:48s\n",
      "epoch 245| loss: 0.88528 | train_balanced_accuracy: 0.66837 | valid_balanced_accuracy: 0.64217 |  5:24:07s\n",
      "epoch 246| loss: 0.88942 | train_balanced_accuracy: 0.6791  | valid_balanced_accuracy: 0.65101 |  5:25:28s\n",
      "epoch 247| loss: 0.88281 | train_balanced_accuracy: 0.67898 | valid_balanced_accuracy: 0.6433  |  5:26:49s\n",
      "epoch 248| loss: 0.88334 | train_balanced_accuracy: 0.67631 | valid_balanced_accuracy: 0.65372 |  5:28:10s\n",
      "epoch 249| loss: 0.88152 | train_balanced_accuracy: 0.6771  | valid_balanced_accuracy: 0.64918 |  5:29:31s\n",
      "epoch 250| loss: 0.88265 | train_balanced_accuracy: 0.68039 | valid_balanced_accuracy: 0.64876 |  5:30:52s\n",
      "epoch 251| loss: 0.87985 | train_balanced_accuracy: 0.6729  | valid_balanced_accuracy: 0.64384 |  5:32:13s\n",
      "epoch 252| loss: 0.87893 | train_balanced_accuracy: 0.67663 | valid_balanced_accuracy: 0.64717 |  5:33:34s\n",
      "epoch 253| loss: 0.879   | train_balanced_accuracy: 0.68431 | valid_balanced_accuracy: 0.64923 |  5:34:55s\n",
      "epoch 254| loss: 0.8819  | train_balanced_accuracy: 0.67141 | valid_balanced_accuracy: 0.64646 |  5:36:16s\n",
      "epoch 255| loss: 0.87797 | train_balanced_accuracy: 0.6821  | valid_balanced_accuracy: 0.64965 |  5:37:37s\n",
      "epoch 256| loss: 0.87723 | train_balanced_accuracy: 0.68451 | valid_balanced_accuracy: 0.64861 |  5:38:59s\n",
      "epoch 257| loss: 0.87666 | train_balanced_accuracy: 0.68607 | valid_balanced_accuracy: 0.65321 |  5:40:20s\n",
      "epoch 258| loss: 0.87741 | train_balanced_accuracy: 0.67608 | valid_balanced_accuracy: 0.64801 |  5:41:41s\n",
      "epoch 259| loss: 0.87581 | train_balanced_accuracy: 0.68006 | valid_balanced_accuracy: 0.65148 |  5:43:02s\n",
      "epoch 260| loss: 0.87621 | train_balanced_accuracy: 0.67877 | valid_balanced_accuracy: 0.65026 |  5:44:24s\n",
      "epoch 261| loss: 0.8758  | train_balanced_accuracy: 0.67785 | valid_balanced_accuracy: 0.65138 |  5:45:45s\n",
      "epoch 262| loss: 0.87559 | train_balanced_accuracy: 0.67867 | valid_balanced_accuracy: 0.64697 |  5:47:06s\n",
      "epoch 263| loss: 0.8768  | train_balanced_accuracy: 0.68572 | valid_balanced_accuracy: 0.64929 |  5:48:28s\n",
      "epoch 264| loss: 0.8742  | train_balanced_accuracy: 0.67755 | valid_balanced_accuracy: 0.65212 |  5:49:49s\n",
      "epoch 265| loss: 0.87551 | train_balanced_accuracy: 0.68514 | valid_balanced_accuracy: 0.64966 |  5:51:11s\n",
      "epoch 266| loss: 0.87353 | train_balanced_accuracy: 0.67675 | valid_balanced_accuracy: 0.64959 |  5:52:32s\n",
      "epoch 267| loss: 0.87219 | train_balanced_accuracy: 0.68412 | valid_balanced_accuracy: 0.65317 |  5:53:53s\n",
      "epoch 268| loss: 0.872   | train_balanced_accuracy: 0.67189 | valid_balanced_accuracy: 0.64345 |  5:55:14s\n",
      "epoch 269| loss: 0.86909 | train_balanced_accuracy: 0.68271 | valid_balanced_accuracy: 0.65538 |  5:56:36s\n",
      "epoch 270| loss: 0.87034 | train_balanced_accuracy: 0.67794 | valid_balanced_accuracy: 0.64875 |  5:57:57s\n",
      "epoch 271| loss: 0.87004 | train_balanced_accuracy: 0.67385 | valid_balanced_accuracy: 0.64921 |  5:59:18s\n",
      "epoch 272| loss: 0.86851 | train_balanced_accuracy: 0.67545 | valid_balanced_accuracy: 0.64529 |  6:00:39s\n",
      "epoch 273| loss: 0.87021 | train_balanced_accuracy: 0.67738 | valid_balanced_accuracy: 0.64938 |  6:02:00s\n",
      "epoch 274| loss: 0.87353 | train_balanced_accuracy: 0.67841 | valid_balanced_accuracy: 0.64829 |  6:03:21s\n",
      "epoch 275| loss: 0.87739 | train_balanced_accuracy: 0.6862  | valid_balanced_accuracy: 0.65506 |  6:04:42s\n",
      "epoch 276| loss: 0.87086 | train_balanced_accuracy: 0.67813 | valid_balanced_accuracy: 0.64834 |  6:06:04s\n",
      "epoch 277| loss: 0.8673  | train_balanced_accuracy: 0.67662 | valid_balanced_accuracy: 0.64727 |  6:07:25s\n",
      "epoch 278| loss: 0.86536 | train_balanced_accuracy: 0.67778 | valid_balanced_accuracy: 0.65105 |  6:08:46s\n",
      "epoch 279| loss: 0.86482 | train_balanced_accuracy: 0.68377 | valid_balanced_accuracy: 0.64953 |  6:10:07s\n",
      "epoch 280| loss: 0.86026 | train_balanced_accuracy: 0.67591 | valid_balanced_accuracy: 0.65061 |  6:11:28s\n",
      "epoch 281| loss: 0.86142 | train_balanced_accuracy: 0.68625 | valid_balanced_accuracy: 0.65741 |  6:12:49s\n",
      "epoch 282| loss: 0.86187 | train_balanced_accuracy: 0.67393 | valid_balanced_accuracy: 0.64672 |  6:14:10s\n",
      "epoch 283| loss: 0.86503 | train_balanced_accuracy: 0.68421 | valid_balanced_accuracy: 0.65234 |  6:15:31s\n",
      "epoch 284| loss: 0.86258 | train_balanced_accuracy: 0.67901 | valid_balanced_accuracy: 0.64964 |  6:16:52s\n",
      "epoch 285| loss: 0.86247 | train_balanced_accuracy: 0.68343 | valid_balanced_accuracy: 0.6564  |  6:18:12s\n",
      "epoch 286| loss: 0.85997 | train_balanced_accuracy: 0.67337 | valid_balanced_accuracy: 0.6435  |  6:19:33s\n",
      "epoch 287| loss: 0.85793 | train_balanced_accuracy: 0.68123 | valid_balanced_accuracy: 0.64956 |  6:20:54s\n",
      "epoch 288| loss: 0.85676 | train_balanced_accuracy: 0.67654 | valid_balanced_accuracy: 0.64865 |  6:22:15s\n",
      "epoch 289| loss: 0.86186 | train_balanced_accuracy: 0.67389 | valid_balanced_accuracy: 0.6464  |  6:23:36s\n",
      "epoch 290| loss: 0.8578  | train_balanced_accuracy: 0.6813  | valid_balanced_accuracy: 0.65015 |  6:24:55s\n",
      "epoch 291| loss: 0.86124 | train_balanced_accuracy: 0.68432 | valid_balanced_accuracy: 0.65815 |  6:26:13s\n",
      "epoch 292| loss: 0.86221 | train_balanced_accuracy: 0.69425 | valid_balanced_accuracy: 0.65756 |  6:27:31s\n",
      "epoch 293| loss: 0.8603  | train_balanced_accuracy: 0.67552 | valid_balanced_accuracy: 0.64578 |  6:28:49s\n",
      "epoch 294| loss: 0.85728 | train_balanced_accuracy: 0.68686 | valid_balanced_accuracy: 0.65441 |  6:30:08s\n",
      "epoch 295| loss: 0.85701 | train_balanced_accuracy: 0.68663 | valid_balanced_accuracy: 0.64604 |  6:31:27s\n",
      "epoch 296| loss: 0.86011 | train_balanced_accuracy: 0.67957 | valid_balanced_accuracy: 0.65186 |  6:32:45s\n",
      "epoch 297| loss: 0.85826 | train_balanced_accuracy: 0.68054 | valid_balanced_accuracy: 0.65277 |  6:34:03s\n",
      "epoch 298| loss: 0.85339 | train_balanced_accuracy: 0.68398 | valid_balanced_accuracy: 0.64634 |  6:35:22s\n",
      "epoch 299| loss: 0.85659 | train_balanced_accuracy: 0.67823 | valid_balanced_accuracy: 0.64832 |  6:36:42s\n",
      "epoch 300| loss: 0.85323 | train_balanced_accuracy: 0.68806 | valid_balanced_accuracy: 0.65207 |  6:38:04s\n",
      "epoch 301| loss: 0.85448 | train_balanced_accuracy: 0.67472 | valid_balanced_accuracy: 0.64736 |  6:39:25s\n",
      "epoch 302| loss: 0.85051 | train_balanced_accuracy: 0.68426 | valid_balanced_accuracy: 0.6541  |  6:40:47s\n",
      "epoch 303| loss: 0.85118 | train_balanced_accuracy: 0.6795  | valid_balanced_accuracy: 0.64865 |  6:42:08s\n",
      "epoch 304| loss: 0.8528  | train_balanced_accuracy: 0.68574 | valid_balanced_accuracy: 0.65066 |  6:43:30s\n",
      "epoch 305| loss: 0.85731 | train_balanced_accuracy: 0.6813  | valid_balanced_accuracy: 0.65315 |  6:44:52s\n",
      "epoch 306| loss: 0.85522 | train_balanced_accuracy: 0.68783 | valid_balanced_accuracy: 0.65232 |  6:46:13s\n",
      "epoch 307| loss: 0.85762 | train_balanced_accuracy: 0.68325 | valid_balanced_accuracy: 0.65211 |  6:47:35s\n",
      "epoch 308| loss: 0.85361 | train_balanced_accuracy: 0.68714 | valid_balanced_accuracy: 0.65728 |  6:48:57s\n",
      "epoch 309| loss: 0.851   | train_balanced_accuracy: 0.68558 | valid_balanced_accuracy: 0.65358 |  6:50:19s\n",
      "epoch 310| loss: 0.85298 | train_balanced_accuracy: 0.68517 | valid_balanced_accuracy: 0.65837 |  6:51:40s\n",
      "epoch 311| loss: 0.85529 | train_balanced_accuracy: 0.69694 | valid_balanced_accuracy: 0.66269 |  6:53:02s\n",
      "epoch 312| loss: 0.85635 | train_balanced_accuracy: 0.6852  | valid_balanced_accuracy: 0.65819 |  6:54:24s\n",
      "epoch 313| loss: 0.8552  | train_balanced_accuracy: 0.68636 | valid_balanced_accuracy: 0.65779 |  6:55:46s\n",
      "epoch 314| loss: 0.84804 | train_balanced_accuracy: 0.69516 | valid_balanced_accuracy: 0.6573  |  6:57:07s\n",
      "epoch 315| loss: 0.85307 | train_balanced_accuracy: 0.68904 | valid_balanced_accuracy: 0.65899 |  6:58:28s\n",
      "epoch 316| loss: 0.84624 | train_balanced_accuracy: 0.69745 | valid_balanced_accuracy: 0.65957 |  6:59:49s\n",
      "epoch 317| loss: 0.84934 | train_balanced_accuracy: 0.69648 | valid_balanced_accuracy: 0.65648 |  7:01:08s\n",
      "epoch 318| loss: 0.84942 | train_balanced_accuracy: 0.6867  | valid_balanced_accuracy: 0.65163 |  7:02:28s\n",
      "epoch 319| loss: 0.84832 | train_balanced_accuracy: 0.69052 | valid_balanced_accuracy: 0.65053 |  7:03:48s\n",
      "epoch 320| loss: 0.84937 | train_balanced_accuracy: 0.68524 | valid_balanced_accuracy: 0.65547 |  7:05:08s\n",
      "epoch 321| loss: 0.84569 | train_balanced_accuracy: 0.6873  | valid_balanced_accuracy: 0.65282 |  7:06:27s\n",
      "epoch 322| loss: 0.84704 | train_balanced_accuracy: 0.68545 | valid_balanced_accuracy: 0.6523  |  7:07:47s\n",
      "epoch 323| loss: 0.84564 | train_balanced_accuracy: 0.69601 | valid_balanced_accuracy: 0.65681 |  7:09:06s\n",
      "epoch 324| loss: 0.84109 | train_balanced_accuracy: 0.68419 | valid_balanced_accuracy: 0.65234 |  7:10:26s\n",
      "epoch 325| loss: 0.84034 | train_balanced_accuracy: 0.69542 | valid_balanced_accuracy: 0.65783 |  7:11:46s\n",
      "epoch 326| loss: 0.84372 | train_balanced_accuracy: 0.69481 | valid_balanced_accuracy: 0.65563 |  7:13:05s\n",
      "epoch 327| loss: 0.84028 | train_balanced_accuracy: 0.69677 | valid_balanced_accuracy: 0.65869 |  7:14:25s\n",
      "epoch 328| loss: 0.84438 | train_balanced_accuracy: 0.68423 | valid_balanced_accuracy: 0.65143 |  7:15:46s\n",
      "epoch 329| loss: 0.84446 | train_balanced_accuracy: 0.69561 | valid_balanced_accuracy: 0.65323 |  7:17:05s\n",
      "epoch 330| loss: 0.84444 | train_balanced_accuracy: 0.69661 | valid_balanced_accuracy: 0.66126 |  7:18:23s\n",
      "epoch 331| loss: 0.84473 | train_balanced_accuracy: 0.68405 | valid_balanced_accuracy: 0.65178 |  7:19:41s\n",
      "epoch 332| loss: 0.84958 | train_balanced_accuracy: 0.69764 | valid_balanced_accuracy: 0.65851 |  7:20:59s\n",
      "epoch 333| loss: 0.84981 | train_balanced_accuracy: 0.69465 | valid_balanced_accuracy: 0.65648 |  7:22:18s\n",
      "epoch 334| loss: 0.85358 | train_balanced_accuracy: 0.69648 | valid_balanced_accuracy: 0.66074 |  7:23:36s\n",
      "epoch 335| loss: 0.84844 | train_balanced_accuracy: 0.69393 | valid_balanced_accuracy: 0.65705 |  7:24:54s\n",
      "epoch 336| loss: 0.84967 | train_balanced_accuracy: 0.68947 | valid_balanced_accuracy: 0.65487 |  7:26:12s\n",
      "epoch 337| loss: 0.84646 | train_balanced_accuracy: 0.69996 | valid_balanced_accuracy: 0.65946 |  7:27:30s\n",
      "epoch 338| loss: 0.84312 | train_balanced_accuracy: 0.69483 | valid_balanced_accuracy: 0.65366 |  7:28:48s\n",
      "epoch 339| loss: 0.84531 | train_balanced_accuracy: 0.69179 | valid_balanced_accuracy: 0.65812 |  7:30:06s\n",
      "epoch 340| loss: 0.84275 | train_balanced_accuracy: 0.69532 | valid_balanced_accuracy: 0.65681 |  7:31:24s\n",
      "epoch 341| loss: 0.84032 | train_balanced_accuracy: 0.70338 | valid_balanced_accuracy: 0.66152 |  7:32:43s\n",
      "epoch 342| loss: 0.83956 | train_balanced_accuracy: 0.69524 | valid_balanced_accuracy: 0.654   |  7:34:01s\n",
      "epoch 343| loss: 0.83872 | train_balanced_accuracy: 0.69987 | valid_balanced_accuracy: 0.66159 |  7:35:20s\n",
      "epoch 344| loss: 0.83821 | train_balanced_accuracy: 0.68453 | valid_balanced_accuracy: 0.65616 |  7:36:38s\n",
      "epoch 345| loss: 0.8396  | train_balanced_accuracy: 0.70352 | valid_balanced_accuracy: 0.6613  |  7:37:56s\n",
      "epoch 346| loss: 0.83873 | train_balanced_accuracy: 0.69373 | valid_balanced_accuracy: 0.65376 |  7:39:16s\n",
      "epoch 347| loss: 0.84061 | train_balanced_accuracy: 0.69095 | valid_balanced_accuracy: 0.65928 |  7:40:35s\n",
      "epoch 348| loss: 0.83986 | train_balanced_accuracy: 0.70239 | valid_balanced_accuracy: 0.66058 |  7:41:53s\n",
      "epoch 349| loss: 0.83738 | train_balanced_accuracy: 0.69506 | valid_balanced_accuracy: 0.65775 |  7:43:11s\n",
      "epoch 350| loss: 0.84034 | train_balanced_accuracy: 0.693   | valid_balanced_accuracy: 0.65264 |  7:44:29s\n",
      "epoch 351| loss: 0.83626 | train_balanced_accuracy: 0.6957  | valid_balanced_accuracy: 0.65648 |  7:45:47s\n",
      "epoch 352| loss: 0.8323  | train_balanced_accuracy: 0.70071 | valid_balanced_accuracy: 0.65985 |  7:47:04s\n",
      "epoch 353| loss: 0.83164 | train_balanced_accuracy: 0.70253 | valid_balanced_accuracy: 0.66361 |  7:48:22s\n",
      "epoch 354| loss: 0.82819 | train_balanced_accuracy: 0.69643 | valid_balanced_accuracy: 0.65414 |  7:49:40s\n",
      "epoch 355| loss: 0.83397 | train_balanced_accuracy: 0.68796 | valid_balanced_accuracy: 0.65104 |  7:50:58s\n",
      "epoch 356| loss: 0.83272 | train_balanced_accuracy: 0.70133 | valid_balanced_accuracy: 0.65953 |  7:52:16s\n",
      "epoch 357| loss: 0.83143 | train_balanced_accuracy: 0.6943  | valid_balanced_accuracy: 0.65246 |  7:53:34s\n",
      "epoch 358| loss: 0.83224 | train_balanced_accuracy: 0.69492 | valid_balanced_accuracy: 0.6594  |  7:54:52s\n",
      "epoch 359| loss: 0.83071 | train_balanced_accuracy: 0.70436 | valid_balanced_accuracy: 0.65597 |  7:56:10s\n",
      "epoch 360| loss: 0.82919 | train_balanced_accuracy: 0.70501 | valid_balanced_accuracy: 0.66261 |  7:57:28s\n",
      "epoch 361| loss: 0.82912 | train_balanced_accuracy: 0.7022  | valid_balanced_accuracy: 0.65851 |  7:58:45s\n",
      "epoch 362| loss: 0.83141 | train_balanced_accuracy: 0.69897 | valid_balanced_accuracy: 0.65207 |  8:00:03s\n",
      "epoch 363| loss: 0.82818 | train_balanced_accuracy: 0.70897 | valid_balanced_accuracy: 0.66245 |  8:01:21s\n",
      "epoch 364| loss: 0.82901 | train_balanced_accuracy: 0.70363 | valid_balanced_accuracy: 0.65381 |  8:02:39s\n",
      "epoch 365| loss: 0.8274  | train_balanced_accuracy: 0.69874 | valid_balanced_accuracy: 0.65586 |  8:03:57s\n",
      "epoch 366| loss: 0.82552 | train_balanced_accuracy: 0.69755 | valid_balanced_accuracy: 0.65599 |  8:05:15s\n",
      "epoch 367| loss: 0.82766 | train_balanced_accuracy: 0.70282 | valid_balanced_accuracy: 0.6614  |  8:06:33s\n",
      "epoch 368| loss: 0.82748 | train_balanced_accuracy: 0.71123 | valid_balanced_accuracy: 0.66329 |  8:07:51s\n",
      "epoch 369| loss: 0.82535 | train_balanced_accuracy: 0.70234 | valid_balanced_accuracy: 0.66088 |  8:09:09s\n",
      "epoch 370| loss: 0.82508 | train_balanced_accuracy: 0.69704 | valid_balanced_accuracy: 0.65839 |  8:10:26s\n",
      "epoch 371| loss: 0.82431 | train_balanced_accuracy: 0.70205 | valid_balanced_accuracy: 0.65451 |  8:11:44s\n",
      "epoch 372| loss: 0.82742 | train_balanced_accuracy: 0.70706 | valid_balanced_accuracy: 0.6617  |  8:13:03s\n",
      "epoch 373| loss: 0.82378 | train_balanced_accuracy: 0.7031  | valid_balanced_accuracy: 0.65943 |  8:14:21s\n",
      "epoch 374| loss: 0.82463 | train_balanced_accuracy: 0.70903 | valid_balanced_accuracy: 0.66031 |  8:15:39s\n",
      "epoch 375| loss: 0.82386 | train_balanced_accuracy: 0.70188 | valid_balanced_accuracy: 0.66295 |  8:16:57s\n",
      "epoch 376| loss: 0.82175 | train_balanced_accuracy: 0.69864 | valid_balanced_accuracy: 0.652   |  8:18:15s\n",
      "epoch 377| loss: 0.8217  | train_balanced_accuracy: 0.69814 | valid_balanced_accuracy: 0.65479 |  8:19:33s\n",
      "epoch 378| loss: 0.82218 | train_balanced_accuracy: 0.69556 | valid_balanced_accuracy: 0.65705 |  8:20:51s\n",
      "epoch 379| loss: 0.82562 | train_balanced_accuracy: 0.69756 | valid_balanced_accuracy: 0.65622 |  8:22:09s\n",
      "epoch 380| loss: 0.81718 | train_balanced_accuracy: 0.7098  | valid_balanced_accuracy: 0.66106 |  8:23:27s\n",
      "epoch 381| loss: 0.82253 | train_balanced_accuracy: 0.70688 | valid_balanced_accuracy: 0.66079 |  8:24:45s\n",
      "epoch 382| loss: 0.82299 | train_balanced_accuracy: 0.70877 | valid_balanced_accuracy: 0.66399 |  8:26:03s\n",
      "epoch 383| loss: 0.81926 | train_balanced_accuracy: 0.70093 | valid_balanced_accuracy: 0.66319 |  8:27:21s\n",
      "epoch 384| loss: 0.82031 | train_balanced_accuracy: 0.69435 | valid_balanced_accuracy: 0.6527  |  8:28:39s\n",
      "epoch 385| loss: 0.82425 | train_balanced_accuracy: 0.69492 | valid_balanced_accuracy: 0.6606  |  8:29:57s\n",
      "epoch 386| loss: 0.8284  | train_balanced_accuracy: 0.703   | valid_balanced_accuracy: 0.6592  |  8:31:16s\n",
      "epoch 387| loss: 0.83245 | train_balanced_accuracy: 0.69319 | valid_balanced_accuracy: 0.64954 |  8:32:35s\n",
      "epoch 388| loss: 0.83539 | train_balanced_accuracy: 0.68143 | valid_balanced_accuracy: 0.64865 |  8:33:55s\n",
      "epoch 389| loss: 0.85311 | train_balanced_accuracy: 0.68855 | valid_balanced_accuracy: 0.64386 |  8:35:15s\n",
      "epoch 390| loss: 0.85093 | train_balanced_accuracy: 0.69581 | valid_balanced_accuracy: 0.65097 |  8:36:35s\n",
      "epoch 391| loss: 0.84994 | train_balanced_accuracy: 0.69501 | valid_balanced_accuracy: 0.65215 |  8:37:56s\n",
      "epoch 392| loss: 0.84952 | train_balanced_accuracy: 0.68936 | valid_balanced_accuracy: 0.65372 |  8:39:16s\n",
      "epoch 393| loss: 0.84888 | train_balanced_accuracy: 0.69282 | valid_balanced_accuracy: 0.64961 |  8:40:34s\n",
      "epoch 394| loss: 0.85308 | train_balanced_accuracy: 0.69117 | valid_balanced_accuracy: 0.65592 |  8:41:53s\n",
      "epoch 395| loss: 0.84754 | train_balanced_accuracy: 0.69536 | valid_balanced_accuracy: 0.65641 |  8:43:12s\n",
      "epoch 396| loss: 0.84203 | train_balanced_accuracy: 0.68966 | valid_balanced_accuracy: 0.65621 |  8:44:31s\n",
      "epoch 397| loss: 0.83992 | train_balanced_accuracy: 0.69089 | valid_balanced_accuracy: 0.65601 |  8:45:49s\n",
      "epoch 398| loss: 0.83698 | train_balanced_accuracy: 0.69831 | valid_balanced_accuracy: 0.66234 |  8:47:08s\n",
      "epoch 399| loss: 0.832   | train_balanced_accuracy: 0.69921 | valid_balanced_accuracy: 0.66285 |  8:48:27s\n",
      "epoch 400| loss: 0.83593 | train_balanced_accuracy: 0.70211 | valid_balanced_accuracy: 0.66287 |  8:49:45s\n",
      "epoch 401| loss: 0.83402 | train_balanced_accuracy: 0.69986 | valid_balanced_accuracy: 0.66492 |  8:51:04s\n",
      "epoch 402| loss: 0.83078 | train_balanced_accuracy: 0.69166 | valid_balanced_accuracy: 0.66151 |  8:52:22s\n",
      "epoch 403| loss: 0.83017 | train_balanced_accuracy: 0.69489 | valid_balanced_accuracy: 0.65838 |  8:53:40s\n",
      "epoch 404| loss: 0.83286 | train_balanced_accuracy: 0.69883 | valid_balanced_accuracy: 0.65955 |  8:54:59s\n",
      "epoch 405| loss: 0.82876 | train_balanced_accuracy: 0.69073 | valid_balanced_accuracy: 0.65801 |  8:56:18s\n",
      "epoch 406| loss: 0.83118 | train_balanced_accuracy: 0.69496 | valid_balanced_accuracy: 0.66296 |  8:57:37s\n",
      "epoch 407| loss: 0.82899 | train_balanced_accuracy: 0.69269 | valid_balanced_accuracy: 0.66086 |  8:58:56s\n",
      "epoch 408| loss: 0.82711 | train_balanced_accuracy: 0.69407 | valid_balanced_accuracy: 0.66533 |  9:00:14s\n",
      "epoch 409| loss: 0.83126 | train_balanced_accuracy: 0.69444 | valid_balanced_accuracy: 0.66308 |  9:01:32s\n",
      "epoch 410| loss: 0.82924 | train_balanced_accuracy: 0.70044 | valid_balanced_accuracy: 0.66908 |  9:02:51s\n",
      "epoch 411| loss: 0.8292  | train_balanced_accuracy: 0.69331 | valid_balanced_accuracy: 0.6594  |  9:04:10s\n",
      "epoch 412| loss: 0.83053 | train_balanced_accuracy: 0.70306 | valid_balanced_accuracy: 0.66465 |  9:05:29s\n",
      "epoch 413| loss: 0.82949 | train_balanced_accuracy: 0.69628 | valid_balanced_accuracy: 0.66097 |  9:06:48s\n",
      "epoch 414| loss: 0.8271  | train_balanced_accuracy: 0.70478 | valid_balanced_accuracy: 0.66404 |  9:08:06s\n",
      "epoch 415| loss: 0.82837 | train_balanced_accuracy: 0.69613 | valid_balanced_accuracy: 0.6595  |  9:09:25s\n",
      "epoch 416| loss: 0.83261 | train_balanced_accuracy: 0.70623 | valid_balanced_accuracy: 0.66381 |  9:10:44s\n",
      "epoch 417| loss: 0.85024 | train_balanced_accuracy: 0.69141 | valid_balanced_accuracy: 0.65987 |  9:12:03s\n",
      "epoch 418| loss: 0.83162 | train_balanced_accuracy: 0.69838 | valid_balanced_accuracy: 0.66194 |  9:13:23s\n",
      "epoch 419| loss: 0.82813 | train_balanced_accuracy: 0.70039 | valid_balanced_accuracy: 0.66227 |  9:14:42s\n",
      "epoch 420| loss: 0.82794 | train_balanced_accuracy: 0.69379 | valid_balanced_accuracy: 0.66088 |  9:16:00s\n",
      "epoch 421| loss: 0.82743 | train_balanced_accuracy: 0.69426 | valid_balanced_accuracy: 0.66043 |  9:17:19s\n",
      "epoch 422| loss: 0.82509 | train_balanced_accuracy: 0.69799 | valid_balanced_accuracy: 0.6612  |  9:18:37s\n",
      "epoch 423| loss: 0.82646 | train_balanced_accuracy: 0.68936 | valid_balanced_accuracy: 0.65769 |  9:19:56s\n",
      "epoch 424| loss: 0.82442 | train_balanced_accuracy: 0.6977  | valid_balanced_accuracy: 0.66485 |  9:21:15s\n",
      "epoch 425| loss: 0.82313 | train_balanced_accuracy: 0.69704 | valid_balanced_accuracy: 0.66083 |  9:22:34s\n",
      "epoch 426| loss: 0.8235  | train_balanced_accuracy: 0.70216 | valid_balanced_accuracy: 0.66144 |  9:23:52s\n",
      "epoch 427| loss: 0.82486 | train_balanced_accuracy: 0.69862 | valid_balanced_accuracy: 0.65987 |  9:25:11s\n",
      "epoch 428| loss: 0.82563 | train_balanced_accuracy: 0.69883 | valid_balanced_accuracy: 0.66077 |  9:26:30s\n",
      "epoch 429| loss: 0.82062 | train_balanced_accuracy: 0.68798 | valid_balanced_accuracy: 0.657   |  9:27:48s\n",
      "epoch 430| loss: 0.82011 | train_balanced_accuracy: 0.6984  | valid_balanced_accuracy: 0.6611  |  9:29:08s\n",
      "epoch 431| loss: 0.82001 | train_balanced_accuracy: 0.69486 | valid_balanced_accuracy: 0.66395 |  9:30:29s\n",
      "epoch 432| loss: 0.82329 | train_balanced_accuracy: 0.69102 | valid_balanced_accuracy: 0.65549 |  9:31:49s\n",
      "epoch 433| loss: 0.82373 | train_balanced_accuracy: 0.69306 | valid_balanced_accuracy: 0.65913 |  9:33:10s\n",
      "epoch 434| loss: 0.81991 | train_balanced_accuracy: 0.70776 | valid_balanced_accuracy: 0.6627  |  9:34:30s\n",
      "epoch 435| loss: 0.81666 | train_balanced_accuracy: 0.70883 | valid_balanced_accuracy: 0.66733 |  9:35:50s\n",
      "epoch 436| loss: 0.81823 | train_balanced_accuracy: 0.69672 | valid_balanced_accuracy: 0.65641 |  9:37:10s\n",
      "epoch 437| loss: 0.81236 | train_balanced_accuracy: 0.71043 | valid_balanced_accuracy: 0.66843 |  9:38:30s\n",
      "epoch 438| loss: 0.81091 | train_balanced_accuracy: 0.71366 | valid_balanced_accuracy: 0.66283 |  9:39:51s\n",
      "epoch 439| loss: 0.80965 | train_balanced_accuracy: 0.70918 | valid_balanced_accuracy: 0.66333 |  9:41:11s\n",
      "epoch 440| loss: 0.81031 | train_balanced_accuracy: 0.70167 | valid_balanced_accuracy: 0.65902 |  9:42:31s\n",
      "epoch 441| loss: 0.80576 | train_balanced_accuracy: 0.69496 | valid_balanced_accuracy: 0.66119 |  9:43:51s\n",
      "epoch 442| loss: 0.80769 | train_balanced_accuracy: 0.71038 | valid_balanced_accuracy: 0.66722 |  9:45:12s\n",
      "epoch 443| loss: 0.80904 | train_balanced_accuracy: 0.70427 | valid_balanced_accuracy: 0.66581 |  9:46:32s\n",
      "epoch 444| loss: 0.80877 | train_balanced_accuracy: 0.71793 | valid_balanced_accuracy: 0.66689 |  9:47:52s\n",
      "epoch 445| loss: 0.80689 | train_balanced_accuracy: 0.69972 | valid_balanced_accuracy: 0.66478 |  9:49:11s\n",
      "epoch 446| loss: 0.80472 | train_balanced_accuracy: 0.7046  | valid_balanced_accuracy: 0.66531 |  9:50:29s\n",
      "epoch 447| loss: 0.80525 | train_balanced_accuracy: 0.70109 | valid_balanced_accuracy: 0.66423 |  9:51:48s\n",
      "epoch 448| loss: 0.80846 | train_balanced_accuracy: 0.70961 | valid_balanced_accuracy: 0.6722  |  9:53:07s\n",
      "epoch 449| loss: 0.80583 | train_balanced_accuracy: 0.7053  | valid_balanced_accuracy: 0.66592 |  9:54:26s\n",
      "epoch 450| loss: 0.80556 | train_balanced_accuracy: 0.70785 | valid_balanced_accuracy: 0.66885 |  9:55:45s\n",
      "epoch 451| loss: 0.80566 | train_balanced_accuracy: 0.71191 | valid_balanced_accuracy: 0.66707 |  9:57:03s\n",
      "epoch 452| loss: 0.80361 | train_balanced_accuracy: 0.70422 | valid_balanced_accuracy: 0.65679 |  9:58:21s\n",
      "epoch 453| loss: 0.80504 | train_balanced_accuracy: 0.70953 | valid_balanced_accuracy: 0.66954 |  9:59:39s\n",
      "epoch 454| loss: 0.80552 | train_balanced_accuracy: 0.71039 | valid_balanced_accuracy: 0.6665  |  10:00:58s\n",
      "epoch 455| loss: 0.80225 | train_balanced_accuracy: 0.71453 | valid_balanced_accuracy: 0.66868 |  10:02:17s\n",
      "epoch 456| loss: 0.8031  | train_balanced_accuracy: 0.72662 | valid_balanced_accuracy: 0.67073 |  10:03:36s\n",
      "epoch 457| loss: 0.80199 | train_balanced_accuracy: 0.71635 | valid_balanced_accuracy: 0.67114 |  10:04:55s\n",
      "epoch 458| loss: 0.80315 | train_balanced_accuracy: 0.70175 | valid_balanced_accuracy: 0.65798 |  10:06:16s\n",
      "epoch 459| loss: 0.80171 | train_balanced_accuracy: 0.71569 | valid_balanced_accuracy: 0.6639  |  10:07:37s\n",
      "epoch 460| loss: 0.80577 | train_balanced_accuracy: 0.71023 | valid_balanced_accuracy: 0.66353 |  10:08:57s\n",
      "epoch 461| loss: 0.79987 | train_balanced_accuracy: 0.71734 | valid_balanced_accuracy: 0.66841 |  10:10:18s\n",
      "epoch 462| loss: 0.80457 | train_balanced_accuracy: 0.70981 | valid_balanced_accuracy: 0.66604 |  10:11:38s\n",
      "epoch 463| loss: 0.80638 | train_balanced_accuracy: 0.71181 | valid_balanced_accuracy: 0.66175 |  10:12:59s\n",
      "epoch 464| loss: 0.80147 | train_balanced_accuracy: 0.71902 | valid_balanced_accuracy: 0.67047 |  10:14:20s\n",
      "epoch 465| loss: 0.8059  | train_balanced_accuracy: 0.70626 | valid_balanced_accuracy: 0.65856 |  10:15:41s\n",
      "epoch 466| loss: 0.80141 | train_balanced_accuracy: 0.71393 | valid_balanced_accuracy: 0.66692 |  10:17:02s\n",
      "epoch 467| loss: 0.80044 | train_balanced_accuracy: 0.71792 | valid_balanced_accuracy: 0.66422 |  10:18:22s\n",
      "epoch 468| loss: 0.79743 | train_balanced_accuracy: 0.70963 | valid_balanced_accuracy: 0.66422 |  10:19:43s\n",
      "epoch 469| loss: 0.80375 | train_balanced_accuracy: 0.71602 | valid_balanced_accuracy: 0.66944 |  10:21:04s\n",
      "epoch 470| loss: 0.79895 | train_balanced_accuracy: 0.71021 | valid_balanced_accuracy: 0.6615  |  10:22:24s\n",
      "epoch 471| loss: 0.79511 | train_balanced_accuracy: 0.70756 | valid_balanced_accuracy: 0.66527 |  10:23:44s\n",
      "epoch 472| loss: 0.7991  | train_balanced_accuracy: 0.71707 | valid_balanced_accuracy: 0.67073 |  10:25:04s\n",
      "epoch 473| loss: 0.79871 | train_balanced_accuracy: 0.71762 | valid_balanced_accuracy: 0.67208 |  10:26:24s\n",
      "epoch 474| loss: 0.79945 | train_balanced_accuracy: 0.70875 | valid_balanced_accuracy: 0.66248 |  10:27:42s\n",
      "epoch 475| loss: 0.79993 | train_balanced_accuracy: 0.71264 | valid_balanced_accuracy: 0.66694 |  10:29:01s\n",
      "epoch 476| loss: 0.80257 | train_balanced_accuracy: 0.71668 | valid_balanced_accuracy: 0.6723  |  10:30:20s\n",
      "epoch 477| loss: 0.81605 | train_balanced_accuracy: 0.69505 | valid_balanced_accuracy: 0.64464 |  10:31:38s\n",
      "epoch 478| loss: 0.83618 | train_balanced_accuracy: 0.69179 | valid_balanced_accuracy: 0.64382 |  10:32:57s\n",
      "epoch 479| loss: 0.8199  | train_balanced_accuracy: 0.69893 | valid_balanced_accuracy: 0.66196 |  10:34:16s\n",
      "epoch 480| loss: 0.80169 | train_balanced_accuracy: 0.70167 | valid_balanced_accuracy: 0.65894 |  10:35:35s\n",
      "epoch 481| loss: 0.79862 | train_balanced_accuracy: 0.72113 | valid_balanced_accuracy: 0.66742 |  10:36:54s\n",
      "epoch 482| loss: 0.8146  | train_balanced_accuracy: 0.70373 | valid_balanced_accuracy: 0.66356 |  10:38:13s\n",
      "epoch 483| loss: 0.81917 | train_balanced_accuracy: 0.69623 | valid_balanced_accuracy: 0.64643 |  10:39:31s\n",
      "epoch 484| loss: 0.82521 | train_balanced_accuracy: 0.69577 | valid_balanced_accuracy: 0.64552 |  10:40:50s\n",
      "epoch 485| loss: 0.81901 | train_balanced_accuracy: 0.69629 | valid_balanced_accuracy: 0.64248 |  10:42:09s\n",
      "epoch 486| loss: 0.82014 | train_balanced_accuracy: 0.69182 | valid_balanced_accuracy: 0.64007 |  10:43:27s\n",
      "epoch 487| loss: 0.82438 | train_balanced_accuracy: 0.70222 | valid_balanced_accuracy: 0.65664 |  10:44:46s\n",
      "epoch 488| loss: 0.82638 | train_balanced_accuracy: 0.70076 | valid_balanced_accuracy: 0.656   |  10:46:05s\n",
      "epoch 489| loss: 0.83039 | train_balanced_accuracy: 0.70123 | valid_balanced_accuracy: 0.65366 |  10:47:23s\n",
      "epoch 490| loss: 0.83487 | train_balanced_accuracy: 0.70508 | valid_balanced_accuracy: 0.65936 |  10:48:42s\n",
      "epoch 491| loss: 0.82711 | train_balanced_accuracy: 0.7115  | valid_balanced_accuracy: 0.66101 |  10:50:01s\n",
      "epoch 492| loss: 0.82831 | train_balanced_accuracy: 0.71475 | valid_balanced_accuracy: 0.66424 |  10:51:19s\n",
      "epoch 493| loss: 0.82874 | train_balanced_accuracy: 0.70676 | valid_balanced_accuracy: 0.66202 |  10:52:38s\n",
      "epoch 494| loss: 0.83076 | train_balanced_accuracy: 0.71317 | valid_balanced_accuracy: 0.66096 |  10:53:57s\n",
      "epoch 495| loss: 0.83022 | train_balanced_accuracy: 0.71385 | valid_balanced_accuracy: 0.6589  |  10:55:16s\n",
      "epoch 496| loss: 0.82493 | train_balanced_accuracy: 0.70718 | valid_balanced_accuracy: 0.65685 |  10:56:35s\n",
      "epoch 497| loss: 0.82479 | train_balanced_accuracy: 0.71181 | valid_balanced_accuracy: 0.66474 |  10:57:54s\n",
      "epoch 498| loss: 0.82118 | train_balanced_accuracy: 0.71632 | valid_balanced_accuracy: 0.6646  |  10:59:12s\n",
      "epoch 499| loss: 0.82591 | train_balanced_accuracy: 0.71478 | valid_balanced_accuracy: 0.66146 |  11:00:31s\n",
      "epoch 500| loss: 0.82273 | train_balanced_accuracy: 0.71008 | valid_balanced_accuracy: 0.65817 |  11:01:50s\n",
      "epoch 501| loss: 0.82373 | train_balanced_accuracy: 0.72186 | valid_balanced_accuracy: 0.66619 |  11:03:08s\n",
      "epoch 502| loss: 0.82558 | train_balanced_accuracy: 0.71106 | valid_balanced_accuracy: 0.66014 |  11:04:27s\n",
      "epoch 503| loss: 0.83014 | train_balanced_accuracy: 0.6855  | valid_balanced_accuracy: 0.63803 |  11:05:46s\n",
      "epoch 504| loss: 0.82037 | train_balanced_accuracy: 0.70752 | valid_balanced_accuracy: 0.65726 |  11:07:05s\n",
      "epoch 505| loss: 0.81952 | train_balanced_accuracy: 0.72042 | valid_balanced_accuracy: 0.67324 |  11:08:23s\n",
      "epoch 506| loss: 0.81849 | train_balanced_accuracy: 0.70601 | valid_balanced_accuracy: 0.65919 |  11:09:42s\n",
      "epoch 507| loss: 0.81584 | train_balanced_accuracy: 0.70395 | valid_balanced_accuracy: 0.65209 |  11:11:01s\n",
      "epoch 508| loss: 0.8172  | train_balanced_accuracy: 0.70345 | valid_balanced_accuracy: 0.65544 |  11:12:19s\n",
      "epoch 509| loss: 0.81559 | train_balanced_accuracy: 0.70345 | valid_balanced_accuracy: 0.65927 |  11:13:38s\n",
      "epoch 510| loss: 0.81518 | train_balanced_accuracy: 0.70317 | valid_balanced_accuracy: 0.65935 |  11:14:57s\n",
      "epoch 511| loss: 0.81442 | train_balanced_accuracy: 0.71216 | valid_balanced_accuracy: 0.66158 |  11:16:16s\n",
      "epoch 512| loss: 0.81374 | train_balanced_accuracy: 0.70148 | valid_balanced_accuracy: 0.65508 |  11:17:35s\n",
      "epoch 513| loss: 0.81249 | train_balanced_accuracy: 0.70987 | valid_balanced_accuracy: 0.66217 |  11:18:54s\n",
      "epoch 514| loss: 0.80891 | train_balanced_accuracy: 0.70402 | valid_balanced_accuracy: 0.66295 |  11:20:13s\n",
      "epoch 515| loss: 0.81478 | train_balanced_accuracy: 0.70151 | valid_balanced_accuracy: 0.65767 |  11:21:32s\n",
      "epoch 516| loss: 0.81172 | train_balanced_accuracy: 0.70765 | valid_balanced_accuracy: 0.66029 |  11:22:50s\n",
      "epoch 517| loss: 0.81357 | train_balanced_accuracy: 0.70495 | valid_balanced_accuracy: 0.65568 |  11:24:09s\n",
      "epoch 518| loss: 0.81418 | train_balanced_accuracy: 0.70685 | valid_balanced_accuracy: 0.66284 |  11:25:27s\n",
      "epoch 519| loss: 0.81112 | train_balanced_accuracy: 0.71466 | valid_balanced_accuracy: 0.66329 |  11:26:46s\n",
      "epoch 520| loss: 0.80851 | train_balanced_accuracy: 0.7197  | valid_balanced_accuracy: 0.66624 |  11:28:04s\n",
      "epoch 521| loss: 0.81276 | train_balanced_accuracy: 0.72108 | valid_balanced_accuracy: 0.665   |  11:29:23s\n",
      "epoch 522| loss: 0.81585 | train_balanced_accuracy: 0.71555 | valid_balanced_accuracy: 0.66524 |  11:30:43s\n",
      "epoch 523| loss: 0.81863 | train_balanced_accuracy: 0.71512 | valid_balanced_accuracy: 0.669   |  11:32:05s\n",
      "epoch 524| loss: 0.82006 | train_balanced_accuracy: 0.71527 | valid_balanced_accuracy: 0.66326 |  11:33:27s\n",
      "epoch 525| loss: 0.81769 | train_balanced_accuracy: 0.72202 | valid_balanced_accuracy: 0.66892 |  11:34:49s\n",
      "epoch 526| loss: 0.81854 | train_balanced_accuracy: 0.70764 | valid_balanced_accuracy: 0.66042 |  11:36:11s\n",
      "epoch 527| loss: 0.82256 | train_balanced_accuracy: 0.71397 | valid_balanced_accuracy: 0.66398 |  11:37:34s\n",
      "epoch 528| loss: 0.81682 | train_balanced_accuracy: 0.72113 | valid_balanced_accuracy: 0.66954 |  11:38:55s\n",
      "epoch 529| loss: 0.81387 | train_balanced_accuracy: 0.71271 | valid_balanced_accuracy: 0.66203 |  11:40:17s\n",
      "epoch 530| loss: 0.81591 | train_balanced_accuracy: 0.72476 | valid_balanced_accuracy: 0.6711  |  11:41:38s\n",
      "epoch 531| loss: 0.81293 | train_balanced_accuracy: 0.72248 | valid_balanced_accuracy: 0.67073 |  11:43:00s\n",
      "epoch 532| loss: 0.80963 | train_balanced_accuracy: 0.71118 | valid_balanced_accuracy: 0.66311 |  11:44:21s\n",
      "epoch 533| loss: 0.81234 | train_balanced_accuracy: 0.71722 | valid_balanced_accuracy: 0.67354 |  11:45:42s\n",
      "epoch 534| loss: 0.81048 | train_balanced_accuracy: 0.71183 | valid_balanced_accuracy: 0.66521 |  11:47:03s\n",
      "epoch 535| loss: 0.80865 | train_balanced_accuracy: 0.71635 | valid_balanced_accuracy: 0.66985 |  11:48:24s\n",
      "epoch 536| loss: 0.81026 | train_balanced_accuracy: 0.71908 | valid_balanced_accuracy: 0.66775 |  11:49:44s\n",
      "epoch 537| loss: 0.80718 | train_balanced_accuracy: 0.73151 | valid_balanced_accuracy: 0.67202 |  11:51:04s\n",
      "epoch 538| loss: 0.81007 | train_balanced_accuracy: 0.72294 | valid_balanced_accuracy: 0.67174 |  11:52:25s\n",
      "epoch 539| loss: 0.81397 | train_balanced_accuracy: 0.71841 | valid_balanced_accuracy: 0.66324 |  11:53:46s\n",
      "epoch 540| loss: 0.8117  | train_balanced_accuracy: 0.72863 | valid_balanced_accuracy: 0.66903 |  11:55:07s\n",
      "epoch 541| loss: 0.81387 | train_balanced_accuracy: 0.72466 | valid_balanced_accuracy: 0.66903 |  11:56:28s\n",
      "epoch 542| loss: 0.80899 | train_balanced_accuracy: 0.72206 | valid_balanced_accuracy: 0.66928 |  11:57:49s\n",
      "epoch 543| loss: 0.8089  | train_balanced_accuracy: 0.72507 | valid_balanced_accuracy: 0.66932 |  11:59:10s\n",
      "epoch 544| loss: 0.80698 | train_balanced_accuracy: 0.72323 | valid_balanced_accuracy: 0.66985 |  12:00:31s\n",
      "epoch 545| loss: 0.80768 | train_balanced_accuracy: 0.73256 | valid_balanced_accuracy: 0.67155 |  12:01:53s\n",
      "epoch 546| loss: 0.80744 | train_balanced_accuracy: 0.71622 | valid_balanced_accuracy: 0.6645  |  12:03:14s\n",
      "epoch 547| loss: 0.80705 | train_balanced_accuracy: 0.73092 | valid_balanced_accuracy: 0.67342 |  12:04:36s\n",
      "epoch 548| loss: 0.80615 | train_balanced_accuracy: 0.70949 | valid_balanced_accuracy: 0.66937 |  12:05:58s\n",
      "epoch 549| loss: 0.81097 | train_balanced_accuracy: 0.72742 | valid_balanced_accuracy: 0.67224 |  12:07:19s\n",
      "epoch 550| loss: 0.80905 | train_balanced_accuracy: 0.72027 | valid_balanced_accuracy: 0.66825 |  12:08:41s\n",
      "epoch 551| loss: 0.80287 | train_balanced_accuracy: 0.72646 | valid_balanced_accuracy: 0.67512 |  12:10:02s\n",
      "epoch 552| loss: 0.80698 | train_balanced_accuracy: 0.71937 | valid_balanced_accuracy: 0.67153 |  12:11:21s\n",
      "epoch 553| loss: 0.80586 | train_balanced_accuracy: 0.71244 | valid_balanced_accuracy: 0.66665 |  12:12:39s\n",
      "epoch 554| loss: 0.80574 | train_balanced_accuracy: 0.7232  | valid_balanced_accuracy: 0.67063 |  12:13:58s\n",
      "epoch 555| loss: 0.80425 | train_balanced_accuracy: 0.71468 | valid_balanced_accuracy: 0.66528 |  12:15:16s\n",
      "epoch 556| loss: 0.81219 | train_balanced_accuracy: 0.72662 | valid_balanced_accuracy: 0.6646  |  12:16:35s\n",
      "epoch 557| loss: 0.80999 | train_balanced_accuracy: 0.71867 | valid_balanced_accuracy: 0.66763 |  12:17:53s\n",
      "epoch 558| loss: 0.80674 | train_balanced_accuracy: 0.72746 | valid_balanced_accuracy: 0.67438 |  12:19:12s\n",
      "epoch 559| loss: 0.8036  | train_balanced_accuracy: 0.73077 | valid_balanced_accuracy: 0.67009 |  12:20:30s\n",
      "epoch 560| loss: 0.80157 | train_balanced_accuracy: 0.72509 | valid_balanced_accuracy: 0.66906 |  12:21:49s\n",
      "epoch 561| loss: 0.80427 | train_balanced_accuracy: 0.72602 | valid_balanced_accuracy: 0.67127 |  12:23:10s\n",
      "epoch 562| loss: 0.80194 | train_balanced_accuracy: 0.72334 | valid_balanced_accuracy: 0.67015 |  12:24:32s\n",
      "epoch 563| loss: 0.80237 | train_balanced_accuracy: 0.7218  | valid_balanced_accuracy: 0.67143 |  12:25:54s\n",
      "epoch 564| loss: 0.79882 | train_balanced_accuracy: 0.73056 | valid_balanced_accuracy: 0.67814 |  12:27:16s\n",
      "epoch 565| loss: 0.79868 | train_balanced_accuracy: 0.72763 | valid_balanced_accuracy: 0.6722  |  12:28:38s\n",
      "epoch 566| loss: 0.80164 | train_balanced_accuracy: 0.71075 | valid_balanced_accuracy: 0.66651 |  12:30:00s\n",
      "epoch 567| loss: 0.79975 | train_balanced_accuracy: 0.73109 | valid_balanced_accuracy: 0.67075 |  12:31:22s\n",
      "epoch 568| loss: 0.80279 | train_balanced_accuracy: 0.72997 | valid_balanced_accuracy: 0.66774 |  12:32:43s\n",
      "epoch 569| loss: 0.80041 | train_balanced_accuracy: 0.73397 | valid_balanced_accuracy: 0.66736 |  12:34:05s\n",
      "epoch 570| loss: 0.80147 | train_balanced_accuracy: 0.7278  | valid_balanced_accuracy: 0.66579 |  12:35:26s\n",
      "epoch 571| loss: 0.80108 | train_balanced_accuracy: 0.72689 | valid_balanced_accuracy: 0.66892 |  12:36:47s\n",
      "epoch 572| loss: 0.80175 | train_balanced_accuracy: 0.71869 | valid_balanced_accuracy: 0.66848 |  12:38:09s\n",
      "epoch 573| loss: 0.79807 | train_balanced_accuracy: 0.72282 | valid_balanced_accuracy: 0.6638  |  12:39:31s\n",
      "epoch 574| loss: 0.80175 | train_balanced_accuracy: 0.72277 | valid_balanced_accuracy: 0.66353 |  12:40:51s\n",
      "epoch 575| loss: 0.80033 | train_balanced_accuracy: 0.73339 | valid_balanced_accuracy: 0.66817 |  12:42:12s\n",
      "epoch 576| loss: 0.81975 | train_balanced_accuracy: 0.70959 | valid_balanced_accuracy: 0.66208 |  12:43:30s\n",
      "epoch 577| loss: 0.80595 | train_balanced_accuracy: 0.71948 | valid_balanced_accuracy: 0.66343 |  12:44:49s\n",
      "epoch 578| loss: 0.80633 | train_balanced_accuracy: 0.71561 | valid_balanced_accuracy: 0.65874 |  12:46:08s\n",
      "epoch 579| loss: 0.7995  | train_balanced_accuracy: 0.72764 | valid_balanced_accuracy: 0.66832 |  12:47:27s\n",
      "epoch 580| loss: 0.80841 | train_balanced_accuracy: 0.71632 | valid_balanced_accuracy: 0.66026 |  12:48:45s\n",
      "epoch 581| loss: 0.80153 | train_balanced_accuracy: 0.71533 | valid_balanced_accuracy: 0.66437 |  12:50:04s\n",
      "epoch 582| loss: 0.80316 | train_balanced_accuracy: 0.72482 | valid_balanced_accuracy: 0.66573 |  12:51:22s\n",
      "epoch 583| loss: 0.80012 | train_balanced_accuracy: 0.72167 | valid_balanced_accuracy: 0.66881 |  12:52:41s\n",
      "epoch 584| loss: 0.79563 | train_balanced_accuracy: 0.72444 | valid_balanced_accuracy: 0.67091 |  12:54:00s\n",
      "epoch 585| loss: 0.79456 | train_balanced_accuracy: 0.71076 | valid_balanced_accuracy: 0.66139 |  12:55:18s\n",
      "epoch 586| loss: 0.79206 | train_balanced_accuracy: 0.72463 | valid_balanced_accuracy: 0.66173 |  12:56:37s\n",
      "epoch 587| loss: 0.79365 | train_balanced_accuracy: 0.72892 | valid_balanced_accuracy: 0.66664 |  12:57:55s\n",
      "epoch 588| loss: 0.79156 | train_balanced_accuracy: 0.73714 | valid_balanced_accuracy: 0.66982 |  12:59:13s\n",
      "epoch 589| loss: 0.79146 | train_balanced_accuracy: 0.72606 | valid_balanced_accuracy: 0.66551 |  13:00:32s\n",
      "epoch 590| loss: 0.79244 | train_balanced_accuracy: 0.73175 | valid_balanced_accuracy: 0.66829 |  13:01:50s\n",
      "epoch 591| loss: 0.79158 | train_balanced_accuracy: 0.72874 | valid_balanced_accuracy: 0.6709  |  13:03:08s\n",
      "epoch 592| loss: 0.79214 | train_balanced_accuracy: 0.72809 | valid_balanced_accuracy: 0.66687 |  13:04:26s\n",
      "epoch 593| loss: 0.78652 | train_balanced_accuracy: 0.73103 | valid_balanced_accuracy: 0.66886 |  13:05:44s\n",
      "epoch 594| loss: 0.78355 | train_balanced_accuracy: 0.72633 | valid_balanced_accuracy: 0.66816 |  13:07:02s\n",
      "epoch 595| loss: 0.78577 | train_balanced_accuracy: 0.73045 | valid_balanced_accuracy: 0.67104 |  13:08:20s\n",
      "epoch 596| loss: 0.78227 | train_balanced_accuracy: 0.7278  | valid_balanced_accuracy: 0.67434 |  13:09:38s\n",
      "epoch 597| loss: 0.78139 | train_balanced_accuracy: 0.72879 | valid_balanced_accuracy: 0.67346 |  13:10:56s\n",
      "epoch 598| loss: 0.78085 | train_balanced_accuracy: 0.74154 | valid_balanced_accuracy: 0.67437 |  13:12:14s\n",
      "epoch 599| loss: 0.78261 | train_balanced_accuracy: 0.73726 | valid_balanced_accuracy: 0.67434 |  13:13:32s\n",
      "epoch 600| loss: 0.7794  | train_balanced_accuracy: 0.72873 | valid_balanced_accuracy: 0.67056 |  13:14:50s\n",
      "epoch 601| loss: 0.78028 | train_balanced_accuracy: 0.73563 | valid_balanced_accuracy: 0.67565 |  13:16:09s\n",
      "epoch 602| loss: 0.78119 | train_balanced_accuracy: 0.73452 | valid_balanced_accuracy: 0.67679 |  13:17:28s\n",
      "epoch 603| loss: 0.77726 | train_balanced_accuracy: 0.73912 | valid_balanced_accuracy: 0.67108 |  13:18:46s\n",
      "epoch 604| loss: 0.77859 | train_balanced_accuracy: 0.73177 | valid_balanced_accuracy: 0.67544 |  13:20:06s\n",
      "epoch 605| loss: 0.78403 | train_balanced_accuracy: 0.73388 | valid_balanced_accuracy: 0.67053 |  13:21:28s\n",
      "epoch 606| loss: 0.77843 | train_balanced_accuracy: 0.7298  | valid_balanced_accuracy: 0.6726  |  13:22:47s\n",
      "epoch 607| loss: 0.77541 | train_balanced_accuracy: 0.72828 | valid_balanced_accuracy: 0.67044 |  13:24:06s\n",
      "epoch 608| loss: 0.78207 | train_balanced_accuracy: 0.73306 | valid_balanced_accuracy: 0.67539 |  13:25:24s\n",
      "epoch 609| loss: 0.78612 | train_balanced_accuracy: 0.73127 | valid_balanced_accuracy: 0.67177 |  13:26:42s\n",
      "epoch 610| loss: 0.78927 | train_balanced_accuracy: 0.73301 | valid_balanced_accuracy: 0.67439 |  13:28:01s\n",
      "epoch 611| loss: 0.78924 | train_balanced_accuracy: 0.73353 | valid_balanced_accuracy: 0.67171 |  13:29:21s\n",
      "epoch 612| loss: 0.78668 | train_balanced_accuracy: 0.72741 | valid_balanced_accuracy: 0.66566 |  13:30:42s\n",
      "epoch 613| loss: 0.79414 | train_balanced_accuracy: 0.71832 | valid_balanced_accuracy: 0.66055 |  13:32:03s\n",
      "epoch 614| loss: 0.79654 | train_balanced_accuracy: 0.72092 | valid_balanced_accuracy: 0.67164 |  13:33:24s\n",
      "epoch 615| loss: 0.78633 | train_balanced_accuracy: 0.71832 | valid_balanced_accuracy: 0.66921 |  13:34:45s\n",
      "epoch 616| loss: 0.78607 | train_balanced_accuracy: 0.72525 | valid_balanced_accuracy: 0.67082 |  13:36:06s\n",
      "epoch 617| loss: 0.77927 | train_balanced_accuracy: 0.73091 | valid_balanced_accuracy: 0.67563 |  13:37:27s\n",
      "epoch 618| loss: 0.78007 | train_balanced_accuracy: 0.73264 | valid_balanced_accuracy: 0.67281 |  13:38:49s\n",
      "epoch 619| loss: 0.77376 | train_balanced_accuracy: 0.73513 | valid_balanced_accuracy: 0.67312 |  13:40:10s\n",
      "epoch 620| loss: 0.77567 | train_balanced_accuracy: 0.73094 | valid_balanced_accuracy: 0.67606 |  13:41:30s\n",
      "epoch 621| loss: 0.77495 | train_balanced_accuracy: 0.73215 | valid_balanced_accuracy: 0.67149 |  13:42:51s\n",
      "epoch 622| loss: 0.77347 | train_balanced_accuracy: 0.74465 | valid_balanced_accuracy: 0.68066 |  13:44:11s\n",
      "epoch 623| loss: 0.77215 | train_balanced_accuracy: 0.73477 | valid_balanced_accuracy: 0.67413 |  13:45:30s\n",
      "epoch 624| loss: 0.77494 | train_balanced_accuracy: 0.72915 | valid_balanced_accuracy: 0.67597 |  13:46:49s\n",
      "epoch 625| loss: 0.77462 | train_balanced_accuracy: 0.72823 | valid_balanced_accuracy: 0.67143 |  13:48:08s\n",
      "epoch 626| loss: 0.77043 | train_balanced_accuracy: 0.73146 | valid_balanced_accuracy: 0.67255 |  13:49:27s\n",
      "epoch 627| loss: 0.77209 | train_balanced_accuracy: 0.7372  | valid_balanced_accuracy: 0.67473 |  13:50:46s\n",
      "epoch 628| loss: 0.77009 | train_balanced_accuracy: 0.73511 | valid_balanced_accuracy: 0.66606 |  13:52:06s\n",
      "epoch 629| loss: 0.76988 | train_balanced_accuracy: 0.73641 | valid_balanced_accuracy: 0.67716 |  13:53:24s\n",
      "epoch 630| loss: 0.76933 | train_balanced_accuracy: 0.73052 | valid_balanced_accuracy: 0.66749 |  13:54:43s\n",
      "epoch 631| loss: 0.78661 | train_balanced_accuracy: 0.73837 | valid_balanced_accuracy: 0.67281 |  13:56:02s\n",
      "epoch 632| loss: 0.77445 | train_balanced_accuracy: 0.73536 | valid_balanced_accuracy: 0.66977 |  13:57:20s\n",
      "epoch 633| loss: 0.77992 | train_balanced_accuracy: 0.73254 | valid_balanced_accuracy: 0.67023 |  13:58:38s\n",
      "epoch 634| loss: 0.78088 | train_balanced_accuracy: 0.73962 | valid_balanced_accuracy: 0.67402 |  13:59:57s\n",
      "epoch 635| loss: 0.77482 | train_balanced_accuracy: 0.73455 | valid_balanced_accuracy: 0.67981 |  14:01:15s\n",
      "epoch 636| loss: 0.78019 | train_balanced_accuracy: 0.7343  | valid_balanced_accuracy: 0.67002 |  14:02:33s\n",
      "epoch 637| loss: 0.78123 | train_balanced_accuracy: 0.72671 | valid_balanced_accuracy: 0.67074 |  14:03:51s\n",
      "epoch 638| loss: 0.77319 | train_balanced_accuracy: 0.73375 | valid_balanced_accuracy: 0.67406 |  14:05:09s\n",
      "epoch 639| loss: 0.76841 | train_balanced_accuracy: 0.74846 | valid_balanced_accuracy: 0.67717 |  14:06:27s\n",
      "epoch 640| loss: 0.76824 | train_balanced_accuracy: 0.74421 | valid_balanced_accuracy: 0.6721  |  14:07:45s\n",
      "epoch 641| loss: 0.77031 | train_balanced_accuracy: 0.73268 | valid_balanced_accuracy: 0.67293 |  14:09:04s\n",
      "epoch 642| loss: 0.76957 | train_balanced_accuracy: 0.73439 | valid_balanced_accuracy: 0.67467 |  14:10:22s\n",
      "epoch 643| loss: 0.76712 | train_balanced_accuracy: 0.73642 | valid_balanced_accuracy: 0.67962 |  14:11:40s\n",
      "epoch 644| loss: 0.76706 | train_balanced_accuracy: 0.74303 | valid_balanced_accuracy: 0.67892 |  14:12:58s\n",
      "epoch 645| loss: 0.76598 | train_balanced_accuracy: 0.74219 | valid_balanced_accuracy: 0.67732 |  14:14:16s\n",
      "epoch 646| loss: 0.7673  | train_balanced_accuracy: 0.73755 | valid_balanced_accuracy: 0.67893 |  14:15:34s\n",
      "epoch 647| loss: 0.76867 | train_balanced_accuracy: 0.74247 | valid_balanced_accuracy: 0.67781 |  14:16:52s\n",
      "epoch 648| loss: 0.77104 | train_balanced_accuracy: 0.73632 | valid_balanced_accuracy: 0.6736  |  14:18:11s\n",
      "epoch 649| loss: 0.76824 | train_balanced_accuracy: 0.74288 | valid_balanced_accuracy: 0.67041 |  14:19:29s\n",
      "epoch 650| loss: 0.76622 | train_balanced_accuracy: 0.73946 | valid_balanced_accuracy: 0.67647 |  14:20:47s\n",
      "epoch 651| loss: 0.76297 | train_balanced_accuracy: 0.74944 | valid_balanced_accuracy: 0.6797  |  14:22:05s\n",
      "epoch 652| loss: 0.76754 | train_balanced_accuracy: 0.7504  | valid_balanced_accuracy: 0.67855 |  14:23:23s\n",
      "epoch 653| loss: 0.76641 | train_balanced_accuracy: 0.74821 | valid_balanced_accuracy: 0.67928 |  14:24:41s\n",
      "epoch 654| loss: 0.76619 | train_balanced_accuracy: 0.74781 | valid_balanced_accuracy: 0.68314 |  14:26:00s\n",
      "epoch 655| loss: 0.76366 | train_balanced_accuracy: 0.7469  | valid_balanced_accuracy: 0.67604 |  14:27:18s\n",
      "epoch 656| loss: 0.76702 | train_balanced_accuracy: 0.7477  | valid_balanced_accuracy: 0.67844 |  14:28:36s\n",
      "epoch 657| loss: 0.77375 | train_balanced_accuracy: 0.74903 | valid_balanced_accuracy: 0.67801 |  14:29:54s\n",
      "epoch 658| loss: 0.77453 | train_balanced_accuracy: 0.74046 | valid_balanced_accuracy: 0.67849 |  14:31:12s\n",
      "epoch 659| loss: 0.7747  | train_balanced_accuracy: 0.74145 | valid_balanced_accuracy: 0.67206 |  14:32:30s\n",
      "epoch 660| loss: 0.76907 | train_balanced_accuracy: 0.74084 | valid_balanced_accuracy: 0.67949 |  14:33:48s\n",
      "epoch 661| loss: 0.76721 | train_balanced_accuracy: 0.74324 | valid_balanced_accuracy: 0.67552 |  14:35:06s\n",
      "epoch 662| loss: 0.7658  | train_balanced_accuracy: 0.74287 | valid_balanced_accuracy: 0.6779  |  14:36:25s\n",
      "epoch 663| loss: 0.76638 | train_balanced_accuracy: 0.73735 | valid_balanced_accuracy: 0.67627 |  14:37:43s\n",
      "epoch 664| loss: 0.76627 | train_balanced_accuracy: 0.74252 | valid_balanced_accuracy: 0.67982 |  14:39:01s\n",
      "epoch 665| loss: 0.76602 | train_balanced_accuracy: 0.7462  | valid_balanced_accuracy: 0.67249 |  14:40:19s\n",
      "epoch 666| loss: 0.76407 | train_balanced_accuracy: 0.74654 | valid_balanced_accuracy: 0.67473 |  14:41:37s\n",
      "epoch 667| loss: 0.76693 | train_balanced_accuracy: 0.74431 | valid_balanced_accuracy: 0.67902 |  14:42:55s\n",
      "epoch 668| loss: 0.7683  | train_balanced_accuracy: 0.73828 | valid_balanced_accuracy: 0.67443 |  14:44:14s\n",
      "epoch 669| loss: 0.76104 | train_balanced_accuracy: 0.7485  | valid_balanced_accuracy: 0.68214 |  14:45:32s\n",
      "epoch 670| loss: 0.76344 | train_balanced_accuracy: 0.75354 | valid_balanced_accuracy: 0.6777  |  14:46:50s\n",
      "epoch 671| loss: 0.76587 | train_balanced_accuracy: 0.74258 | valid_balanced_accuracy: 0.66583 |  14:48:08s\n",
      "epoch 672| loss: 0.76711 | train_balanced_accuracy: 0.73782 | valid_balanced_accuracy: 0.67637 |  14:49:26s\n",
      "epoch 673| loss: 0.76691 | train_balanced_accuracy: 0.7462  | valid_balanced_accuracy: 0.68095 |  14:50:45s\n",
      "epoch 674| loss: 0.76585 | train_balanced_accuracy: 0.7482  | valid_balanced_accuracy: 0.67921 |  14:52:03s\n",
      "epoch 675| loss: 0.76625 | train_balanced_accuracy: 0.74151 | valid_balanced_accuracy: 0.67431 |  14:53:21s\n",
      "epoch 676| loss: 0.7655  | train_balanced_accuracy: 0.73179 | valid_balanced_accuracy: 0.67557 |  14:54:39s\n",
      "epoch 677| loss: 0.7641  | train_balanced_accuracy: 0.7393  | valid_balanced_accuracy: 0.67941 |  14:55:57s\n",
      "epoch 678| loss: 0.7655  | train_balanced_accuracy: 0.73005 | valid_balanced_accuracy: 0.67307 |  14:57:15s\n",
      "epoch 679| loss: 0.76471 | train_balanced_accuracy: 0.7354  | valid_balanced_accuracy: 0.67579 |  14:58:34s\n",
      "epoch 680| loss: 0.76655 | train_balanced_accuracy: 0.73654 | valid_balanced_accuracy: 0.67648 |  14:59:52s\n",
      "epoch 681| loss: 0.76312 | train_balanced_accuracy: 0.7437  | valid_balanced_accuracy: 0.67928 |  15:01:10s\n",
      "epoch 682| loss: 0.7624  | train_balanced_accuracy: 0.74763 | valid_balanced_accuracy: 0.67969 |  15:02:28s\n",
      "epoch 683| loss: 0.76051 | train_balanced_accuracy: 0.75245 | valid_balanced_accuracy: 0.6795  |  15:03:46s\n",
      "epoch 684| loss: 0.75872 | train_balanced_accuracy: 0.74975 | valid_balanced_accuracy: 0.68343 |  15:05:04s\n",
      "epoch 685| loss: 0.75981 | train_balanced_accuracy: 0.74035 | valid_balanced_accuracy: 0.67981 |  15:06:23s\n",
      "epoch 686| loss: 0.75724 | train_balanced_accuracy: 0.74936 | valid_balanced_accuracy: 0.67911 |  15:07:41s\n",
      "epoch 687| loss: 0.76277 | train_balanced_accuracy: 0.75492 | valid_balanced_accuracy: 0.68178 |  15:08:59s\n",
      "epoch 688| loss: 0.76273 | train_balanced_accuracy: 0.74592 | valid_balanced_accuracy: 0.67374 |  15:10:17s\n",
      "epoch 689| loss: 0.75738 | train_balanced_accuracy: 0.74166 | valid_balanced_accuracy: 0.6706  |  15:11:35s\n",
      "epoch 690| loss: 0.7626  | train_balanced_accuracy: 0.74408 | valid_balanced_accuracy: 0.67421 |  15:12:53s\n",
      "epoch 691| loss: 0.75896 | train_balanced_accuracy: 0.7446  | valid_balanced_accuracy: 0.67599 |  15:14:11s\n",
      "epoch 692| loss: 0.75984 | train_balanced_accuracy: 0.74399 | valid_balanced_accuracy: 0.67591 |  15:15:30s\n",
      "epoch 693| loss: 0.76098 | train_balanced_accuracy: 0.73541 | valid_balanced_accuracy: 0.6698  |  15:16:48s\n",
      "epoch 694| loss: 0.76035 | train_balanced_accuracy: 0.75773 | valid_balanced_accuracy: 0.67959 |  15:18:06s\n",
      "epoch 695| loss: 0.75776 | train_balanced_accuracy: 0.74964 | valid_balanced_accuracy: 0.6775  |  15:19:24s\n",
      "epoch 696| loss: 0.75848 | train_balanced_accuracy: 0.74068 | valid_balanced_accuracy: 0.6815  |  15:20:42s\n",
      "epoch 697| loss: 0.75937 | train_balanced_accuracy: 0.73265 | valid_balanced_accuracy: 0.67582 |  15:22:00s\n",
      "epoch 698| loss: 0.75618 | train_balanced_accuracy: 0.73122 | valid_balanced_accuracy: 0.66933 |  15:23:19s\n",
      "epoch 699| loss: 0.75925 | train_balanced_accuracy: 0.73898 | valid_balanced_accuracy: 0.67642 |  15:24:37s\n",
      "epoch 700| loss: 0.757   | train_balanced_accuracy: 0.74093 | valid_balanced_accuracy: 0.67293 |  15:25:55s\n",
      "epoch 701| loss: 0.75593 | train_balanced_accuracy: 0.74479 | valid_balanced_accuracy: 0.67397 |  15:27:13s\n",
      "epoch 702| loss: 0.75877 | train_balanced_accuracy: 0.73314 | valid_balanced_accuracy: 0.67289 |  15:28:31s\n",
      "epoch 703| loss: 0.75477 | train_balanced_accuracy: 0.74478 | valid_balanced_accuracy: 0.68105 |  15:29:49s\n",
      "epoch 704| loss: 0.75559 | train_balanced_accuracy: 0.73787 | valid_balanced_accuracy: 0.67589 |  15:31:07s\n",
      "epoch 705| loss: 0.75498 | train_balanced_accuracy: 0.74189 | valid_balanced_accuracy: 0.68069 |  15:32:26s\n",
      "epoch 706| loss: 0.75738 | train_balanced_accuracy: 0.73566 | valid_balanced_accuracy: 0.67193 |  15:33:44s\n",
      "epoch 707| loss: 0.76098 | train_balanced_accuracy: 0.74214 | valid_balanced_accuracy: 0.67852 |  15:35:02s\n",
      "epoch 708| loss: 0.76313 | train_balanced_accuracy: 0.7468  | valid_balanced_accuracy: 0.67832 |  15:36:20s\n",
      "epoch 709| loss: 0.76381 | train_balanced_accuracy: 0.74107 | valid_balanced_accuracy: 0.68056 |  15:37:38s\n",
      "epoch 710| loss: 0.76374 | train_balanced_accuracy: 0.73439 | valid_balanced_accuracy: 0.6694  |  15:38:57s\n",
      "epoch 711| loss: 0.76496 | train_balanced_accuracy: 0.74229 | valid_balanced_accuracy: 0.67514 |  15:40:15s\n",
      "epoch 712| loss: 0.76574 | train_balanced_accuracy: 0.74531 | valid_balanced_accuracy: 0.68269 |  15:41:33s\n",
      "epoch 713| loss: 0.76284 | train_balanced_accuracy: 0.7354  | valid_balanced_accuracy: 0.67706 |  15:42:52s\n",
      "epoch 714| loss: 0.76196 | train_balanced_accuracy: 0.74086 | valid_balanced_accuracy: 0.67676 |  15:44:10s\n",
      "epoch 715| loss: 0.76137 | train_balanced_accuracy: 0.74917 | valid_balanced_accuracy: 0.68341 |  15:45:29s\n",
      "epoch 716| loss: 0.76475 | train_balanced_accuracy: 0.75059 | valid_balanced_accuracy: 0.67598 |  15:46:48s\n",
      "epoch 717| loss: 0.7639  | train_balanced_accuracy: 0.7493  | valid_balanced_accuracy: 0.67501 |  15:48:07s\n",
      "epoch 718| loss: 0.7606  | train_balanced_accuracy: 0.74833 | valid_balanced_accuracy: 0.67382 |  15:49:26s\n",
      "epoch 719| loss: 0.76208 | train_balanced_accuracy: 0.75261 | valid_balanced_accuracy: 0.67525 |  15:50:44s\n",
      "epoch 720| loss: 0.75961 | train_balanced_accuracy: 0.74694 | valid_balanced_accuracy: 0.67276 |  15:52:03s\n",
      "epoch 721| loss: 0.75497 | train_balanced_accuracy: 0.74113 | valid_balanced_accuracy: 0.67946 |  15:53:22s\n",
      "epoch 722| loss: 0.75721 | train_balanced_accuracy: 0.75078 | valid_balanced_accuracy: 0.68464 |  15:54:41s\n",
      "epoch 723| loss: 0.75789 | train_balanced_accuracy: 0.75504 | valid_balanced_accuracy: 0.68115 |  15:56:01s\n",
      "epoch 724| loss: 0.75748 | train_balanced_accuracy: 0.75148 | valid_balanced_accuracy: 0.67411 |  15:57:20s\n",
      "epoch 725| loss: 0.76342 | train_balanced_accuracy: 0.74731 | valid_balanced_accuracy: 0.68341 |  15:58:39s\n",
      "epoch 726| loss: 0.76603 | train_balanced_accuracy: 0.74561 | valid_balanced_accuracy: 0.68002 |  15:59:58s\n",
      "epoch 727| loss: 0.75581 | train_balanced_accuracy: 0.73625 | valid_balanced_accuracy: 0.6778  |  16:01:16s\n",
      "epoch 728| loss: 0.75272 | train_balanced_accuracy: 0.74632 | valid_balanced_accuracy: 0.67939 |  16:02:35s\n",
      "epoch 729| loss: 0.76653 | train_balanced_accuracy: 0.74241 | valid_balanced_accuracy: 0.67642 |  16:03:53s\n",
      "epoch 730| loss: 0.7657  | train_balanced_accuracy: 0.74098 | valid_balanced_accuracy: 0.67486 |  16:05:12s\n",
      "epoch 731| loss: 0.76373 | train_balanced_accuracy: 0.73706 | valid_balanced_accuracy: 0.67161 |  16:06:31s\n",
      "epoch 732| loss: 0.76383 | train_balanced_accuracy: 0.73831 | valid_balanced_accuracy: 0.66542 |  16:07:49s\n",
      "epoch 733| loss: 0.76586 | train_balanced_accuracy: 0.73843 | valid_balanced_accuracy: 0.67305 |  16:09:08s\n",
      "epoch 734| loss: 0.76187 | train_balanced_accuracy: 0.74931 | valid_balanced_accuracy: 0.6827  |  16:10:27s\n",
      "epoch 735| loss: 0.75811 | train_balanced_accuracy: 0.73353 | valid_balanced_accuracy: 0.67041 |  16:11:45s\n",
      "epoch 736| loss: 0.75834 | train_balanced_accuracy: 0.73849 | valid_balanced_accuracy: 0.67504 |  16:13:03s\n",
      "epoch 737| loss: 0.7573  | train_balanced_accuracy: 0.74978 | valid_balanced_accuracy: 0.67463 |  16:14:21s\n",
      "epoch 738| loss: 0.7583  | train_balanced_accuracy: 0.74106 | valid_balanced_accuracy: 0.67658 |  16:15:40s\n",
      "epoch 739| loss: 0.76058 | train_balanced_accuracy: 0.74014 | valid_balanced_accuracy: 0.67619 |  16:16:58s\n",
      "epoch 740| loss: 0.75743 | train_balanced_accuracy: 0.74174 | valid_balanced_accuracy: 0.68226 |  16:18:16s\n",
      "epoch 741| loss: 0.75654 | train_balanced_accuracy: 0.73285 | valid_balanced_accuracy: 0.67074 |  16:19:34s\n",
      "epoch 742| loss: 0.75557 | train_balanced_accuracy: 0.74252 | valid_balanced_accuracy: 0.67153 |  16:20:52s\n",
      "epoch 743| loss: 0.75743 | train_balanced_accuracy: 0.74568 | valid_balanced_accuracy: 0.6804  |  16:22:10s\n",
      "epoch 744| loss: 0.75546 | train_balanced_accuracy: 0.74143 | valid_balanced_accuracy: 0.67734 |  16:23:28s\n",
      "epoch 745| loss: 0.75189 | train_balanced_accuracy: 0.73318 | valid_balanced_accuracy: 0.67363 |  16:24:46s\n",
      "epoch 746| loss: 0.7553  | train_balanced_accuracy: 0.74333 | valid_balanced_accuracy: 0.67867 |  16:26:04s\n",
      "epoch 747| loss: 0.75324 | train_balanced_accuracy: 0.74297 | valid_balanced_accuracy: 0.67205 |  16:27:23s\n",
      "epoch 748| loss: 0.76221 | train_balanced_accuracy: 0.74422 | valid_balanced_accuracy: 0.67807 |  16:28:41s\n",
      "epoch 749| loss: 0.76024 | train_balanced_accuracy: 0.74    | valid_balanced_accuracy: 0.67567 |  16:29:59s\n",
      "epoch 750| loss: 0.75621 | train_balanced_accuracy: 0.73554 | valid_balanced_accuracy: 0.67269 |  16:31:18s\n",
      "epoch 751| loss: 0.75555 | train_balanced_accuracy: 0.74582 | valid_balanced_accuracy: 0.68363 |  16:32:36s\n",
      "epoch 752| loss: 0.75556 | train_balanced_accuracy: 0.74426 | valid_balanced_accuracy: 0.67819 |  16:33:54s\n",
      "epoch 753| loss: 0.75773 | train_balanced_accuracy: 0.74799 | valid_balanced_accuracy: 0.685   |  16:35:13s\n",
      "epoch 754| loss: 0.7535  | train_balanced_accuracy: 0.75201 | valid_balanced_accuracy: 0.6877  |  16:36:31s\n",
      "epoch 755| loss: 0.75101 | train_balanced_accuracy: 0.75189 | valid_balanced_accuracy: 0.68659 |  16:37:49s\n",
      "epoch 756| loss: 0.7528  | train_balanced_accuracy: 0.74709 | valid_balanced_accuracy: 0.67921 |  16:39:08s\n",
      "epoch 757| loss: 0.74948 | train_balanced_accuracy: 0.75318 | valid_balanced_accuracy: 0.67894 |  16:40:27s\n",
      "epoch 758| loss: 0.75076 | train_balanced_accuracy: 0.7436  | valid_balanced_accuracy: 0.67894 |  16:41:45s\n",
      "epoch 759| loss: 0.75486 | train_balanced_accuracy: 0.75304 | valid_balanced_accuracy: 0.67585 |  16:43:04s\n",
      "epoch 760| loss: 0.74999 | train_balanced_accuracy: 0.75464 | valid_balanced_accuracy: 0.68716 |  16:44:23s\n",
      "epoch 761| loss: 0.75152 | train_balanced_accuracy: 0.74476 | valid_balanced_accuracy: 0.68025 |  16:45:42s\n",
      "epoch 762| loss: 0.75306 | train_balanced_accuracy: 0.74337 | valid_balanced_accuracy: 0.67608 |  16:47:00s\n",
      "epoch 763| loss: 0.75093 | train_balanced_accuracy: 0.74013 | valid_balanced_accuracy: 0.67227 |  16:48:19s\n",
      "epoch 764| loss: 0.76524 | train_balanced_accuracy: 0.73144 | valid_balanced_accuracy: 0.67289 |  16:49:38s\n",
      "epoch 765| loss: 0.75946 | train_balanced_accuracy: 0.75439 | valid_balanced_accuracy: 0.67713 |  16:50:57s\n",
      "epoch 766| loss: 0.75666 | train_balanced_accuracy: 0.74117 | valid_balanced_accuracy: 0.6779  |  16:52:15s\n",
      "epoch 767| loss: 0.75725 | train_balanced_accuracy: 0.74616 | valid_balanced_accuracy: 0.67937 |  16:53:34s\n",
      "epoch 768| loss: 0.75789 | train_balanced_accuracy: 0.74534 | valid_balanced_accuracy: 0.6694  |  16:54:52s\n",
      "epoch 769| loss: 0.75338 | train_balanced_accuracy: 0.73891 | valid_balanced_accuracy: 0.67222 |  16:56:10s\n",
      "epoch 770| loss: 0.75481 | train_balanced_accuracy: 0.74095 | valid_balanced_accuracy: 0.6784  |  16:57:29s\n",
      "epoch 771| loss: 0.75132 | train_balanced_accuracy: 0.75494 | valid_balanced_accuracy: 0.67737 |  16:58:47s\n",
      "epoch 772| loss: 0.75283 | train_balanced_accuracy: 0.74142 | valid_balanced_accuracy: 0.67622 |  17:00:05s\n",
      "epoch 773| loss: 0.75033 | train_balanced_accuracy: 0.73897 | valid_balanced_accuracy: 0.67611 |  17:01:23s\n",
      "epoch 774| loss: 0.75113 | train_balanced_accuracy: 0.74286 | valid_balanced_accuracy: 0.67635 |  17:02:42s\n",
      "epoch 775| loss: 0.75312 | train_balanced_accuracy: 0.73602 | valid_balanced_accuracy: 0.66439 |  17:04:00s\n",
      "epoch 776| loss: 0.74977 | train_balanced_accuracy: 0.74672 | valid_balanced_accuracy: 0.68106 |  17:05:19s\n",
      "epoch 777| loss: 0.74701 | train_balanced_accuracy: 0.74442 | valid_balanced_accuracy: 0.67728 |  17:06:37s\n",
      "epoch 778| loss: 0.74456 | train_balanced_accuracy: 0.75164 | valid_balanced_accuracy: 0.67629 |  17:07:56s\n",
      "epoch 779| loss: 0.74675 | train_balanced_accuracy: 0.74172 | valid_balanced_accuracy: 0.67053 |  17:09:14s\n",
      "epoch 780| loss: 0.74974 | train_balanced_accuracy: 0.74855 | valid_balanced_accuracy: 0.67816 |  17:10:32s\n",
      "epoch 781| loss: 0.75029 | train_balanced_accuracy: 0.74928 | valid_balanced_accuracy: 0.67881 |  17:11:50s\n",
      "epoch 782| loss: 0.74573 | train_balanced_accuracy: 0.75347 | valid_balanced_accuracy: 0.67858 |  17:13:09s\n",
      "epoch 783| loss: 0.74811 | train_balanced_accuracy: 0.74467 | valid_balanced_accuracy: 0.67474 |  17:14:28s\n",
      "epoch 784| loss: 0.74668 | train_balanced_accuracy: 0.74831 | valid_balanced_accuracy: 0.67392 |  17:15:49s\n",
      "epoch 785| loss: 0.74538 | train_balanced_accuracy: 0.75568 | valid_balanced_accuracy: 0.67637 |  17:17:09s\n",
      "epoch 786| loss: 0.74311 | train_balanced_accuracy: 0.75158 | valid_balanced_accuracy: 0.67777 |  17:18:30s\n",
      "epoch 787| loss: 0.74664 | train_balanced_accuracy: 0.73459 | valid_balanced_accuracy: 0.66953 |  17:19:49s\n",
      "epoch 788| loss: 0.74559 | train_balanced_accuracy: 0.75317 | valid_balanced_accuracy: 0.67639 |  17:21:08s\n",
      "epoch 789| loss: 0.74851 | train_balanced_accuracy: 0.74852 | valid_balanced_accuracy: 0.67798 |  17:22:27s\n",
      "epoch 790| loss: 0.74429 | train_balanced_accuracy: 0.74355 | valid_balanced_accuracy: 0.67749 |  17:23:46s\n",
      "epoch 791| loss: 0.74631 | train_balanced_accuracy: 0.74148 | valid_balanced_accuracy: 0.67608 |  17:25:05s\n",
      "epoch 792| loss: 0.74142 | train_balanced_accuracy: 0.73557 | valid_balanced_accuracy: 0.67554 |  17:26:23s\n",
      "epoch 793| loss: 0.74435 | train_balanced_accuracy: 0.74372 | valid_balanced_accuracy: 0.67522 |  17:27:42s\n",
      "epoch 794| loss: 0.74368 | train_balanced_accuracy: 0.73696 | valid_balanced_accuracy: 0.67109 |  17:29:00s\n",
      "epoch 795| loss: 0.74381 | train_balanced_accuracy: 0.74362 | valid_balanced_accuracy: 0.67838 |  17:30:19s\n",
      "epoch 796| loss: 0.74274 | train_balanced_accuracy: 0.73962 | valid_balanced_accuracy: 0.67241 |  17:31:37s\n",
      "epoch 797| loss: 0.74329 | train_balanced_accuracy: 0.74581 | valid_balanced_accuracy: 0.67624 |  17:32:55s\n",
      "epoch 798| loss: 0.74191 | train_balanced_accuracy: 0.73787 | valid_balanced_accuracy: 0.66861 |  17:34:13s\n",
      "epoch 799| loss: 0.74486 | train_balanced_accuracy: 0.75117 | valid_balanced_accuracy: 0.68102 |  17:35:31s\n",
      "epoch 800| loss: 0.74086 | train_balanced_accuracy: 0.72981 | valid_balanced_accuracy: 0.66692 |  17:36:50s\n",
      "epoch 801| loss: 0.74209 | train_balanced_accuracy: 0.74759 | valid_balanced_accuracy: 0.6747  |  17:38:08s\n",
      "epoch 802| loss: 0.74187 | train_balanced_accuracy: 0.74489 | valid_balanced_accuracy: 0.67456 |  17:39:26s\n",
      "epoch 803| loss: 0.74334 | train_balanced_accuracy: 0.74421 | valid_balanced_accuracy: 0.67507 |  17:40:44s\n",
      "epoch 804| loss: 0.73931 | train_balanced_accuracy: 0.73216 | valid_balanced_accuracy: 0.67073 |  17:42:02s\n",
      "epoch 805| loss: 0.74346 | train_balanced_accuracy: 0.74568 | valid_balanced_accuracy: 0.67869 |  17:43:21s\n",
      "epoch 806| loss: 0.74219 | train_balanced_accuracy: 0.73841 | valid_balanced_accuracy: 0.67637 |  17:44:39s\n",
      "epoch 807| loss: 0.74222 | train_balanced_accuracy: 0.75096 | valid_balanced_accuracy: 0.6812  |  17:45:57s\n",
      "epoch 808| loss: 0.82417 | train_balanced_accuracy: 0.7281  | valid_balanced_accuracy: 0.66943 |  17:47:15s\n",
      "epoch 809| loss: 0.8013  | train_balanced_accuracy: 0.72437 | valid_balanced_accuracy: 0.66423 |  17:48:33s\n",
      "epoch 810| loss: 0.75616 | train_balanced_accuracy: 0.73002 | valid_balanced_accuracy: 0.6692  |  17:49:51s\n",
      "epoch 811| loss: 0.74367 | train_balanced_accuracy: 0.73503 | valid_balanced_accuracy: 0.67021 |  17:51:10s\n",
      "epoch 812| loss: 0.74361 | train_balanced_accuracy: 0.75234 | valid_balanced_accuracy: 0.67807 |  17:52:28s\n",
      "epoch 813| loss: 0.74067 | train_balanced_accuracy: 0.74271 | valid_balanced_accuracy: 0.67223 |  17:53:46s\n",
      "epoch 814| loss: 0.73853 | train_balanced_accuracy: 0.75165 | valid_balanced_accuracy: 0.67481 |  17:55:05s\n",
      "epoch 815| loss: 0.74238 | train_balanced_accuracy: 0.74112 | valid_balanced_accuracy: 0.67325 |  17:56:24s\n",
      "epoch 816| loss: 0.7394  | train_balanced_accuracy: 0.74321 | valid_balanced_accuracy: 0.67249 |  17:57:44s\n",
      "epoch 817| loss: 0.74191 | train_balanced_accuracy: 0.73496 | valid_balanced_accuracy: 0.67328 |  17:59:04s\n",
      "epoch 818| loss: 0.74174 | train_balanced_accuracy: 0.75439 | valid_balanced_accuracy: 0.68335 |  18:00:23s\n",
      "epoch 819| loss: 0.74056 | train_balanced_accuracy: 0.73225 | valid_balanced_accuracy: 0.66943 |  18:01:42s\n",
      "epoch 820| loss: 0.73833 | train_balanced_accuracy: 0.73867 | valid_balanced_accuracy: 0.67251 |  18:03:01s\n",
      "epoch 821| loss: 0.73976 | train_balanced_accuracy: 0.73863 | valid_balanced_accuracy: 0.66714 |  18:04:19s\n",
      "epoch 822| loss: 0.7394  | train_balanced_accuracy: 0.74886 | valid_balanced_accuracy: 0.67223 |  18:05:38s\n",
      "epoch 823| loss: 0.73697 | train_balanced_accuracy: 0.75766 | valid_balanced_accuracy: 0.67745 |  18:06:56s\n",
      "epoch 824| loss: 0.73584 | train_balanced_accuracy: 0.74815 | valid_balanced_accuracy: 0.67407 |  18:08:17s\n",
      "epoch 825| loss: 0.7357  | train_balanced_accuracy: 0.74435 | valid_balanced_accuracy: 0.67733 |  18:09:38s\n",
      "epoch 826| loss: 0.73635 | train_balanced_accuracy: 0.74558 | valid_balanced_accuracy: 0.6714  |  18:10:58s\n",
      "epoch 827| loss: 0.74025 | train_balanced_accuracy: 0.75282 | valid_balanced_accuracy: 0.67686 |  18:12:16s\n",
      "epoch 828| loss: 0.73695 | train_balanced_accuracy: 0.73891 | valid_balanced_accuracy: 0.6696  |  18:13:35s\n",
      "epoch 829| loss: 0.7382  | train_balanced_accuracy: 0.74281 | valid_balanced_accuracy: 0.67391 |  18:14:54s\n",
      "epoch 830| loss: 0.73841 | train_balanced_accuracy: 0.74821 | valid_balanced_accuracy: 0.67533 |  18:16:13s\n",
      "epoch 831| loss: 0.73693 | train_balanced_accuracy: 0.7491  | valid_balanced_accuracy: 0.67958 |  18:17:32s\n",
      "epoch 832| loss: 0.73709 | train_balanced_accuracy: 0.74497 | valid_balanced_accuracy: 0.67853 |  18:18:51s\n",
      "epoch 833| loss: 0.73287 | train_balanced_accuracy: 0.74255 | valid_balanced_accuracy: 0.67098 |  18:20:09s\n",
      "epoch 834| loss: 0.73753 | train_balanced_accuracy: 0.73479 | valid_balanced_accuracy: 0.66996 |  18:21:28s\n",
      "epoch 835| loss: 0.73723 | train_balanced_accuracy: 0.73944 | valid_balanced_accuracy: 0.66853 |  18:22:48s\n",
      "epoch 836| loss: 0.73578 | train_balanced_accuracy: 0.74698 | valid_balanced_accuracy: 0.67944 |  18:24:07s\n",
      "epoch 837| loss: 0.74014 | train_balanced_accuracy: 0.75215 | valid_balanced_accuracy: 0.68611 |  18:25:27s\n",
      "epoch 838| loss: 0.73616 | train_balanced_accuracy: 0.74431 | valid_balanced_accuracy: 0.67843 |  18:26:46s\n",
      "epoch 839| loss: 0.73768 | train_balanced_accuracy: 0.74151 | valid_balanced_accuracy: 0.67398 |  18:28:05s\n",
      "epoch 840| loss: 0.73928 | train_balanced_accuracy: 0.75595 | valid_balanced_accuracy: 0.68001 |  18:29:24s\n",
      "epoch 841| loss: 0.7357  | train_balanced_accuracy: 0.75659 | valid_balanced_accuracy: 0.68435 |  18:30:42s\n",
      "epoch 842| loss: 0.7355  | train_balanced_accuracy: 0.75913 | valid_balanced_accuracy: 0.6814  |  18:32:00s\n",
      "epoch 843| loss: 0.73668 | train_balanced_accuracy: 0.75793 | valid_balanced_accuracy: 0.68512 |  18:33:19s\n",
      "epoch 844| loss: 0.73843 | train_balanced_accuracy: 0.75089 | valid_balanced_accuracy: 0.67125 |  18:34:37s\n",
      "epoch 845| loss: 0.73728 | train_balanced_accuracy: 0.74691 | valid_balanced_accuracy: 0.67687 |  18:35:55s\n",
      "epoch 846| loss: 0.7406  | train_balanced_accuracy: 0.74707 | valid_balanced_accuracy: 0.66974 |  18:37:13s\n",
      "epoch 847| loss: 0.74106 | train_balanced_accuracy: 0.7583  | valid_balanced_accuracy: 0.67775 |  18:38:32s\n",
      "epoch 848| loss: 0.73757 | train_balanced_accuracy: 0.7548  | valid_balanced_accuracy: 0.67702 |  18:39:50s\n",
      "epoch 849| loss: 0.73598 | train_balanced_accuracy: 0.75709 | valid_balanced_accuracy: 0.68024 |  18:41:08s\n",
      "epoch 850| loss: 0.73805 | train_balanced_accuracy: 0.7472  | valid_balanced_accuracy: 0.67749 |  18:42:26s\n",
      "epoch 851| loss: 0.73324 | train_balanced_accuracy: 0.74535 | valid_balanced_accuracy: 0.66702 |  18:43:45s\n",
      "epoch 852| loss: 0.73763 | train_balanced_accuracy: 0.75291 | valid_balanced_accuracy: 0.67711 |  18:45:03s\n",
      "epoch 853| loss: 0.73558 | train_balanced_accuracy: 0.75194 | valid_balanced_accuracy: 0.67489 |  18:46:21s\n",
      "epoch 854| loss: 0.7331  | train_balanced_accuracy: 0.74721 | valid_balanced_accuracy: 0.67671 |  18:47:39s\n",
      "\n",
      "Early stopping occured at epoch 854 with best_epoch = 754 and best_valid_balanced_accuracy = 0.6877\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "tab_net.fit(X_train=x_train, y_train=y_train,eval_set=[(x_train, y_train),(x_val, y_val)], eval_name=['train', 'valid'], eval_metric=['balanced_accuracy'], max_epochs=1000, patience=100, num_workers=0, drop_last=False, batch_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 372.103125 248.518125\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 372.103125 248.518125 \nL 372.103125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 224.64 \nL 364.903125 224.64 \nL 364.903125 7.2 \nL 30.103125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mb6d106cf66\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#mb6d106cf66\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(42.140057 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"116.600847\" xlink:href=\"#mb6d106cf66\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 200 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(107.057097 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"187.880387\" xlink:href=\"#mb6d106cf66\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 400 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(178.336637 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"259.159927\" xlink:href=\"#mb6d106cf66\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 600 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(249.616177 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"330.439467\" xlink:href=\"#mb6d106cf66\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 800 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(320.895717 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_6\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m50598d5919\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m50598d5919\" y=\"201.570261\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0.1 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(7.2 205.36948)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m50598d5919\" y=\"173.580972\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.2 -->\n      <g transform=\"translate(7.2 177.380191)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m50598d5919\" y=\"145.591684\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.3 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(7.2 149.390903)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m50598d5919\" y=\"117.602396\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.4 -->\n      <g transform=\"translate(7.2 121.401614)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m50598d5919\" y=\"89.613107\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(7.2 93.412326)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m50598d5919\" y=\"61.623819\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.6 -->\n      <g transform=\"translate(7.2 65.423038)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m50598d5919\" y=\"33.63453\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.7 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(7.2 37.433749)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_13\">\n    <path clip-path=\"url(#p3e0b231bfb)\" d=\"M 45.321307 214.329373 \nL 45.677705 198.20479 \nL 46.034102 196.668307 \nL 47.103295 173.413898 \nL 47.459693 168.447391 \nL 48.172488 150.481328 \nL 48.528886 147.083349 \nL 49.241682 134.546547 \nL 49.598079 131.034146 \nL 49.954477 131.15026 \nL 50.310875 119.35363 \nL 50.667272 119.805542 \nL 51.02367 113.271522 \nL 51.380068 110.412811 \nL 52.092863 108.776546 \nL 52.449261 105.674118 \nL 52.805659 104.149866 \nL 53.162056 103.710522 \nL 53.518454 101.942042 \nL 53.874852 100.779223 \nL 54.231249 102.614583 \nL 54.587647 98.235541 \nL 54.944045 98.041033 \nL 55.300442 98.623663 \nL 55.65684 95.194702 \nL 56.013238 90.561659 \nL 56.369636 91.56207 \nL 56.726033 89.960392 \nL 57.082431 85.435615 \nL 57.438829 86.083269 \nL 57.795226 82.518736 \nL 58.151624 83.757055 \nL 58.508022 83.58159 \nL 58.864419 81.141907 \nL 59.220817 79.988311 \nL 59.577215 82.653645 \nL 59.933613 78.994765 \nL 60.29001 76.548076 \nL 60.646408 76.591574 \nL 61.002806 78.898972 \nL 61.359203 78.626406 \nL 61.715601 77.455172 \nL 62.071999 85.416338 \nL 62.428396 82.615753 \nL 62.784794 83.265016 \nL 63.141192 74.565588 \nL 63.49759 76.636743 \nL 63.853987 76.71366 \nL 64.210385 73.981806 \nL 64.92318 70.161058 \nL 65.635976 69.150461 \nL 65.992373 69.012161 \nL 66.705169 66.279031 \nL 67.061567 68.492044 \nL 67.417964 65.885295 \nL 67.774362 67.807362 \nL 68.13076 66.005784 \nL 68.487157 66.115145 \nL 68.843555 65.771066 \nL 69.199953 67.855955 \nL 69.55635 64.024803 \nL 69.912748 63.681025 \nL 70.269146 66.175462 \nL 70.625544 63.848535 \nL 71.338339 65.586801 \nL 71.694737 64.84786 \nL 72.051134 62.922409 \nL 72.407532 66.732454 \nL 72.76393 65.001036 \nL 73.120327 66.943383 \nL 73.476725 64.530575 \nL 73.833123 65.570243 \nL 74.189521 62.756971 \nL 74.545918 65.616343 \nL 74.902316 62.683546 \nL 75.615111 63.355966 \nL 75.971509 61.450018 \nL 76.327907 62.796698 \nL 76.684304 61.07012 \nL 77.040702 63.03128 \nL 77.3971 63.551463 \nL 77.753498 62.129661 \nL 78.109895 61.340421 \nL 78.466293 61.919951 \nL 78.822691 61.418667 \nL 79.179088 61.655155 \nL 79.535486 59.816398 \nL 79.891884 62.889813 \nL 80.248281 62.657886 \nL 80.604679 60.691313 \nL 80.961077 57.43038 \nL 81.317475 58.29706 \nL 81.673872 61.801122 \nL 82.03027 58.155492 \nL 82.386668 62.162657 \nL 82.743065 57.930563 \nL 83.099463 58.44984 \nL 83.455861 58.251139 \nL 83.812258 56.950028 \nL 84.168656 58.96045 \nL 84.525054 56.93496 \nL 84.881452 57.471899 \nL 85.237849 61.275975 \nL 85.594247 58.294708 \nL 85.950645 60.162331 \nL 86.307042 55.699184 \nL 86.66344 56.543717 \nL 87.019838 56.582754 \nL 87.376235 56.257725 \nL 87.732633 55.443088 \nL 88.089031 55.967728 \nL 88.445429 53.684441 \nL 88.801826 57.223912 \nL 89.158224 56.39137 \nL 89.514622 55.028012 \nL 89.871019 56.781679 \nL 90.227417 52.536681 \nL 90.583815 54.456312 \nL 90.940213 54.323858 \nL 91.29661 52.63006 \nL 91.653008 54.370176 \nL 92.009406 52.672093 \nL 92.365803 54.855486 \nL 92.722201 54.361204 \nL 93.078599 52.137802 \nL 93.434996 54.197513 \nL 93.791394 50.973535 \nL 94.147792 50.480928 \nL 94.50419 51.79943 \nL 95.216985 49.947928 \nL 95.573383 52.451089 \nL 95.92978 50.31472 \nL 96.286178 48.945766 \nL 96.642576 48.19642 \nL 96.998973 49.501749 \nL 97.355371 49.284863 \nL 97.711769 49.496897 \nL 98.068167 48.66863 \nL 98.424564 48.618299 \nL 98.780962 48.197145 \nL 99.13736 48.23875 \nL 99.493757 49.855985 \nL 99.850155 47.651719 \nL 100.206553 49.505476 \nL 100.56295 48.270848 \nL 100.919348 49.396663 \nL 101.632144 47.332391 \nL 101.988541 47.878152 \nL 102.701337 49.622611 \nL 103.057734 45.977912 \nL 103.414132 47.581214 \nL 103.77053 47.770672 \nL 104.126927 47.204224 \nL 104.483325 47.199518 \nL 104.839723 46.306265 \nL 105.552518 46.914429 \nL 105.908916 46.054559 \nL 106.265314 47.134944 \nL 106.621711 47.574228 \nL 106.978109 50.398197 \nL 107.334507 45.345588 \nL 107.690904 46.556535 \nL 108.047302 45.201743 \nL 108.4037 45.599447 \nL 108.760098 47.110837 \nL 109.116495 46.998855 \nL 109.472893 45.149461 \nL 110.542086 55.067697 \nL 110.898484 50.825354 \nL 111.254881 44.571038 \nL 111.611279 46.442138 \nL 111.967677 45.090847 \nL 112.324075 45.606643 \nL 112.680472 45.59633 \nL 113.03687 44.320762 \nL 113.393268 47.163907 \nL 113.749665 45.094462 \nL 114.106063 44.445817 \nL 114.462461 44.720149 \nL 114.818858 45.538566 \nL 115.175256 45.062732 \nL 115.531654 44.87787 \nL 115.888052 45.157425 \nL 116.244449 44.539216 \nL 116.600847 44.558871 \nL 117.313642 48.97425 \nL 118.026438 45.15283 \nL 118.382835 45.570117 \nL 118.739233 46.959422 \nL 119.452029 44.40455 \nL 119.808426 45.870387 \nL 120.164824 43.092593 \nL 120.521222 44.716666 \nL 120.877619 45.764615 \nL 121.234017 44.094588 \nL 121.946812 42.942075 \nL 122.30321 43.158109 \nL 122.659608 43.186582 \nL 123.372403 42.293083 \nL 123.728801 44.531798 \nL 124.085199 43.619848 \nL 124.441596 41.682566 \nL 124.797994 42.507908 \nL 125.154392 44.87547 \nL 125.510789 42.034693 \nL 125.867187 44.350254 \nL 126.223585 41.979173 \nL 126.579983 42.045482 \nL 126.93638 41.880643 \nL 127.292778 41.589885 \nL 127.649176 40.450052 \nL 128.005573 41.244798 \nL 128.361971 40.805661 \nL 128.718369 41.034144 \nL 129.074766 42.255321 \nL 129.431164 40.157379 \nL 130.14396 42.572989 \nL 130.500357 41.910936 \nL 130.856755 40.293855 \nL 131.213153 41.056043 \nL 131.56955 40.366754 \nL 131.925948 40.103164 \nL 132.282346 40.662272 \nL 132.638743 42.488494 \nL 132.995141 39.485645 \nL 133.351539 39.51684 \nL 133.707937 40.265137 \nL 134.064334 40.044814 \nL 134.420732 39.12201 \nL 134.77713 41.219245 \nL 135.133527 40.174298 \nL 135.489925 38.0252 \nL 135.846323 41.636953 \nL 136.20272 38.644402 \nL 136.915516 37.532594 \nL 137.271914 40.328234 \nL 137.628311 39.216158 \nL 138.341107 39.833407 \nL 138.697504 39.604547 \nL 139.053902 37.632394 \nL 139.4103 39.919006 \nL 139.766697 37.792432 \nL 140.123095 40.140871 \nL 140.479493 38.079834 \nL 140.835891 41.501236 \nL 141.192288 38.474714 \nL 141.905084 40.954694 \nL 142.974277 39.6761 \nL 143.330674 37.497662 \nL 143.687072 39.756595 \nL 144.04347 40.178791 \nL 144.399868 39.853111 \nL 144.756265 38.178187 \nL 145.112663 40.377561 \nL 145.469061 37.482439 \nL 145.825458 40.931427 \nL 146.181856 38.054114 \nL 146.538254 39.510295 \nL 146.894652 38.272908 \nL 147.251049 41.087091 \nL 147.607447 38.887922 \nL 147.963845 40.201188 \nL 148.320242 40.943375 \nL 148.67664 38.868943 \nL 149.033038 38.023849 \nL 149.389435 35.245078 \nL 149.745833 40.485722 \nL 150.102231 37.313526 \nL 150.458629 37.375323 \nL 150.815026 39.352067 \nL 151.171424 39.080648 \nL 151.527822 38.117849 \nL 151.884219 39.726918 \nL 152.240617 36.976335 \nL 152.597015 40.711235 \nL 152.953412 38.040464 \nL 153.30981 39.37106 \nL 153.666208 37.626571 \nL 154.022606 38.86738 \nL 154.379003 37.040733 \nL 154.735401 38.321621 \nL 155.091799 37.234708 \nL 155.448196 37.670437 \nL 155.804594 37.78627 \nL 156.160992 34.49061 \nL 156.517389 37.776203 \nL 156.873787 37.45114 \nL 157.230185 34.990497 \nL 157.586583 36.701549 \nL 157.94298 34.347706 \nL 158.299378 34.61972 \nL 158.655776 37.358105 \nL 159.012173 36.28924 \nL 159.368571 37.76445 \nL 159.724969 37.188134 \nL 160.081366 37.708313 \nL 160.437764 34.752209 \nL 160.794162 38.059333 \nL 161.15056 34.91775 \nL 161.506957 35.087241 \nL 161.863355 34.537347 \nL 162.219753 38.048029 \nL 162.57615 34.86229 \nL 162.932548 34.583701 \nL 163.288946 38.099439 \nL 163.645343 34.295224 \nL 164.001741 35.131174 \nL 164.358139 34.619459 \nL 164.714537 35.333459 \nL 165.070934 36.580731 \nL 165.427332 33.644591 \nL 165.78373 35.082793 \nL 166.140127 35.933262 \nL 166.496525 34.944385 \nL 166.852923 32.687652 \nL 167.20932 34.965449 \nL 167.565718 33.66959 \nL 167.922116 37.963419 \nL 168.278514 32.648947 \nL 168.634911 35.388184 \nL 168.991309 36.168899 \nL 169.347707 32.964904 \nL 169.704104 35.01814 \nL 170.060502 35.593373 \nL 170.4169 34.838751 \nL 170.773297 33.435908 \nL 171.129695 32.926041 \nL 171.486093 34.634217 \nL 171.842491 37.005438 \nL 172.198888 33.263313 \nL 172.555286 35.230443 \nL 172.911684 35.055208 \nL 173.268081 32.414851 \nL 173.624479 32.231321 \nL 174.337274 33.921888 \nL 174.693672 31.122749 \nL 175.406468 33.98836 \nL 175.762865 34.321122 \nL 176.119263 32.845321 \nL 176.475661 30.491422 \nL 176.832058 32.980065 \nL 177.188456 34.463428 \nL 177.901251 31.658292 \nL 178.257649 32.767573 \nL 178.614047 31.107789 \nL 178.970445 33.108107 \nL 179.326842 34.014051 \nL 179.68324 34.155682 \nL 180.039638 34.876183 \nL 180.396035 34.316608 \nL 180.752433 30.89051 \nL 181.108831 31.709664 \nL 181.465228 31.180422 \nL 182.178024 35.215617 \nL 182.534422 35.056805 \nL 182.890819 32.793783 \nL 183.603615 38.830859 \nL 184.31641 34.808257 \nL 184.672808 35.031897 \nL 185.029205 36.613334 \nL 185.385603 35.644353 \nL 185.742001 36.106355 \nL 186.098399 34.933471 \nL 186.454796 36.528309 \nL 186.811194 36.185449 \nL 187.167592 34.107418 \nL 187.523989 33.855721 \nL 187.880387 33.044223 \nL 188.236785 33.673453 \nL 188.593182 35.967917 \nL 189.305978 33.960656 \nL 189.662376 36.230369 \nL 190.018773 35.044978 \nL 190.375171 35.681611 \nL 190.731569 35.293888 \nL 191.087966 35.1913 \nL 191.444364 33.512719 \nL 191.800762 35.505939 \nL 192.157159 32.779416 \nL 192.513557 34.674583 \nL 192.869955 32.296352 \nL 193.226353 34.718996 \nL 193.58275 31.891481 \nL 193.939148 36.039557 \nL 194.295546 34.087838 \nL 194.651943 33.525795 \nL 195.008341 35.372831 \nL 195.364739 35.242072 \nL 195.721136 34.197335 \nL 196.077534 36.613639 \nL 196.433932 34.279061 \nL 196.79033 34.462424 \nL 197.146727 33.029452 \nL 197.503125 34.021119 \nL 197.859523 33.961615 \nL 198.21592 36.998202 \nL 198.572318 34.083098 \nL 199.285114 36.148654 \nL 199.641511 35.577728 \nL 199.997909 31.462039 \nL 200.354307 31.161902 \nL 200.710704 34.553056 \nL 201.067102 30.716334 \nL 201.4235 29.812447 \nL 201.779897 31.064348 \nL 202.492693 35.044786 \nL 202.849091 30.727918 \nL 203.205488 32.440256 \nL 203.561886 28.615558 \nL 203.918284 33.714024 \nL 204.274681 32.3461 \nL 204.631079 33.329278 \nL 204.987477 30.945226 \nL 205.343874 32.1515 \nL 205.700272 31.43675 \nL 206.05667 30.3009 \nL 206.413068 32.45206 \nL 206.769465 30.968147 \nL 207.125863 30.726865 \nL 207.482261 29.568277 \nL 207.838658 26.184941 \nL 208.195056 29.057184 \nL 208.551454 33.14542 \nL 208.907851 29.243802 \nL 209.264249 30.771689 \nL 209.620647 28.781885 \nL 209.977045 30.887987 \nL 210.333442 30.329541 \nL 210.68984 28.309711 \nL 211.046238 31.882731 \nL 211.402635 29.736426 \nL 211.759033 28.619871 \nL 212.115431 30.939401 \nL 212.471828 29.149443 \nL 212.828226 30.778107 \nL 213.184624 31.517619 \nL 213.541022 28.858009 \nL 213.897419 28.702794 \nL 214.253817 31.18599 \nL 214.966612 28.966036 \nL 215.32301 35.021221 \nL 215.679408 35.932471 \nL 216.035805 33.934339 \nL 216.392203 33.165998 \nL 216.748601 27.721463 \nL 217.104999 32.590554 \nL 217.461396 34.691029 \nL 217.817794 34.817465 \nL 218.174192 34.672995 \nL 218.530589 35.923284 \nL 218.886987 33.013975 \nL 219.243385 33.422109 \nL 219.599782 33.291269 \nL 219.95618 32.211998 \nL 220.312578 30.415031 \nL 220.668976 29.50749 \nL 221.025373 31.74275 \nL 221.381771 29.948428 \nL 221.738169 29.756704 \nL 222.094566 31.623571 \nL 222.807362 29.067637 \nL 223.163759 29.498964 \nL 223.520157 30.812227 \nL 223.876555 27.516731 \nL 224.232953 30.537926 \nL 224.58935 37.69214 \nL 224.945748 31.529462 \nL 225.302146 27.919544 \nL 225.658543 31.951478 \nL 226.014941 32.528352 \nL 226.371339 32.668476 \nL 227.084134 32.747803 \nL 227.440532 30.229656 \nL 227.79693 33.221344 \nL 228.153327 30.871736 \nL 228.509725 32.509968 \nL 228.866123 33.212136 \nL 229.22252 31.494724 \nL 229.578918 32.250108 \nL 229.935316 31.716914 \nL 230.291713 29.53007 \nL 230.648111 28.12089 \nL 231.004509 27.734657 \nL 231.360907 29.281737 \nL 231.717304 29.401208 \nL 232.073702 29.359748 \nL 232.4301 27.471949 \nL 232.786497 31.497194 \nL 233.499293 27.720079 \nL 233.85569 30.077635 \nL 234.212088 26.704019 \nL 234.568486 27.341717 \nL 234.924884 30.504506 \nL 235.281281 28.815014 \nL 235.637679 30.323003 \nL 235.994077 29.057497 \nL 236.350474 28.294798 \nL 236.706872 24.814295 \nL 237.06327 27.212802 \nL 237.419667 28.482528 \nL 237.776065 25.620599 \nL 238.132463 26.731853 \nL 238.488861 27.461009 \nL 238.845258 26.618675 \nL 239.201656 27.132386 \nL 239.558054 24.522585 \nL 239.914451 29.095591 \nL 240.270849 24.980053 \nL 240.627247 30.97809 \nL 240.983644 25.959594 \nL 241.340042 27.960562 \nL 241.69644 26.228728 \nL 242.409235 30.152808 \nL 242.765633 27.140594 \nL 243.122031 29.526694 \nL 243.478428 26.182772 \nL 243.834826 28.408726 \nL 244.191224 25.949096 \nL 244.547621 25.022149 \nL 244.904019 26.613003 \nL 245.260417 26.351173 \nL 245.616815 27.103103 \nL 245.973212 27.53351 \nL 246.32961 25.081877 \nL 246.686008 25.899935 \nL 247.042405 30.626194 \nL 247.398803 24.9314 \nL 247.755201 25.245116 \nL 248.111598 24.127674 \nL 248.467996 25.852319 \nL 248.824394 26.109568 \nL 249.180792 28.404571 \nL 249.537189 27.247116 \nL 249.893587 27.260338 \nL 250.249985 24.288165 \nL 250.606382 30.949366 \nL 250.96278 28.182329 \nL 251.319178 29.265475 \nL 251.675576 25.898052 \nL 252.031973 29.065307 \nL 252.388371 29.344116 \nL 252.744769 26.687664 \nL 253.101166 27.56855 \nL 253.457564 26.794223 \nL 253.813962 30.624163 \nL 254.170359 26.740872 \nL 254.526757 25.540759 \nL 254.883155 23.240292 \nL 255.239553 26.340402 \nL 255.59595 24.747148 \nL 255.952348 25.590847 \nL 256.308746 25.772946 \nL 256.665143 24.949063 \nL 257.021541 26.265533 \nL 257.377939 25.112969 \nL 257.734336 25.852953 \nL 258.090734 25.57769 \nL 258.447132 22.00902 \nL 258.80353 23.206605 \nL 259.159927 25.593834 \nL 259.516325 23.662405 \nL 259.872723 23.9714 \nL 260.22912 22.684553 \nL 260.585518 24.741937 \nL 260.941916 24.151547 \nL 261.298313 25.294889 \nL 261.654711 25.719002 \nL 262.011109 24.380679 \nL 262.367507 24.883032 \nL 262.723904 24.394031 \nL 263.080302 24.249828 \nL 263.4367 25.963325 \nL 263.793097 28.508129 \nL 264.149495 27.778922 \nL 264.505893 28.505846 \nL 265.218688 24.984071 \nL 265.575086 24.499946 \nL 265.931484 23.802544 \nL 266.287881 24.973678 \nL 266.644279 24.636054 \nL 267.000677 21.137153 \nL 267.357074 23.903996 \nL 267.713472 25.474641 \nL 268.06987 25.73407 \nL 268.426267 24.830155 \nL 268.782665 23.222628 \nL 269.139063 23.807602 \nL 269.495461 23.442345 \nL 269.851858 25.093181 \nL 270.208256 22.894649 \nL 270.921051 24.527067 \nL 271.277449 22.546002 \nL 271.633847 23.964202 \nL 271.990244 24.033381 \nL 272.346642 26.158214 \nL 272.70304 24.187679 \nL 273.059438 20.0713 \nL 273.415835 21.259679 \nL 273.772233 24.487601 \nL 274.485028 23.44181 \nL 274.841426 21.591697 \nL 275.197824 21.825789 \nL 275.554221 23.125679 \nL 275.910619 21.747779 \nL 276.267017 23.468049 \nL 276.623415 21.63194 \nL 276.979812 22.589974 \nL 277.33621 19.796109 \nL 277.692608 19.527382 \nL 278.049005 20.140965 \nL 278.405403 20.254041 \nL 278.761801 20.508551 \nL 279.118198 20.282405 \nL 279.474596 19.910896 \nL 279.830994 22.309909 \nL 280.187392 22.033329 \nL 280.543789 22.203406 \nL 280.900187 21.532268 \nL 281.256585 21.634425 \nL 281.612982 23.179494 \nL 282.325778 20.703086 \nL 282.682175 20.60848 \nL 283.038573 21.232477 \nL 283.394971 22.918907 \nL 283.751369 20.05851 \nL 284.107766 18.649041 \nL 284.464164 21.715893 \nL 284.820562 23.048913 \nL 285.176959 20.703274 \nL 285.533357 20.143227 \nL 285.889755 22.014809 \nL 286.246152 24.736394 \nL 286.60255 22.634843 \nL 286.958948 25.225115 \nL 287.315346 23.726417 \nL 287.671743 23.406691 \nL 288.028141 21.403947 \nL 288.740936 18.955039 \nL 289.097334 19.70953 \nL 289.453732 22.340195 \nL 289.810129 19.817671 \nL 290.166527 18.261502 \nL 290.522925 20.78106 \nL 290.879323 21.973868 \nL 291.23572 21.297903 \nL 291.592118 21.149909 \nL 291.948516 21.321557 \nL 292.304913 23.722949 \nL 292.661311 17.477608 \nL 293.730504 24.496251 \nL 294.086902 24.897042 \nL 294.4433 22.725292 \nL 294.799697 22.179159 \nL 295.156095 21.096764 \nL 295.512493 24.359576 \nL 295.86889 21.101427 \nL 296.225288 23.034337 \nL 296.581686 21.910217 \nL 296.938083 23.65494 \nL 297.650879 20.536068 \nL 298.363674 24.010273 \nL 298.720072 21.797935 \nL 299.07647 20.953455 \nL 299.432867 23.725151 \nL 299.789265 22.199098 \nL 300.145663 19.871886 \nL 300.50206 19.475057 \nL 301.214856 20.107306 \nL 301.571254 18.909803 \nL 302.284049 22.122952 \nL 302.640447 19.420834 \nL 302.996844 18.229072 \nL 303.70964 20.391813 \nL 304.066037 20.867336 \nL 304.422435 23.489641 \nL 304.778833 20.669735 \nL 305.135231 21.763164 \nL 305.491628 22.164366 \nL 305.848026 23.260898 \nL 306.204424 22.911383 \nL 306.560821 22.878812 \nL 306.917219 19.833012 \nL 307.273617 24.249107 \nL 307.630015 22.861354 \nL 307.986412 19.701563 \nL 308.34281 22.141699 \nL 308.699208 22.400371 \nL 309.055605 21.95223 \nL 309.412003 24.439281 \nL 309.768401 21.733014 \nL 310.124798 20.849384 \nL 310.481196 22.037668 \nL 310.837594 24.348105 \nL 311.193992 21.507489 \nL 311.550389 21.607992 \nL 311.906787 21.257795 \nL 312.619582 23.688206 \nL 312.97598 20.809848 \nL 313.332378 21.247285 \nL 314.045173 19.076456 \nL 314.401571 19.109905 \nL 314.757969 20.454763 \nL 315.114366 18.748595 \nL 315.470764 21.43208 \nL 315.827162 18.78842 \nL 316.183559 18.340241 \nL 316.539957 21.107178 \nL 316.896355 21.494372 \nL 317.252752 22.403391 \nL 317.60915 24.83418 \nL 317.965548 18.410687 \nL 318.321946 22.112211 \nL 318.678343 20.713575 \nL 319.034741 20.945302 \nL 319.391139 22.74515 \nL 319.747536 22.172582 \nL 320.103934 18.256416 \nL 320.460332 22.0412 \nL 320.816729 22.726255 \nL 321.173127 21.637256 \nL 321.529525 23.551836 \nL 321.885923 20.557845 \nL 322.24232 21.201642 \nL 322.598718 19.181573 \nL 322.955116 21.958416 \nL 323.311513 20.045437 \nL 323.667911 19.842222 \nL 324.024309 18.669794 \nL 324.380706 21.131716 \nL 324.737104 20.113595 \nL 325.093502 18.050079 \nL 325.4499 19.197443 \nL 325.806297 23.953015 \nL 326.162695 18.752396 \nL 326.87549 21.446116 \nL 327.231888 22.024698 \nL 327.588286 23.678709 \nL 327.944683 21.397468 \nL 328.301081 23.288717 \nL 328.657479 21.424747 \nL 329.013877 22.545932 \nL 329.370274 20.813548 \nL 329.726672 23.035343 \nL 330.08307 19.313234 \nL 330.439467 25.290931 \nL 330.795865 20.315807 \nL 331.152263 21.071214 \nL 331.50866 21.261786 \nL 331.865058 24.63378 \nL 332.221456 20.848832 \nL 332.577854 22.882789 \nL 332.934251 19.371976 \nL 333.290649 25.770653 \nL 333.647047 26.812954 \nL 334.359842 23.829335 \nL 334.71624 18.985857 \nL 335.072637 21.679622 \nL 335.429035 19.178029 \nL 335.785433 22.124262 \nL 336.141831 21.54119 \nL 336.498228 23.850411 \nL 336.854626 18.412038 \nL 337.211024 24.608497 \nL 337.567421 22.810347 \nL 337.923819 22.821352 \nL 338.636614 17.495234 \nL 338.993012 20.158516 \nL 339.34941 21.219945 \nL 339.705808 20.877381 \nL 340.062205 18.849325 \nL 340.418603 22.744372 \nL 340.775001 21.651594 \nL 341.131398 20.141749 \nL 341.487796 19.891774 \nL 341.844194 21.048477 \nL 342.200591 21.725443 \nL 342.556989 23.898327 \nL 342.913387 22.594357 \nL 343.626182 19.039188 \nL 343.98258 21.232388 \nL 344.338978 22.017096 \nL 344.695375 17.973927 \nL 345.051773 17.794812 \nL 345.408171 17.083636 \nL 345.764568 17.420953 \nL 346.120966 19.390079 \nL 346.477364 20.506122 \nL 346.833762 20.460506 \nL 347.190159 17.315864 \nL 347.546557 18.29599 \nL 347.902955 17.654976 \nL 348.259352 20.423123 \nL 348.61575 20.940841 \nL 348.972148 18.824135 \nL 349.328545 19.097685 \nL 349.684943 20.421303 \nL 349.684943 20.421303 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#p3e0b231bfb)\" d=\"M 45.321307 214.756364 \nL 45.677705 197.835293 \nL 46.034102 195.983799 \nL 47.103295 172.307965 \nL 47.459693 167.605445 \nL 47.816091 157.341238 \nL 48.172488 150.684875 \nL 48.528886 146.546802 \nL 49.241682 133.817207 \nL 49.598079 130.629052 \nL 49.954477 131.957541 \nL 50.310875 119.389605 \nL 50.667272 120.668528 \nL 51.02367 113.139665 \nL 51.380068 111.409554 \nL 51.736465 110.40634 \nL 52.092863 110.371803 \nL 52.805659 105.162357 \nL 53.162056 105.862177 \nL 53.518454 102.650244 \nL 53.874852 101.138477 \nL 54.231249 104.010506 \nL 54.587647 99.278738 \nL 54.944045 97.84988 \nL 55.300442 98.284823 \nL 55.65684 96.794274 \nL 56.013238 90.97495 \nL 56.369636 91.929854 \nL 56.726033 89.580621 \nL 57.082431 86.088949 \nL 57.438829 85.980155 \nL 57.795226 82.485611 \nL 58.151624 86.035437 \nL 58.508022 84.312134 \nL 58.864419 81.600987 \nL 59.220817 81.679658 \nL 59.577215 82.681176 \nL 60.29001 76.865634 \nL 60.646408 77.735346 \nL 61.002806 80.13636 \nL 61.359203 78.995203 \nL 61.715601 78.547618 \nL 62.071999 84.729097 \nL 62.784794 84.777285 \nL 63.141192 76.891049 \nL 63.49759 78.497116 \nL 63.853987 79.316875 \nL 64.92318 71.85794 \nL 65.279578 71.752562 \nL 65.992373 72.613142 \nL 66.348771 70.329053 \nL 66.705169 68.971097 \nL 67.061567 70.64804 \nL 67.417964 69.341198 \nL 67.774362 70.780652 \nL 68.13076 68.794177 \nL 68.487157 68.153198 \nL 68.843555 69.472708 \nL 69.199953 71.628903 \nL 69.55635 67.620973 \nL 69.912748 66.991942 \nL 70.269146 69.91554 \nL 70.625544 66.007005 \nL 70.981941 67.093145 \nL 71.338339 70.296366 \nL 71.694737 68.169437 \nL 72.051134 67.764203 \nL 72.407532 69.981105 \nL 72.76393 67.395057 \nL 73.120327 69.536911 \nL 73.476725 66.832251 \nL 73.833123 67.283623 \nL 74.189521 64.95396 \nL 74.545918 68.130134 \nL 74.902316 65.814634 \nL 75.258714 65.484965 \nL 75.615111 66.519076 \nL 75.971509 64.538261 \nL 76.327907 65.611138 \nL 76.684304 64.003305 \nL 77.040702 66.009888 \nL 77.3971 66.418601 \nL 78.466293 63.913507 \nL 78.822691 62.442218 \nL 79.179088 64.210237 \nL 79.535486 61.474105 \nL 79.891884 65.345918 \nL 80.604679 62.147677 \nL 80.961077 59.868517 \nL 81.673872 61.769166 \nL 82.03027 59.808564 \nL 82.386668 62.537281 \nL 82.743065 59.495867 \nL 83.099463 59.192657 \nL 83.455861 59.546227 \nL 83.812258 58.141318 \nL 84.168656 61.559947 \nL 84.525054 57.617082 \nL 84.881452 58.762976 \nL 85.237849 63.022846 \nL 85.594247 61.082511 \nL 85.950645 62.142785 \nL 86.307042 56.614035 \nL 86.66344 58.900896 \nL 87.732633 56.749465 \nL 88.089031 56.970315 \nL 88.445429 55.073659 \nL 88.801826 58.484173 \nL 89.158224 58.126395 \nL 89.514622 57.992866 \nL 89.871019 59.707978 \nL 90.227417 53.921171 \nL 90.583815 56.747344 \nL 90.940213 57.629976 \nL 91.29661 55.661133 \nL 91.653008 57.445424 \nL 92.009406 55.487644 \nL 92.365803 57.838982 \nL 92.722201 55.651566 \nL 93.078599 54.83482 \nL 93.434996 56.943773 \nL 93.791394 52.66745 \nL 94.50419 54.92008 \nL 94.860587 53.676458 \nL 95.573383 54.631791 \nL 95.92978 53.076831 \nL 96.642576 52.536158 \nL 96.998973 53.98766 \nL 97.355371 53.21271 \nL 97.711769 53.597757 \nL 98.068167 51.256943 \nL 98.424564 52.422125 \nL 98.780962 51.182742 \nL 99.493757 53.820213 \nL 99.850155 52.133733 \nL 100.206553 53.929531 \nL 100.56295 52.56565 \nL 100.919348 52.310445 \nL 101.275746 53.431909 \nL 101.632144 52.249207 \nL 101.988541 50.242313 \nL 102.344939 53.036298 \nL 102.701337 52.885553 \nL 103.057734 50.465837 \nL 103.414132 51.694722 \nL 103.77053 52.352839 \nL 104.126927 50.927305 \nL 104.483325 51.707886 \nL 104.839723 50.413428 \nL 105.196121 51.982931 \nL 105.552518 51.935818 \nL 105.908916 49.523585 \nL 106.265314 52.704128 \nL 106.621711 52.795151 \nL 106.978109 56.567797 \nL 107.334507 50.102733 \nL 107.690904 50.807449 \nL 108.047302 50.697519 \nL 108.760098 52.091674 \nL 109.116495 52.399586 \nL 109.472893 50.534976 \nL 110.542086 59.78145 \nL 111.254881 49.406988 \nL 111.611279 50.485347 \nL 111.967677 50.27255 \nL 112.324075 49.508859 \nL 112.680472 51.408421 \nL 113.03687 49.892705 \nL 113.393268 52.332611 \nL 113.749665 49.872607 \nL 114.106063 48.838961 \nL 114.462461 49.101583 \nL 114.818858 49.8116 \nL 115.175256 49.458953 \nL 115.531654 50.005451 \nL 115.888052 49.871018 \nL 116.244449 49.539838 \nL 116.600847 50.07806 \nL 116.957245 52.143485 \nL 117.313642 53.403792 \nL 117.67004 53.110402 \nL 118.026438 51.632448 \nL 118.382835 51.112025 \nL 118.739233 53.49125 \nL 119.452029 49.890602 \nL 119.808426 52.191012 \nL 120.164824 47.991064 \nL 120.521222 51.107151 \nL 120.877619 52.374801 \nL 121.234017 51.772129 \nL 121.590415 50.61708 \nL 122.30321 49.523691 \nL 122.659608 49.73206 \nL 123.016006 48.896519 \nL 123.372403 50.912241 \nL 123.728801 51.247708 \nL 124.085199 50.198002 \nL 124.441596 49.573471 \nL 124.797994 48.454216 \nL 125.154392 51.644128 \nL 125.510789 48.470491 \nL 125.867187 50.77628 \nL 126.223585 51.563926 \nL 126.579983 49.878617 \nL 126.93638 49.549526 \nL 127.292778 49.380885 \nL 127.649176 47.835414 \nL 128.005573 48.949454 \nL 128.361971 48.573225 \nL 128.718369 47.369005 \nL 129.074766 49.481821 \nL 129.431164 47.443326 \nL 129.787562 49.521645 \nL 130.14396 49.287228 \nL 130.500357 50.28683 \nL 130.856755 48.339477 \nL 131.213153 48.815436 \nL 131.56955 48.165689 \nL 131.925948 46.355034 \nL 132.282346 46.82645 \nL 132.638743 49.820692 \nL 132.995141 47.347124 \nL 133.351539 49.503772 \nL 133.707937 46.58894 \nL 134.064334 47.857463 \nL 134.420732 47.97566 \nL 134.77713 49.354358 \nL 135.133527 48.420523 \nL 135.489925 47.845117 \nL 135.846323 48.620277 \nL 136.20272 47.726248 \nL 136.559118 48.01914 \nL 136.915516 46.730375 \nL 137.271914 48.185699 \nL 137.628311 47.213644 \nL 137.984709 47.556092 \nL 138.341107 47.242049 \nL 138.697504 48.477507 \nL 139.4103 47.034727 \nL 139.766697 47.723713 \nL 140.123095 47.744795 \nL 140.479493 46.7413 \nL 140.835891 49.462282 \nL 141.192288 46.122185 \nL 141.548686 47.977816 \nL 141.905084 47.85005 \nL 142.261481 48.946816 \nL 142.617879 47.802002 \nL 142.974277 48.108641 \nL 143.330674 46.212004 \nL 143.687072 48.092565 \nL 144.04347 48.394056 \nL 144.399868 47.335415 \nL 144.756265 47.761544 \nL 145.112663 47.45725 \nL 145.469061 45.554167 \nL 145.825458 48.546911 \nL 146.181856 46.974681 \nL 146.538254 47.729852 \nL 146.894652 45.836568 \nL 147.251049 49.448887 \nL 147.607447 47.751224 \nL 147.963845 48.007043 \nL 148.320242 48.637744 \nL 148.67664 47.586978 \nL 149.033038 45.349331 \nL 149.389435 45.512306 \nL 149.745833 48.810881 \nL 150.102231 46.395646 \nL 150.458629 48.738289 \nL 150.815026 47.109324 \nL 151.171424 46.855142 \nL 151.527822 48.652322 \nL 151.884219 48.098918 \nL 152.240617 47.048962 \nL 152.597015 48.368945 \nL 152.953412 46.480953 \nL 153.30981 48.008174 \nL 154.022606 46.747058 \nL 154.379003 46.979478 \nL 154.735401 47.037581 \nL 155.091799 45.59115 \nL 155.448196 46.627381 \nL 156.160992 44.078318 \nL 156.517389 45.336392 \nL 157.230185 45.585262 \nL 157.586583 45.113402 \nL 157.94298 44.951049 \nL 158.299378 45.814713 \nL 158.655776 47.173267 \nL 159.012173 47.480341 \nL 159.368571 46.096996 \nL 159.724969 46.840546 \nL 160.081366 46.986584 \nL 160.437764 45.723725 \nL 160.794162 46.973778 \nL 161.15056 45.436885 \nL 161.506957 46.054509 \nL 161.863355 45.195719 \nL 162.219753 47.22925 \nL 162.57615 46.725412 \nL 162.932548 44.477857 \nL 163.288946 47.1312 \nL 163.645343 45.248581 \nL 164.001741 45.814761 \nL 164.358139 44.623873 \nL 164.714537 45.656751 \nL 165.070934 46.266388 \nL 165.427332 44.980557 \nL 165.78373 46.603556 \nL 166.140127 45.356789 \nL 166.496525 45.724276 \nL 166.852923 44.40342 \nL 167.20932 46.510357 \nL 167.565718 44.385177 \nL 167.922116 45.904123 \nL 168.278514 44.465108 \nL 168.634911 46.576258 \nL 168.991309 45.031842 \nL 169.347707 44.668656 \nL 169.704104 45.459121 \nL 170.060502 46.891225 \nL 171.129695 43.819803 \nL 171.486093 46.469405 \nL 171.842491 47.338451 \nL 172.198888 44.960687 \nL 172.555286 46.941354 \nL 172.911684 44.9986 \nL 173.268081 45.958086 \nL 173.624479 44.098478 \nL 173.980877 45.247221 \nL 174.337274 47.04863 \nL 174.693672 44.145783 \nL 175.05007 46.563291 \nL 175.406468 45.988179 \nL 175.762865 45.952892 \nL 176.119263 44.438336 \nL 176.475661 43.909423 \nL 177.188456 45.280772 \nL 177.544854 46.365979 \nL 177.901251 44.354424 \nL 178.257649 44.989612 \nL 178.614047 44.743743 \nL 178.970445 44.004864 \nL 179.326842 47.06828 \nL 180.039638 45.656386 \nL 180.396035 45.888195 \nL 180.752433 44.532647 \nL 181.108831 44.609443 \nL 181.465228 43.712235 \nL 181.821626 43.938426 \nL 182.178024 46.872784 \nL 182.534422 44.662615 \nL 182.890819 45.055185 \nL 183.247217 47.756952 \nL 183.603615 48.006569 \nL 183.960012 49.348837 \nL 184.31641 47.357092 \nL 185.029205 46.588202 \nL 185.385603 47.737016 \nL 185.742001 45.971899 \nL 186.098399 45.834661 \nL 186.811194 45.945738 \nL 187.167592 44.175604 \nL 187.523989 44.033775 \nL 187.880387 44.027137 \nL 188.236785 43.452764 \nL 188.94958 45.283578 \nL 189.305978 44.956173 \nL 189.662376 45.386724 \nL 190.018773 44.001708 \nL 190.375171 44.589724 \nL 190.731569 43.339374 \nL 191.087966 43.966919 \nL 191.444364 42.289643 \nL 191.800762 44.998388 \nL 192.157159 43.529953 \nL 192.513557 44.558739 \nL 192.869955 43.698905 \nL 193.226353 44.970853 \nL 193.58275 43.765049 \nL 193.939148 44.867787 \nL 194.295546 44.286258 \nL 194.651943 44.195572 \nL 195.008341 44.583699 \nL 195.364739 44.710137 \nL 195.721136 44.494933 \nL 196.077534 45.476002 \nL 196.433932 43.472944 \nL 196.79033 44.598386 \nL 197.146727 44.426959 \nL 197.503125 44.86573 \nL 197.859523 44.613391 \nL 198.21592 45.670967 \nL 198.928716 43.725848 \nL 199.285114 46.09244 \nL 200.354307 42.777502 \nL 200.710704 45.836334 \nL 201.067102 42.471008 \nL 201.4235 44.037597 \nL 201.779897 43.899469 \nL 202.136295 45.10528 \nL 202.492693 44.498034 \nL 202.849091 42.810403 \nL 203.205488 43.203023 \nL 203.561886 42.900528 \nL 203.918284 43.491557 \nL 204.274681 43.342804 \nL 204.631079 43.647193 \nL 204.987477 41.416174 \nL 205.343874 43.173601 \nL 205.700272 42.354196 \nL 206.05667 42.851311 \nL 206.413068 45.728142 \nL 206.769465 42.159714 \nL 207.125863 43.010351 \nL 207.838658 41.827198 \nL 208.195056 41.711227 \nL 208.551454 45.396666 \nL 208.907851 43.73887 \nL 209.264249 43.841768 \nL 209.620647 42.4753 \nL 209.977045 43.140712 \nL 210.333442 44.340426 \nL 210.68984 41.900699 \nL 211.046238 45.232723 \nL 211.402635 42.894566 \nL 211.759033 43.649361 \nL 212.115431 43.648386 \nL 212.471828 42.188665 \nL 212.828226 44.411552 \nL 213.184624 43.35596 \nL 213.541022 41.825928 \nL 213.897419 41.449253 \nL 214.253817 44.135526 \nL 214.966612 41.388399 \nL 215.32301 49.128871 \nL 215.679408 49.358981 \nL 216.035805 44.280862 \nL 216.392203 45.125751 \nL 216.748601 42.753936 \nL 217.104999 43.834181 \nL 217.461396 48.627849 \nL 217.817794 48.884441 \nL 218.530589 50.409407 \nL 218.886987 45.771533 \nL 219.243385 45.949626 \nL 219.599782 46.6049 \nL 219.95618 45.009166 \nL 220.312578 44.54625 \nL 220.668976 43.642271 \nL 221.025373 44.265843 \nL 221.381771 44.560228 \nL 222.094566 45.710584 \nL 222.450964 43.503076 \nL 222.807362 43.542179 \nL 223.520157 45.342906 \nL 223.876555 43.097448 \nL 224.232953 44.79024 \nL 224.58935 50.978751 \nL 225.302146 41.124116 \nL 225.658543 45.055934 \nL 226.014941 47.045568 \nL 226.727736 45.034263 \nL 227.084134 45.010828 \nL 227.440532 44.386741 \nL 227.79693 46.208234 \nL 228.153327 44.221519 \nL 228.509725 44.003848 \nL 228.866123 45.483272 \nL 229.22252 44.749719 \nL 229.578918 46.039341 \nL 229.935316 44.035908 \nL 230.291713 43.90874 \nL 230.648111 43.083585 \nL 231.004509 43.431813 \nL 231.360907 43.362947 \nL 231.717304 42.310555 \nL 232.073702 43.917981 \nL 232.4301 42.333602 \nL 232.786497 44.711798 \nL 233.142895 43.715227 \nL 233.499293 42.160435 \nL 233.85569 44.261624 \nL 234.212088 41.722519 \nL 234.568486 41.827912 \nL 234.924884 43.959055 \nL 235.281281 41.040322 \nL 235.637679 43.372807 \nL 235.994077 42.072017 \nL 236.350474 42.661853 \nL 236.706872 41.465284 \nL 237.06327 41.543885 \nL 237.419667 43.923065 \nL 237.776065 42.304072 \nL 238.845258 42.222009 \nL 239.201656 42.072983 \nL 239.558054 41.597264 \nL 239.914451 43.570182 \nL 240.270849 41.072885 \nL 240.627247 42.208572 \nL 240.983644 41.405429 \nL 241.340042 42.520954 \nL 241.69644 40.599024 \nL 242.052838 41.603195 \nL 242.409235 42.96993 \nL 242.765633 41.854403 \nL 243.122031 43.352788 \nL 243.478428 43.543924 \nL 243.834826 42.693997 \nL 244.191224 40.804881 \nL 244.547621 42.00718 \nL 244.904019 42.295313 \nL 245.260417 41.67635 \nL 245.616815 41.98906 \nL 245.973212 41.630291 \nL 246.32961 39.7533 \nL 247.042405 43.008556 \nL 247.398803 41.820621 \nL 247.755201 42.66361 \nL 248.111598 42.771326 \nL 248.467996 43.208698 \nL 248.824394 42.332877 \nL 249.180792 42.456412 \nL 249.537189 43.765879 \nL 249.893587 43.843444 \nL 250.249985 42.542987 \nL 250.606382 44.246717 \nL 250.96278 43.869508 \nL 251.319178 45.18284 \nL 251.675576 42.502157 \nL 252.031973 44.757085 \nL 252.388371 43.60831 \nL 252.744769 43.225429 \nL 253.457564 41.776798 \nL 253.813962 44.441189 \nL 254.170359 44.346899 \nL 254.526757 42.972906 \nL 254.883155 42.080951 \nL 255.239553 43.289357 \nL 255.952348 41.78052 \nL 256.308746 42.906048 \nL 256.665143 42.351669 \nL 257.021541 42.547471 \nL 257.734336 40.817471 \nL 258.090734 41.062583 \nL 258.447132 40.807153 \nL 258.80353 40.816895 \nL 259.159927 41.873368 \nL 259.516325 40.45001 \nL 259.872723 40.132015 \nL 260.22912 41.728053 \nL 260.585518 40.50899 \nL 260.941916 41.882022 \nL 261.298313 41.302341 \nL 261.654711 41.908706 \nL 262.011109 40.522978 \nL 262.367507 41.534833 \nL 262.723904 40.801272 \nL 263.080302 41.553449 \nL 263.793097 44.67754 \nL 264.149495 41.572248 \nL 264.505893 42.253423 \nL 264.86229 41.800454 \nL 265.218688 40.454846 \nL 265.575086 41.244077 \nL 265.931484 41.159268 \nL 266.287881 40.334298 \nL 266.644279 41.61415 \nL 267.000677 39.047262 \nL 267.357074 40.874425 \nL 267.713472 40.361735 \nL 268.06987 41.630297 \nL 268.426267 41.317058 \nL 268.782665 40.708451 \nL 269.139063 43.135323 \nL 269.495461 40.027328 \nL 269.851858 42.733578 \nL 270.208256 41.245776 \nL 270.564654 42.09648 \nL 270.921051 41.965864 \nL 271.277449 40.906228 \nL 271.633847 39.285227 \nL 271.990244 42.024399 \nL 272.346642 41.825162 \nL 273.059438 40.02451 \nL 273.415835 41.442586 \nL 273.772233 41.210971 \nL 274.128631 40.723865 \nL 274.485028 39.33755 \nL 274.841426 39.533361 \nL 275.197824 39.981506 \nL 275.554221 39.53203 \nL 275.910619 39.84569 \nL 276.623415 41.915812 \nL 276.979812 40.219732 \nL 277.33621 39.315516 \nL 277.692608 39.63935 \nL 278.049005 39.434362 \nL 278.405403 38.353931 \nL 278.761801 40.340194 \nL 279.118198 39.6677 \nL 279.474596 39.788521 \nL 279.830994 39.654283 \nL 280.187392 41.453395 \nL 280.543789 39.374132 \nL 280.900187 40.486966 \nL 281.256585 39.821423 \nL 281.612982 40.276285 \nL 281.96938 39.282612 \nL 282.325778 41.3356 \nL 282.682175 40.70797 \nL 283.038573 39.507273 \nL 283.394971 40.792697 \nL 283.751369 38.634295 \nL 284.107766 39.877142 \nL 284.464164 43.197251 \nL 284.820562 40.247161 \nL 285.176959 38.966967 \nL 285.533357 39.453987 \nL 285.889755 40.825076 \nL 286.246152 40.473602 \nL 286.60255 39.397662 \nL 286.958948 41.172302 \nL 287.315346 40.411349 \nL 287.671743 40.217051 \nL 288.028141 39.434766 \nL 288.384539 39.318893 \nL 288.740936 39.372765 \nL 289.097334 38.271072 \nL 289.453732 39.28606 \nL 289.810129 39.481836 \nL 290.166527 38.733165 \nL 290.522925 40.98372 \nL 290.879323 41.862394 \nL 291.23572 40.85392 \nL 291.592118 40.353509 \nL 291.948516 40.375825 \nL 292.304913 42.086301 \nL 292.661311 39.347308 \nL 293.017709 39.93178 \nL 293.374106 38.811409 \nL 294.086902 42.217986 \nL 294.4433 40.234329 \nL 294.799697 41.210355 \nL 295.156095 40.919999 \nL 295.512493 41.221473 \nL 295.86889 38.938621 \nL 296.225288 40.382773 \nL 296.581686 39.03933 \nL 296.938083 41.491687 \nL 297.294481 39.645524 \nL 297.650879 39.703021 \nL 298.007277 39.0762 \nL 298.363674 42.19828 \nL 299.07647 38.480483 \nL 299.432867 40.054521 \nL 299.789265 40.140314 \nL 300.145663 38.278294 \nL 300.50206 40.356624 \nL 301.214856 40.960971 \nL 301.571254 40.561084 \nL 301.927651 41.258958 \nL 302.640447 37.934543 \nL 302.996844 38.911073 \nL 303.353242 40.880064 \nL 303.70964 38.279344 \nL 304.066037 39.226092 \nL 304.422435 39.846762 \nL 304.778833 39.402489 \nL 305.135231 40.234563 \nL 305.491628 40.672271 \nL 305.848026 41.581513 \nL 306.204424 43.313532 \nL 306.917219 38.475309 \nL 307.273617 41.917039 \nL 307.630015 40.619956 \nL 307.986412 40.734741 \nL 308.34281 40.190973 \nL 308.699208 40.29828 \nL 309.055605 38.59873 \nL 309.412003 41.822994 \nL 309.768401 41.602022 \nL 310.124798 39.120998 \nL 310.837594 41.015734 \nL 311.193992 39.603725 \nL 311.550389 41.458284 \nL 311.906787 39.773651 \nL 312.619582 41.278493 \nL 312.97598 38.215983 \nL 313.332378 39.738237 \nL 313.688775 37.831821 \nL 314.045173 37.076404 \nL 314.401571 37.387597 \nL 314.757969 39.454193 \nL 315.470764 39.528592 \nL 315.827162 40.394673 \nL 316.183559 37.228911 \nL 316.539957 39.162794 \nL 317.252752 41.397186 \nL 317.60915 41.22339 \nL 317.965548 40.034474 \nL 318.321946 39.820236 \nL 318.678343 39.407534 \nL 319.034741 42.199463 \nL 319.391139 41.409812 \nL 319.747536 39.680132 \nL 320.460332 40.289827 \nL 321.173127 40.253942 \nL 321.529525 43.602012 \nL 321.885923 38.936159 \nL 322.24232 39.994469 \nL 322.598718 40.270385 \nL 322.955116 41.883016 \nL 323.311513 39.748223 \nL 323.667911 39.564266 \nL 324.024309 39.630858 \nL 324.380706 40.704913 \nL 324.737104 40.934477 \nL 325.093502 40.247328 \nL 325.4499 39.857288 \nL 325.806297 42.163887 \nL 326.162695 40.241971 \nL 326.519093 39.798578 \nL 326.87549 39.934873 \nL 327.231888 40.32871 \nL 327.944683 40.571332 \nL 328.301081 41.725829 \nL 328.657479 39.684481 \nL 329.013877 41.356306 \nL 329.370274 40.284547 \nL 329.726672 42.421628 \nL 330.08307 38.947954 \nL 330.439467 42.892613 \nL 330.795865 40.714672 \nL 331.152263 40.753988 \nL 331.50866 40.612344 \nL 331.865058 41.826103 \nL 332.221456 39.59858 \nL 332.577854 40.249226 \nL 332.934251 38.89713 \nL 333.290649 42.189634 \nL 333.647047 43.645306 \nL 334.003444 42.254254 \nL 334.359842 41.972305 \nL 334.71624 39.771491 \nL 335.072637 41.407091 \nL 335.429035 40.685505 \nL 335.785433 41.122763 \nL 336.141831 41.335346 \nL 336.498228 41.113106 \nL 336.854626 38.295796 \nL 337.211024 42.191339 \nL 337.567421 41.328538 \nL 337.923819 42.832259 \nL 338.636614 39.945359 \nL 338.993012 40.891803 \nL 339.34941 39.979929 \nL 339.705808 41.639954 \nL 340.062205 40.110205 \nL 340.418603 42.142062 \nL 340.775001 40.93628 \nL 341.131398 40.538962 \nL 341.487796 39.349364 \nL 341.844194 39.64376 \nL 342.200591 41.755871 \nL 342.913387 42.443201 \nL 343.269785 39.387788 \nL 343.626182 37.520982 \nL 343.98258 39.670936 \nL 344.338978 40.916103 \nL 345.051773 38.014491 \nL 345.408171 38.839368 \nL 345.764568 37.798328 \nL 346.120966 41.681731 \nL 346.477364 40.107101 \nL 346.833762 42.103455 \nL 347.190159 39.862071 \nL 347.546557 40.06568 \nL 347.902955 39.165086 \nL 348.259352 39.934136 \nL 348.61575 42.86535 \nL 348.972148 40.04173 \nL 349.328545 40.66149 \nL 349.684943 40.154416 \nL 349.684943 40.154416 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 224.64 \nL 30.103125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 364.903125 224.64 \nL 364.903125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 224.64 \nL 364.903125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 7.2 \nL 364.903125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 226.75 219.64 \nL 357.903125 219.64 \nQ 359.903125 219.64 359.903125 217.64 \nL 359.903125 189.28375 \nQ 359.903125 187.28375 357.903125 187.28375 \nL 226.75 187.28375 \nQ 224.75 187.28375 224.75 189.28375 \nL 224.75 217.64 \nQ 224.75 219.64 226.75 219.64 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_15\">\n     <path d=\"M 228.75 195.382187 \nL 248.75 195.382187 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_16\"/>\n    <g id=\"text_13\">\n     <!-- Train Accuracy -->\n     <defs>\n      <path d=\"M -0.296875 72.90625 \nL 61.375 72.90625 \nL 61.375 64.59375 \nL 35.5 64.59375 \nL 35.5 0 \nL 25.59375 0 \nL 25.59375 64.59375 \nL -0.296875 64.59375 \nz\n\" id=\"DejaVuSans-84\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 34.1875 63.1875 \nL 20.796875 26.90625 \nL 47.609375 26.90625 \nz\nM 28.609375 72.90625 \nL 39.796875 72.90625 \nL 67.578125 0 \nL 57.328125 0 \nL 50.6875 18.703125 \nL 17.828125 18.703125 \nL 11.1875 0 \nL 0.78125 0 \nz\n\" id=\"DejaVuSans-65\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n      <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n     </defs>\n     <g transform=\"translate(256.75 198.882187)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-84\"/>\n      <use x=\"60.865234\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"101.978516\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"163.257812\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"191.041016\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"254.419922\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"286.207031\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"354.599609\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"409.580078\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"464.560547\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"527.939453\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"569.052734\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"630.332031\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"685.3125\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n    <g id=\"line2d_17\">\n     <path d=\"M 228.75 210.060312 \nL 248.75 210.060312 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_18\"/>\n    <g id=\"text_14\">\n     <!-- Validation Accuracy -->\n     <defs>\n      <path d=\"M 28.609375 0 \nL 0.78125 72.90625 \nL 11.078125 72.90625 \nL 34.1875 11.53125 \nL 57.328125 72.90625 \nL 67.578125 72.90625 \nL 39.796875 0 \nz\n\" id=\"DejaVuSans-86\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n     </defs>\n     <g transform=\"translate(256.75 213.560312)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-86\"/>\n      <use x=\"68.298828\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"129.578125\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"157.361328\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"185.144531\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"248.621094\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"309.900391\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"349.109375\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"376.892578\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"438.074219\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"501.453125\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"533.240234\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"601.632812\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"656.613281\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"711.59375\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"774.972656\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"816.085938\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"877.365234\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"932.345703\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p3e0b231bfb\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(tab_net.history['train_balanced_accuracy'], label='Train Accuracy')\n",
    "plt.plot(tab_net.history['valid_balanced_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 372.103125 248.518125\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 372.103125 248.518125 \nL 372.103125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 224.64 \nL 364.903125 224.64 \nL 364.903125 7.2 \nL 30.103125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m93e134cf04\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#m93e134cf04\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(42.140057 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"116.600847\" xlink:href=\"#m93e134cf04\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 200 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(107.057097 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"187.880387\" xlink:href=\"#m93e134cf04\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 400 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(178.336637 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"259.159927\" xlink:href=\"#m93e134cf04\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 600 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(249.616177 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"330.439467\" xlink:href=\"#m93e134cf04\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 800 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(320.895717 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_6\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m023bf7e341\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m023bf7e341\" y=\"197.498982\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 1.0 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(7.2 201.298201)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m023bf7e341\" y=\"165.197145\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 1.5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(7.2 168.996364)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m023bf7e341\" y=\"132.895308\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 2.0 -->\n      <g transform=\"translate(7.2 136.694527)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m023bf7e341\" y=\"100.593472\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 2.5 -->\n      <g transform=\"translate(7.2 104.39269)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m023bf7e341\" y=\"68.291635\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 3.0 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(7.2 72.090854)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m023bf7e341\" y=\"35.989798\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 3.5 -->\n      <g transform=\"translate(7.2 39.789017)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_12\">\n    <path clip-path=\"url(#pcde556bb72)\" d=\"M 45.321307 17.083636 \nL 45.677705 84.558362 \nL 46.034102 115.385402 \nL 47.459693 157.327107 \nL 47.816091 163.672746 \nL 48.172488 167.178824 \nL 49.598079 175.89705 \nL 50.310875 178.772553 \nL 51.736465 182.656678 \nL 52.092863 182.883621 \nL 52.449261 184.401714 \nL 52.805659 185.316177 \nL 53.162056 185.577391 \nL 53.518454 185.046004 \nL 54.587647 186.496421 \nL 54.944045 186.66849 \nL 56.013238 187.78701 \nL 56.369636 188.441789 \nL 56.726033 188.612392 \nL 57.795226 190.025075 \nL 58.508022 190.860263 \nL 58.864419 190.951937 \nL 59.220817 191.256425 \nL 59.577215 191.141443 \nL 59.933613 191.505342 \nL 61.002806 192.069814 \nL 61.359203 192.155909 \nL 61.715601 192.35904 \nL 62.071999 191.902515 \nL 62.428396 192.102961 \nL 63.141192 192.822691 \nL 63.49759 192.822675 \nL 64.210385 193.070936 \nL 64.566783 193.032109 \nL 64.92318 193.201174 \nL 65.279578 193.628773 \nL 65.992373 194.158499 \nL 66.348771 194.027476 \nL 66.705169 194.444552 \nL 67.061567 194.524425 \nL 67.417964 194.818874 \nL 67.774362 194.85076 \nL 68.13076 195.214757 \nL 68.487157 194.887739 \nL 69.199953 195.252515 \nL 69.55635 195.2173 \nL 70.269146 194.910491 \nL 70.625544 195.383493 \nL 70.981941 195.403574 \nL 71.338339 195.113441 \nL 71.694737 195.297933 \nL 72.76393 195.079735 \nL 73.120327 195.466522 \nL 73.476725 195.4651 \nL 73.833123 195.644244 \nL 74.545918 195.791653 \nL 75.258714 195.626764 \nL 75.615111 195.629697 \nL 75.971509 195.973482 \nL 76.327907 195.796581 \nL 76.684304 195.944718 \nL 77.040702 195.821825 \nL 77.3971 196.214552 \nL 77.753498 196.049635 \nL 78.109895 196.235629 \nL 78.466293 196.120368 \nL 78.822691 196.167266 \nL 79.179088 196.095204 \nL 79.535486 196.475083 \nL 80.604679 197.069313 \nL 80.961077 197.00343 \nL 81.317475 197.098464 \nL 82.03027 196.964197 \nL 82.743065 197.692274 \nL 83.099463 197.881466 \nL 83.455861 197.873115 \nL 83.812258 198.063687 \nL 84.525054 197.89419 \nL 84.881452 198.06376 \nL 85.237849 198.067463 \nL 85.594247 197.820124 \nL 85.950645 198.13546 \nL 86.307042 198.142741 \nL 87.019838 198.44207 \nL 87.376235 198.526333 \nL 87.732633 198.93934 \nL 88.089031 198.622851 \nL 88.445429 198.615895 \nL 89.158224 198.960789 \nL 90.940213 199.537454 \nL 91.29661 199.845157 \nL 92.009406 199.78574 \nL 92.365803 199.636908 \nL 92.722201 199.90144 \nL 93.078599 199.996194 \nL 93.434996 199.884536 \nL 94.147792 200.178048 \nL 94.50419 200.323184 \nL 95.216985 200.368954 \nL 95.92978 200.692893 \nL 96.998973 200.93027 \nL 97.355371 200.843813 \nL 97.711769 201.12535 \nL 99.13736 201.255888 \nL 99.493757 201.139516 \nL 99.850155 201.385721 \nL 100.206553 201.259953 \nL 100.56295 201.364024 \nL 100.919348 201.303377 \nL 102.344939 201.778068 \nL 102.701337 201.824107 \nL 103.057734 201.686189 \nL 103.414132 201.917123 \nL 104.126927 202.067031 \nL 104.839723 202.058296 \nL 105.552518 202.262857 \nL 106.978109 202.561396 \nL 107.334507 202.463801 \nL 108.4037 202.484245 \nL 108.760098 202.407589 \nL 109.472893 202.55803 \nL 110.185688 202.157326 \nL 112.324075 203.259038 \nL 112.680472 203.221949 \nL 113.393268 202.082729 \nL 113.749665 202.753041 \nL 114.106063 203.054237 \nL 114.462461 202.872347 \nL 115.175256 203.074387 \nL 115.531654 203.365821 \nL 115.888052 203.275151 \nL 116.600847 203.430214 \nL 116.957245 203.687355 \nL 118.026438 203.466293 \nL 118.382835 203.608194 \nL 118.739233 203.407339 \nL 119.095631 203.4827 \nL 119.452029 203.442761 \nL 120.521222 203.69312 \nL 120.877619 203.755958 \nL 121.590415 204.052391 \nL 121.946812 204.091465 \nL 122.30321 204.332245 \nL 123.372403 204.260377 \nL 124.085199 204.34763 \nL 124.441596 204.312791 \nL 124.797994 203.637236 \nL 125.510789 203.55271 \nL 127.292778 204.614173 \nL 127.649176 204.396884 \nL 128.005573 204.572325 \nL 128.361971 204.2911 \nL 128.718369 204.592229 \nL 129.431164 204.817434 \nL 130.500357 204.609507 \nL 131.213153 204.623 \nL 131.925948 204.785108 \nL 132.282346 204.99702 \nL 132.638743 204.910188 \nL 132.995141 204.643127 \nL 133.351539 205.07018 \nL 133.707937 205.035861 \nL 134.064334 205.153 \nL 134.420732 205.080156 \nL 135.133527 205.320255 \nL 135.489925 205.315919 \nL 135.846323 205.128608 \nL 136.20272 205.382725 \nL 137.628311 205.521881 \nL 139.053902 205.458062 \nL 139.4103 205.625843 \nL 139.766697 205.541767 \nL 140.479493 205.756093 \nL 140.835891 205.768573 \nL 141.192288 205.956223 \nL 141.905084 205.895151 \nL 142.261481 205.993954 \nL 142.974277 205.669149 \nL 143.330674 205.420205 \nL 143.687072 205.841985 \nL 144.399868 206.197414 \nL 144.756265 206.232144 \nL 145.112663 206.526789 \nL 146.181856 206.218272 \nL 146.538254 206.377006 \nL 146.894652 206.383713 \nL 147.963845 206.753078 \nL 148.320242 206.423403 \nL 148.67664 206.685462 \nL 149.033038 206.463281 \nL 149.389435 206.400513 \nL 150.458629 206.736756 \nL 150.815026 206.536097 \nL 151.171424 206.655751 \nL 151.527822 206.970749 \nL 151.884219 206.763548 \nL 152.240617 206.981107 \nL 152.597015 206.900172 \nL 152.953412 207.156685 \nL 153.666208 207.008649 \nL 154.022606 206.717595 \nL 154.379003 206.852257 \nL 154.735401 206.69708 \nL 155.448196 207.125038 \nL 156.517389 206.779118 \nL 156.873787 206.8539 \nL 157.230185 207.316034 \nL 157.586583 206.991011 \nL 157.94298 207.432229 \nL 158.299378 207.232 \nL 159.368571 207.230215 \nL 159.724969 207.467738 \nL 160.081366 207.380492 \nL 160.437764 207.470934 \nL 160.794162 207.76544 \nL 161.15056 207.813751 \nL 161.506957 207.595013 \nL 161.863355 207.817634 \nL 162.219753 207.552509 \nL 163.288946 207.529875 \nL 163.645343 207.21691 \nL 164.001741 207.201576 \nL 164.358139 206.957966 \nL 164.714537 207.290124 \nL 165.070934 207.211003 \nL 165.78373 207.633784 \nL 166.140127 207.492205 \nL 166.852923 207.814665 \nL 167.922116 207.951444 \nL 168.278514 207.861723 \nL 168.634911 207.91765 \nL 168.991309 207.796344 \nL 169.704104 208.004895 \nL 170.060502 207.813603 \nL 170.773297 208.332877 \nL 171.129695 208.375866 \nL 171.486093 208.598552 \nL 171.842491 208.224949 \nL 172.555286 208.389235 \nL 172.911684 208.337203 \nL 173.980877 208.538773 \nL 174.337274 208.390531 \nL 174.693672 208.599379 \nL 175.05007 208.545852 \nL 175.762865 208.771166 \nL 176.119263 208.632751 \nL 176.475661 208.644386 \nL 176.832058 208.782229 \nL 177.544854 208.849073 \nL 177.901251 208.648386 \nL 178.257649 208.883732 \nL 178.970445 208.878248 \nL 179.326842 209.014559 \nL 180.039638 208.986726 \nL 180.396035 208.764875 \nL 180.752433 209.309743 \nL 181.108831 208.964278 \nL 181.465228 208.934709 \nL 181.821626 209.17573 \nL 182.178024 209.107412 \nL 183.603615 208.13337 \nL 183.960012 206.988787 \nL 184.672808 207.193375 \nL 185.385603 207.261747 \nL 185.742001 206.990685 \nL 186.454796 207.704152 \nL 187.167592 208.030517 \nL 187.523989 208.352452 \nL 187.880387 208.098297 \nL 188.94958 208.470852 \nL 189.305978 208.296562 \nL 189.662376 208.561576 \nL 190.018773 208.405486 \nL 190.731569 208.668245 \nL 191.087966 208.400006 \nL 191.444364 208.530786 \nL 192.513557 208.514366 \nL 192.869955 208.669149 \nL 193.226353 208.587122 \nL 193.58275 208.313242 \nL 193.939148 207.173839 \nL 194.295546 208.377155 \nL 194.651943 208.602561 \nL 195.364739 208.647505 \nL 195.721136 208.799059 \nL 196.077534 208.710302 \nL 196.79033 208.925623 \nL 197.859523 208.763613 \nL 198.21592 209.08753 \nL 198.928716 209.126702 \nL 199.285114 208.914933 \nL 199.641511 208.886714 \nL 200.354307 209.343593 \nL 200.710704 209.242256 \nL 201.067102 209.621197 \nL 201.779897 209.79639 \nL 202.136295 209.753645 \nL 202.492693 210.047909 \nL 203.205488 209.835543 \nL 203.918284 209.974281 \nL 204.274681 210.114887 \nL 204.631079 210.080564 \nL 204.987477 209.873175 \nL 205.343874 210.043146 \nL 207.125863 210.063215 \nL 207.482261 210.274298 \nL 207.838658 210.219573 \nL 208.195056 210.291229 \nL 208.551454 210.215963 \nL 208.907851 210.309332 \nL 209.264249 210.047242 \nL 209.620647 210.428068 \nL 209.977045 210.124264 \nL 210.333442 210.007364 \nL 210.68984 210.324975 \nL 211.046238 210.038861 \nL 211.402635 210.328433 \nL 211.759033 210.391013 \nL 212.115431 210.586036 \nL 212.471828 210.177503 \nL 213.184624 210.735936 \nL 213.541022 210.477597 \nL 214.253817 210.455474 \nL 214.610215 210.424004 \nL 214.966612 210.253958 \nL 215.32301 209.382578 \nL 215.679408 208.082363 \nL 216.392203 210.310335 \nL 216.748601 210.508882 \nL 217.104999 209.476476 \nL 217.817794 208.791109 \nL 218.174192 209.191332 \nL 218.530589 209.118723 \nL 218.886987 208.844437 \nL 219.243385 208.715521 \nL 219.95618 208.166958 \nL 220.312578 208.668513 \nL 221.738169 208.467541 \nL 222.094566 208.809149 \nL 222.450964 208.818021 \nL 222.807362 209.051299 \nL 223.163759 208.745719 \nL 223.520157 208.951376 \nL 224.232953 208.767084 \nL 224.58935 208.472659 \nL 224.945748 209.103987 \nL 225.658543 209.225409 \nL 226.014941 209.396678 \nL 226.371339 209.308243 \nL 227.084134 209.438781 \nL 228.153327 209.613001 \nL 228.509725 209.843848 \nL 228.866123 209.464684 \nL 229.22252 209.662382 \nL 229.935316 209.503606 \nL 230.648111 209.870045 \nL 231.360907 209.395861 \nL 232.073702 209.124072 \nL 232.4301 209.276669 \nL 232.786497 209.221986 \nL 233.142895 208.961955 \nL 233.499293 209.332952 \nL 233.85569 209.523547 \nL 234.212088 209.392112 \nL 234.924884 209.797367 \nL 235.281281 209.622716 \nL 235.994077 209.86111 \nL 236.350474 209.756797 \nL 236.706872 209.955779 \nL 237.419667 209.516892 \nL 237.776065 209.663833 \nL 238.132463 209.523706 \nL 238.488861 209.838866 \nL 238.845258 209.844855 \nL 239.201656 209.96903 \nL 239.914451 209.938848 \nL 240.627247 210.022541 \nL 240.983644 209.711146 \nL 241.340042 209.835336 \nL 241.69644 210.234593 \nL 242.052838 209.968808 \nL 243.122031 210.145263 \nL 243.478428 209.632483 \nL 244.191224 209.984192 \nL 244.904019 210.318394 \nL 245.260417 210.143775 \nL 245.616815 210.294195 \nL 245.973212 210.26646 \nL 246.32961 210.49573 \nL 246.686008 210.505224 \nL 247.042405 210.31347 \nL 247.398803 210.435757 \nL 247.755201 210.239149 \nL 248.111598 210.393355 \nL 248.824394 210.350058 \nL 249.180792 210.306539 \nL 249.537189 210.544701 \nL 249.893587 210.306627 \nL 250.249985 210.398596 \nL 250.606382 209.143841 \nL 250.96278 210.035231 \nL 251.319178 210.010653 \nL 251.675576 210.452237 \nL 252.031973 209.876231 \nL 252.388371 210.321086 \nL 252.744769 210.215719 \nL 254.170359 210.932946 \nL 254.526757 210.829802 \nL 254.883155 210.964667 \nL 256.308746 210.927421 \nL 256.665143 211.290604 \nL 257.021541 211.482603 \nL 257.377939 211.339304 \nL 257.734336 211.56533 \nL 258.447132 211.657056 \nL 258.80353 211.543328 \nL 259.159927 211.750797 \nL 259.872723 211.634834 \nL 260.22912 211.888492 \nL 260.585518 211.802722 \nL 260.941916 211.451399 \nL 261.298313 211.813535 \nL 261.654711 212.008039 \nL 262.367507 211.316123 \nL 262.723904 211.112647 \nL 263.080302 211.114576 \nL 263.4367 211.280305 \nL 263.793097 210.798041 \nL 264.149495 210.643149 \nL 264.505893 211.303085 \nL 264.86229 211.319925 \nL 265.218688 211.758873 \nL 265.575086 211.707202 \nL 265.931484 212.115055 \nL 266.287881 211.991813 \nL 267.357074 212.218774 \nL 267.713472 212.038764 \nL 268.06987 212.059115 \nL 268.426267 212.32982 \nL 268.782665 212.222542 \nL 269.495461 212.365604 \nL 269.851858 212.400986 \nL 270.208256 211.28472 \nL 270.564654 212.070602 \nL 270.921051 211.716819 \nL 271.277449 211.655009 \nL 271.633847 212.046418 \nL 271.990244 211.699441 \nL 272.346642 211.632053 \nL 272.70304 212.151726 \nL 273.059438 212.460771 \nL 273.415835 212.471536 \nL 273.772233 212.337543 \nL 275.554221 212.532368 \nL 276.267017 212.290634 \nL 277.33621 212.811919 \nL 277.692608 212.516948 \nL 278.761801 212.767484 \nL 279.118198 212.550456 \nL 279.474596 212.115754 \nL 280.187392 212.054432 \nL 280.543789 212.417683 \nL 281.256585 212.629392 \nL 282.325778 212.614845 \nL 282.682175 212.741113 \nL 283.394971 212.46752 \nL 283.751369 212.93695 \nL 284.820562 212.544812 \nL 286.246152 212.648546 \nL 286.60255 212.739057 \nL 286.958948 212.648487 \nL 287.315346 212.69932 \nL 287.671743 212.58067 \nL 288.028141 212.802342 \nL 288.740936 212.971071 \nL 289.097334 213.08671 \nL 289.453732 213.015824 \nL 289.810129 213.182069 \nL 290.166527 212.824589 \nL 290.522925 212.827749 \nL 290.879323 213.17308 \nL 291.23572 212.836154 \nL 291.592118 213.071351 \nL 292.661311 212.981509 \nL 293.017709 213.14862 \nL 293.730504 213.044846 \nL 294.086902 213.250627 \nL 294.4433 213.052447 \nL 295.156095 213.266507 \nL 295.512493 213.083063 \nL 295.86889 213.341974 \nL 296.938083 213.173429 \nL 297.650879 212.801444 \nL 299.07647 212.633003 \nL 299.432867 212.820257 \nL 300.145663 212.915282 \nL 300.50206 212.697262 \nL 300.858458 212.751906 \nL 301.214856 212.965051 \nL 301.571254 212.869472 \nL 301.927651 213.02898 \nL 302.284049 213.328958 \nL 302.996844 213.140458 \nL 303.353242 213.166739 \nL 303.70964 212.783196 \nL 304.066037 212.614129 \nL 304.422435 213.274368 \nL 304.778833 213.474256 \nL 305.135231 212.582135 \nL 306.204424 212.756537 \nL 306.560821 212.625333 \nL 307.273617 213.125836 \nL 308.699208 212.96649 \nL 309.055605 213.169954 \nL 309.768401 213.29006 \nL 310.124798 213.170214 \nL 310.837594 213.527487 \nL 311.193992 213.307731 \nL 311.550389 213.440678 \nL 311.906787 212.860967 \nL 312.263185 212.988268 \nL 312.619582 213.248656 \nL 313.332378 213.290487 \nL 313.688775 213.150356 \nL 314.401571 213.584902 \nL 314.757969 213.469307 \nL 315.114366 213.683355 \nL 315.470764 213.600709 \nL 315.827162 213.336208 \nL 316.183559 213.650791 \nL 316.896355 213.45217 \nL 317.252752 213.589734 \nL 317.60915 212.665234 \nL 317.965548 213.038516 \nL 318.321946 213.21955 \nL 319.034741 213.140413 \nL 319.391139 213.431414 \nL 319.747536 213.33928 \nL 320.103934 213.564451 \nL 320.460332 213.467306 \nL 320.816729 213.628373 \nL 321.529525 213.44851 \nL 322.598718 214.001124 \nL 323.667911 213.631202 \nL 324.024309 213.925538 \nL 324.380706 213.772306 \nL 325.4499 214.094741 \nL 325.806297 213.866832 \nL 326.162695 213.934696 \nL 326.519093 213.745955 \nL 326.87549 214.018676 \nL 327.231888 213.888596 \nL 327.588286 214.20417 \nL 327.944683 214.014838 \nL 329.726672 214.172448 \nL 330.08307 213.98202 \nL 330.439467 214.24067 \nL 331.50866 214.08039 \nL 331.865058 214.340779 \nL 332.221456 214.072202 \nL 332.934251 214.152219 \nL 333.290649 208.858464 \nL 333.647047 210.335719 \nL 334.003444 213.251932 \nL 334.359842 214.058586 \nL 334.71624 214.062886 \nL 335.429035 214.390949 \nL 335.785433 214.142288 \nL 336.141831 214.334469 \nL 336.498228 214.172275 \nL 337.211024 214.259686 \nL 337.567421 214.403994 \nL 337.923819 214.311494 \nL 338.280217 214.334838 \nL 338.993012 214.564422 \nL 339.705808 214.531845 \nL 340.062205 214.279769 \nL 340.418603 214.493215 \nL 341.131398 214.398868 \nL 342.200591 214.756364 \nL 342.556989 214.45568 \nL 343.269785 214.568754 \nL 343.626182 214.287178 \nL 343.98258 214.543779 \nL 344.695375 214.342773 \nL 345.051773 214.573517 \nL 345.764568 214.510261 \nL 346.120966 214.397322 \nL 346.477364 214.471803 \nL 346.833762 214.257316 \nL 347.190159 214.2272 \nL 347.902955 214.555322 \nL 348.259352 214.421804 \nL 348.61575 214.73235 \nL 348.972148 214.449054 \nL 349.684943 214.741665 \nL 349.684943 214.741665 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 224.64 \nL 30.103125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 364.903125 224.64 \nL 364.903125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 224.64 \nL 364.903125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 7.2 \nL 364.903125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 275.176563 29.878125 \nL 357.903125 29.878125 \nQ 359.903125 29.878125 359.903125 27.878125 \nL 359.903125 14.2 \nQ 359.903125 12.2 357.903125 12.2 \nL 275.176563 12.2 \nQ 273.176563 12.2 273.176563 14.2 \nL 273.176563 27.878125 \nQ 273.176563 29.878125 275.176563 29.878125 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_13\">\n     <path d=\"M 277.176563 20.298437 \nL 297.176563 20.298437 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_14\"/>\n    <g id=\"text_12\">\n     <!-- Train Loss -->\n     <defs>\n      <path d=\"M -0.296875 72.90625 \nL 61.375 72.90625 \nL 61.375 64.59375 \nL 35.5 64.59375 \nL 35.5 0 \nL 25.59375 0 \nL 25.59375 64.59375 \nL -0.296875 64.59375 \nz\n\" id=\"DejaVuSans-84\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g transform=\"translate(305.176563 23.798437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-84\"/>\n      <use x=\"60.865234\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"101.978516\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"163.257812\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"191.041016\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"254.419922\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"286.207031\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"341.904297\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"403.085938\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"455.185547\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pcde556bb72\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(tab_net.history['loss'], label='Train Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 397.553125 248.518125\" width=\"397.553125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 397.553125 248.518125 \nL 397.553125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 55.553125 224.64 \nL 390.353125 224.64 \nL 390.353125 7.2 \nL 55.553125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m0de2eebeeb\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"70.771307\" xlink:href=\"#m0de2eebeeb\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(67.590057 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"142.050847\" xlink:href=\"#m0de2eebeeb\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 200 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(132.507097 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"213.330387\" xlink:href=\"#m0de2eebeeb\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 400 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(203.786637 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"284.609927\" xlink:href=\"#m0de2eebeeb\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 600 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(275.066177 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"355.889467\" xlink:href=\"#m0de2eebeeb\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 800 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(346.345717 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_6\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"maf1cb83650\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"55.553125\" xlink:href=\"#maf1cb83650\" y=\"194.989091\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0.00096 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       <path d=\"M 10.984375 1.515625 \nL 10.984375 10.5 \nQ 14.703125 8.734375 18.5 7.8125 \nQ 22.3125 6.890625 25.984375 6.890625 \nQ 35.75 6.890625 40.890625 13.453125 \nQ 46.046875 20.015625 46.78125 33.40625 \nQ 43.953125 29.203125 39.59375 26.953125 \nQ 35.25 24.703125 29.984375 24.703125 \nQ 19.046875 24.703125 12.671875 31.3125 \nQ 6.296875 37.9375 6.296875 49.421875 \nQ 6.296875 60.640625 12.9375 67.421875 \nQ 19.578125 74.21875 30.609375 74.21875 \nQ 43.265625 74.21875 49.921875 64.515625 \nQ 56.59375 54.828125 56.59375 36.375 \nQ 56.59375 19.140625 48.40625 8.859375 \nQ 40.234375 -1.421875 26.421875 -1.421875 \nQ 22.703125 -1.421875 18.890625 -0.6875 \nQ 15.09375 0.046875 10.984375 1.515625 \nz\nM 30.609375 32.421875 \nQ 37.25 32.421875 41.125 36.953125 \nQ 45.015625 41.5 45.015625 49.421875 \nQ 45.015625 57.28125 41.125 61.84375 \nQ 37.25 66.40625 30.609375 66.40625 \nQ 23.96875 66.40625 20.09375 61.84375 \nQ 16.21875 57.28125 16.21875 49.421875 \nQ 16.21875 41.5 20.09375 36.953125 \nQ 23.96875 32.421875 30.609375 32.421875 \nz\n\" id=\"DejaVuSans-57\"/>\n      </defs>\n      <g transform=\"translate(7.2 198.78831)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-57\"/>\n       <use x=\"349.902344\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"55.553125\" xlink:href=\"#maf1cb83650\" y=\"155.454545\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.00098 -->\n      <g transform=\"translate(7.2 159.253764)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-57\"/>\n       <use x=\"349.902344\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"55.553125\" xlink:href=\"#maf1cb83650\" y=\"115.92\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.00100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(7.2 119.719219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"349.902344\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"55.553125\" xlink:href=\"#maf1cb83650\" y=\"76.385455\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.00102 -->\n      <g transform=\"translate(7.2 80.184673)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"349.902344\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"55.553125\" xlink:href=\"#maf1cb83650\" y=\"36.850909\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.00104 -->\n      <g transform=\"translate(7.2 40.650128)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"349.902344\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_11\">\n    <path clip-path=\"url(#p806aa2518d)\" d=\"M 70.771307 115.92 \nL 375.134943 115.92 \nL 375.134943 115.92 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 55.553125 224.64 \nL 55.553125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 390.353125 224.64 \nL 390.353125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 55.553125 224.64 \nL 390.353125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 55.553125 7.2 \nL 390.353125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 246.689063 29.878125 \nL 383.353125 29.878125 \nQ 385.353125 29.878125 385.353125 27.878125 \nL 385.353125 14.2 \nQ 385.353125 12.2 383.353125 12.2 \nL 246.689063 12.2 \nQ 244.689063 12.2 244.689063 14.2 \nL 244.689063 27.878125 \nQ 244.689063 29.878125 246.689063 29.878125 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_12\">\n     <path d=\"M 248.689063 20.298438 \nL 268.689063 20.298438 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_13\"/>\n    <g id=\"text_11\">\n     <!-- Learning Rate Decay -->\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 44.390625 34.1875 \nQ 47.5625 33.109375 50.5625 29.59375 \nQ 53.5625 26.078125 56.59375 19.921875 \nL 66.609375 0 \nL 56 0 \nL 46.6875 18.703125 \nQ 43.0625 26.03125 39.671875 28.421875 \nQ 36.28125 30.8125 30.421875 30.8125 \nL 19.671875 30.8125 \nL 19.671875 0 \nL 9.8125 0 \nL 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.578125 72.90625 50.734375 67.671875 \nQ 56.890625 62.453125 56.890625 51.90625 \nQ 56.890625 45.015625 53.6875 40.46875 \nQ 50.484375 35.9375 44.390625 34.1875 \nz\nM 19.671875 64.796875 \nL 19.671875 38.921875 \nL 32.078125 38.921875 \nQ 39.203125 38.921875 42.84375 42.21875 \nQ 46.484375 45.515625 46.484375 51.90625 \nQ 46.484375 58.296875 42.84375 61.546875 \nQ 39.203125 64.796875 32.078125 64.796875 \nz\n\" id=\"DejaVuSans-82\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 19.671875 64.796875 \nL 19.671875 8.109375 \nL 31.59375 8.109375 \nQ 46.6875 8.109375 53.6875 14.9375 \nQ 60.6875 21.78125 60.6875 36.53125 \nQ 60.6875 51.171875 53.6875 57.984375 \nQ 46.6875 64.796875 31.59375 64.796875 \nz\nM 9.8125 72.90625 \nL 30.078125 72.90625 \nQ 51.265625 72.90625 61.171875 64.09375 \nQ 71.09375 55.28125 71.09375 36.53125 \nQ 71.09375 17.671875 61.125 8.828125 \nQ 51.171875 0 30.078125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-68\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n     </defs>\n     <g transform=\"translate(276.689063 23.798438)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"55.697266\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"117.220703\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"178.5\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"219.597656\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"282.976562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"310.759766\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"374.138672\" xlink:href=\"#DejaVuSans-103\"/>\n      <use x=\"437.615234\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"469.402344\" xlink:href=\"#DejaVuSans-82\"/>\n      <use x=\"538.853516\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"600.132812\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"639.341797\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"700.865234\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"732.652344\" xlink:href=\"#DejaVuSans-68\"/>\n      <use x=\"809.654297\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"871.177734\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"926.158203\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"987.4375\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p806aa2518d\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"55.553125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(tab_net.history['lr'], label='Learning Rate Decay')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}